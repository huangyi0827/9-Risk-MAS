# é£æ§ MAS å®è·µç»ƒä¹ 

<details>
<summary><b>ğŸ“ æ–‡ä»¶å®Œæˆæ¸…å•ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</b></summary>

### æ¨¡å—1: çŠ¶æ€å®šä¹‰ (0/1)
- [ ] src/state.py

### æ¨¡å—2: è¾“å…¥è§„èŒƒåŒ– (0/1)
- [ ] src/tools/validate.py

### æ¨¡å—3: æ•°æ®ä¸æŒ‡æ ‡ (0/4)
- [ ] src/tools/utils.py
- [ ] src/tools/csv_data.py
- [ ] src/tools/data_quality.py
- [ ] src/tools/snapshot.py

### æ¨¡å—4: ç¼–æ’è°ƒåº¦ (0/3)
- [ ] src/graph.py
- [ ] src/chains/gatekeeper.py
- [ ] src/chains/supervisor.py

### æ¨¡å—5: åˆ†æé“¾è·¯ (0/5)
- [ ] src/chains/common.py
- [ ] src/chains/market.py
- [ ] src/chains/concentration.py
- [ ] src/chains/diversification.py
- [ ] src/chains/liquidity.py

### æ¨¡å—6: Agent æ¨¡å— (0/3)
- [ ] src/agents/agent_utils.py
- [ ] src/agents/macro_agent.py
- [ ] src/agents/compliance_agent.py

### æ¨¡å—7: Skills ä½“ç³» (0/1)
- [ ] src/skills_runtime.py

### æ¨¡å—8: è§„åˆ™ä¸çº¦æŸ (0/2)
- [ ] src/tools/rules.py
- [ ] src/tools/constraints.py

### æ¨¡å—9: å†³ç­–ä¸å®¡è®¡ (0/4)
- [ ] src/chains/reducer.py
- [ ] src/tools/decision.py
- [ ] src/tools/solver.py
- [ ] src/tools/audit.py

### æ¨¡å—10: é˜ˆå€¼æ ¡å‡† (0/2)
- [ ] src/tools/calibrate_rules.py
- [ ] src/tools/calibrate_macro_series.py
</details>

---

## ğŸ“‹ ç»ƒä¹ è¯´æ˜

æœ¬ç»ƒä¹ è¦æ±‚ä½ ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªåŸºäºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMASï¼‰çš„ ETF æŠ•èµ„ç»„åˆé£æ§æ¡†æ¶ã€‚

**é€‚ç”¨å¯¹è±¡ï¼š**
- åœ¨æ ¡å­¦ç”Ÿï¼ˆé‡‘èå·¥ç¨‹ã€è®¡ç®—æœºç›¸å…³ä¸“ä¸šï¼‰
- é‡‘èä¸šåŠ¡æ¡çº¿æŠ€æœ¯äººå‘˜


**å‰ç½®çŸ¥è¯†ï¼š**
- Python åŸºç¡€
- åŸºæœ¬çš„é‡‘èæ¦‚å¿µï¼ˆETFã€æŠ•èµ„ç»„åˆã€é£é™©æŒ‡æ ‡ï¼‰
- LangChain/LangGraph åŸºç¡€ï¼ˆå»ºè®®å…ˆå®Œæˆå®˜æ–¹æ•™ç¨‹ï¼‰
- ç¬¬äºŒç»ƒRAGã€ç¬¬å››ç»ƒNLP

---

## âš™ï¸ é…ç½®æ–¹å¼

- é¡¹ç›®è¿è¡Œæ—¶é…ç½®ç»Ÿä¸€ç”± `src/config.py` è¯»å–ç¯å¢ƒå˜é‡å¹¶æ³¨å…¥æ¨¡å—ã€‚
- ä¸é…ç½®ä¹Ÿå¯è¿è¡Œï¼ˆä½¿ç”¨é»˜è®¤å€¼ï¼‰ï¼›éœ€è¦å¯ç”¨ LLM / Tushare / RAG æ—¶å†åœ¨ `.env` ä¸­è¡¥å……å¯¹åº”å˜é‡ã€‚

## ğŸ¯ ç»ƒä¹ ç›®æ ‡

æ„å»ºä¸€ä¸ªå®Œæ•´çš„é£æ§ MAS ç³»ç»Ÿï¼Œèƒ½å¤Ÿï¼š

1. âœ… æ¥æ”¶äº¤æ˜“æ„å›¾ï¼ˆIntentï¼‰å’Œç»„åˆä¸Šä¸‹æ–‡ï¼ˆContextï¼‰
2. âœ… æ‰§è¡Œå¤šç»´åº¦é£é™©åˆ†æï¼ˆå¸‚åœºã€é›†ä¸­åº¦ã€åˆ†æ•£åº¦ã€æµåŠ¨æ€§ã€å®è§‚ã€åˆè§„ï¼‰
3. âœ… åŸºäºè§„åˆ™å’Œ LLM åˆ†æç»™å‡ºé£æ§å†³ç­–ï¼ˆpass/warn/restrict/blockï¼‰
4. âœ… åœ¨ restrict æƒ…å†µä¸‹ç”Ÿæˆè°ƒä»“å»ºè®®
5. âœ… è¾“å‡ºå®Œæ•´çš„å®¡è®¡æ—¥å¿—

---

## ğŸ“¦ æ¨¡å—åˆ’åˆ†

ä½ éœ€è¦å®ç°ä»¥ä¸‹ 10 ä¸ªæ¨¡å—ï¼š

| æ¨¡å— | æ–‡ä»¶æ•° | éš¾åº¦ |
|:---|:---:|:---:|
| æ¨¡å—1: çŠ¶æ€å®šä¹‰ | 1 | â­ |
| æ¨¡å—2: è¾“å…¥è§„èŒƒåŒ– | 1 | â­â­ |
| æ¨¡å—3: æ•°æ®ä¸æŒ‡æ ‡ | 4 | â­â­â­ |
| æ¨¡å—4: ç¼–æ’è°ƒåº¦ | 3 | â­â­â­â­ |
| æ¨¡å—5: åˆ†æé“¾è·¯ | 5 | â­â­â­ |
| æ¨¡å—6: Agent æ¨¡å— | 3 | â­â­â­â­ |
| æ¨¡å—7: Skills ä½“ç³» | 1 | â­â­ |
| æ¨¡å—8: è§„åˆ™ä¸çº¦æŸ | 2 | â­â­â­ |
| æ¨¡å—9: å†³ç­–ä¸å®¡è®¡ | 4 | â­â­â­ |
| æ¨¡å—10: é˜ˆå€¼æ ¡å‡† | 2 | â­â­â­ |
---

## ğŸ“ æ¨¡å—1: çŠ¶æ€å®šä¹‰

<details open>
<summary><b>ğŸ“‹ æ¨¡å—ä¿¡æ¯</b></summary>

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **éš¾åº¦** | â­ å…¥é—¨ |
| **æ–‡ä»¶æ•°** | 1ä¸ª |
| **æ–‡ä»¶è·¯å¾„** | `src/state.py` |
| **ä¾èµ–æ¨¡å—** | æ—  |
| **è¢«ä¾èµ–** | æ‰€æœ‰å…¶ä»–æ¨¡å— |

</details>

<details>
<summary><b>ğŸ¯ å­¦ä¹ ç›®æ ‡</b></summary>

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ å°†æŒæ¡ï¼š
- [ ] TypedDict çš„ä½¿ç”¨æ–¹æ³•
- [ ] å¦‚ä½•è®¾è®¡å·¥ä½œæµçŠ¶æ€ç»“æ„
- [ ] NotRequired ç±»å‹æ ‡æ³¨çš„åº”ç”¨
- [ ] çŠ¶æ€å­—æ®µçš„åˆ†ç»„è®¾è®¡åŸåˆ™

</details>

<details>
<summary><b>ğŸ“– åŠŸèƒ½è¯´æ˜</b></summary>

å®šä¹‰æ•´ä¸ªå·¥ä½œæµçš„çŠ¶æ€ç»“æ„ï¼ˆRiskStateï¼‰ï¼Œä½¿ç”¨ TypedDict ç¡®ä¿ç±»å‹å®‰å…¨ã€‚

**æ ¸å¿ƒæ¦‚å¿µï¼š**
- **æ˜¾å¼çŠ¶æ€ç®¡ç†**ï¼šæ‰€æœ‰èŠ‚ç‚¹å…±äº«åŒä¸€ä¸ªçŠ¶æ€å¯¹è±¡
- **ç±»å‹å®‰å…¨**ï¼šä½¿ç”¨ TypedDict æä¾›ç±»å‹æç¤º
- **å­—æ®µåˆ†ç»„**ï¼šæŒ‰åŠŸèƒ½å°†å­—æ®µåˆ†ä¸ºè¾“å…¥ã€é¢„å¤„ç†ã€åˆ†æã€å†³ç­–å››å¤§ç±»

**ä¸ºä»€ä¹ˆé‡è¦ï¼š**
çŠ¶æ€å®šä¹‰æ˜¯æ•´ä¸ªç³»ç»Ÿçš„åŸºç¡€ï¼Œæ‰€æœ‰èŠ‚ç‚¹éƒ½ä¾èµ–è¿™ä¸ªçŠ¶æ€ç»“æ„è¿›è¡Œæ•°æ®ä¼ é€’ã€‚

</details>

<details>
<summary><b>âœ… å®ç°è¦æ±‚</b></summary>

å®šä¹‰ `Intent`ã€`Context`ã€`Finding` ä¸‰ä¸ªå­ TypedDict å’Œä¸»çŠ¶æ€ `RiskState`ï¼Œä»¥åŠå·¥å‚å‡½æ•° `new_state()`ã€‚

**1. å­ç±»å‹å®šä¹‰**

`Intent` TypedDictï¼ˆå¿…å¡«å­—æ®µï¼‰ï¼š
- `date`: äº¤æ˜“æ—¥æœŸï¼ˆstrï¼‰
- `mode`: æ¨¡å¼ target æˆ– deltaï¼ˆstrï¼‰
- `targets`: ç›®æ ‡æƒé‡æ˜ å°„ï¼ˆDict[str, float]ï¼‰

`Context` TypedDictï¼ˆ`total=False`ï¼Œå…¨éƒ¨å¯é€‰ï¼‰ï¼š
- `current_positions`: å½“å‰æŒä»“æƒé‡ï¼ˆDict[str, float]ï¼‰
- `current_positions_date`: æŒä»“æ—¥æœŸï¼ˆstrï¼‰
- `universe`: ETF ä»£ç åˆ—è¡¨ï¼ˆList[str]ï¼‰
- `trade_calendar`: äº¤æ˜“æ—¥å†æ ‡è¯†ï¼ˆstrï¼‰
- `account_type`: è´¦æˆ·ç±»å‹ï¼ˆstrï¼‰
- `jurisdiction`: å¸æ³•ç®¡è¾–åŒºï¼ˆstrï¼‰
- `policy_profile`: è§„åˆ™é…ç½®åï¼ˆstrï¼‰
- `aum`: èµ„äº§ç®¡ç†è§„æ¨¡ï¼ˆfloatï¼‰

`Finding` TypedDictï¼ˆ`total=False`ï¼Œå…¨éƒ¨å¯é€‰ï¼‰ï¼š
- `agent`: äº§ç”Ÿè¯¥ç»“æœçš„ Agent åç§°ï¼ˆstrï¼‰
- `risk_type`: é£é™©ç±»å‹ï¼ˆstrï¼‰
- `severity`: é£é™©ç­‰çº§ 0-3ï¼ˆintï¼‰
- `summary`: ä¸€å¥è¯æ€»ç»“ï¼ˆstrï¼‰
- `evidence`: è¯æ®åˆ—è¡¨ï¼ˆList[Dict[str, Any]]ï¼‰
- `metrics`: æŒ‡æ ‡å­—å…¸ï¼ˆDict[str, Any]ï¼‰
- `recommendations`: å»ºè®®åˆ—è¡¨ï¼ˆList[str]ï¼‰
- `policy_ids`: å…³è”çš„æ”¿ç­– IDï¼ˆList[str]ï¼‰

**2. RiskState å­—æ®µåˆ†ç»„**

`RiskState` TypedDictï¼ˆ`total=False`ï¼‰ï¼ŒæŒ‰ä»¥ä¸‹åˆ†ç»„å®šä¹‰å­—æ®µï¼š

*è¾“å…¥å­—æ®µ*
- `intent`: äº¤æ˜“æ„å›¾ï¼ˆIntentï¼‰
- `context`: ç»„åˆä¸Šä¸‹æ–‡ï¼ˆContextï¼‰

*é¢„å¤„ç†å­—æ®µ*
- `normalized`: å½’ä¸€åŒ–åçš„æ•°æ®ï¼ˆDict[str, Any]ï¼‰
- `validation`: éªŒè¯ç»“æœï¼ˆDict[str, Any]ï¼‰

*ç¡®å®šæ€§å·¥å…·å­—æ®µ*
- `data_quality`: æ•°æ®è´¨é‡æ£€æŸ¥ç»“æœï¼ˆDict[str, Any]ï¼‰
- `data_gaps`: æ•°æ®ç¼ºå£åˆ—è¡¨ï¼ˆList[Dict[str, Any]]ï¼‰
- `snapshot_metrics`: æŒ‡æ ‡å¿«ç…§ï¼ˆDict[str, Any]ï¼‰
- `rule_findings`: è§„åˆ™æ£€æŸ¥ç»“æœåˆ—è¡¨ï¼ˆList[Dict[str, Any]]ï¼‰
- `compliance_blocklist`: ç¡¬ç¦æŠ•æ¸…å•ï¼ˆList[str]ï¼‰
- `compliance_blocklist_soft`: è½¯ç¦æŠ•æ¸…å•ï¼ˆList[str]ï¼‰
- `compliance_blocklist_meta`: ç¦æŠ•æ¸…å•å…ƒæ•°æ®ï¼ˆDict[str, Any]ï¼‰

*è·¯ç”±å­—æ®µ*
- `candidate_nodes`: å€™é€‰èŠ‚ç‚¹åˆ—è¡¨ï¼ˆList[str]ï¼‰
- `nodes_to_run`: å®é™…è¿è¡ŒèŠ‚ç‚¹åˆ—è¡¨ï¼ˆList[str]ï¼‰
- `pending_agents`: å¾…æ‰§è¡Œ Agent åˆ—è¡¨ï¼ˆList[str]ï¼‰
- `next_agent`: ä¸‹ä¸€ä¸ª Agentï¼ˆstrï¼‰
- `stop_condition`: æ˜¯å¦ç»ˆæ­¢ï¼ˆboolï¼‰
- `cost_budget`: æˆæœ¬é¢„ç®—ï¼ˆDict[str, Any]ï¼‰
- `gatekeeper_used`: æ˜¯å¦ä½¿ç”¨äº† Gatekeeperï¼ˆboolï¼‰
- `gatekeeper_rationale`: Gatekeeper å†³ç­–ç†ç”±ï¼ˆstrï¼‰
- `supervisor_used`: æ˜¯å¦ä½¿ç”¨äº† Supervisorï¼ˆboolï¼‰
- `supervisor_rationale`: Supervisor å†³ç­–ç†ç”±ï¼ˆstrï¼‰
- `supervisor_model`: Supervisor ä½¿ç”¨çš„æ¨¡å‹ï¼ˆstrï¼‰

*å¹¶è¡Œåˆ†æå­—æ®µ*
- `finding_market`: å¸‚åœºé£é™©åˆ†æç»“æœï¼ˆFindingï¼‰
- `finding_concentration`: é›†ä¸­åº¦é£é™©åˆ†æç»“æœï¼ˆFindingï¼‰
- `finding_diversification`: åˆ†æ•£åº¦é£é™©åˆ†æç»“æœï¼ˆFindingï¼‰
- `finding_liquidity`: æµåŠ¨æ€§é£é™©åˆ†æç»“æœï¼ˆFindingï¼‰
- `finding_macro`: å®è§‚é£é™©åˆ†æç»“æœï¼ˆFindingï¼‰
- `finding_compliance`: åˆè§„é£é™©åˆ†æç»“æœï¼ˆFindingï¼‰
- `tool_calls_macro`: å®è§‚ Agent å·¥å…·è°ƒç”¨è®°å½•ï¼ˆList[Dict[str, Any]]ï¼‰
- `tool_calls_compliance`: åˆè§„ Agent å·¥å…·è°ƒç”¨è®°å½•ï¼ˆList[Dict[str, Any]]ï¼‰
- `llm_used_macro`: å®è§‚åˆ†ææ˜¯å¦ä½¿ç”¨äº† LLMï¼ˆboolï¼‰
- `llm_used_compliance`: åˆè§„åˆ†ææ˜¯å¦ä½¿ç”¨äº† LLMï¼ˆboolï¼‰
- `llm_model_macro`: å®è§‚åˆ†æä½¿ç”¨çš„æ¨¡å‹åï¼ˆstrï¼‰
- `llm_model_compliance`: åˆè§„åˆ†æä½¿ç”¨çš„æ¨¡å‹åï¼ˆstrï¼‰

*æ±‡æ€»å­—æ®µ*
- `findings`: æ‰€æœ‰åˆ†æç»“æœåˆ—è¡¨ï¼ˆList[Finding]ï¼‰
- `risk_report`: é£é™©æŠ¥å‘Šï¼ˆDict[str, Any]ï¼‰

*å†³ç­–å­—æ®µ*
- `decision`: å†³ç­–ç»“æœï¼ˆDict[str, Any]ï¼‰
- `binding_constraints`: çº¦æŸæ¡ä»¶åˆ—è¡¨ï¼ˆList[Dict[str, Any]]ï¼‰
- `recommended_actions`: è°ƒä»“å»ºè®®åˆ—è¡¨ï¼ˆList[Dict[str, Any]]ï¼‰

*å®¡è®¡å­—æ®µ*
- `audit`: å®¡è®¡æ—¥å¿—ï¼ˆDict[str, Any]ï¼‰

**3. å·¥å‚å‡½æ•°**
- `new_state(intent, context=None) -> RiskState`: åˆ›å»ºåˆå§‹çŠ¶æ€ï¼Œåªå¡«å…¥ intent å’Œ context

</details>

<details>
<summary><b>ğŸ’» ä»£ç æ¨¡æ¿</b></summary>

```python
"""
é£æ§ MAS çŠ¶æ€å®šä¹‰æ¨¡å—
å®šä¹‰æ•´ä¸ªå·¥ä½œæµçš„çŠ¶æ€ç»“æ„
"""
from __future__ import annotations

from typing import Any, Dict, List, Optional, TypedDict


class Intent(TypedDict):
    """äº¤æ˜“æ„å›¾"""
    # TODO: å®šä¹‰ date å­—æ®µï¼ˆäº¤æ˜“æ—¥æœŸï¼Œstrï¼‰
    # TODO: å®šä¹‰ mode å­—æ®µï¼ˆæ¨¡å¼: target | deltaï¼Œstrï¼‰
    # TODO: å®šä¹‰ targets å­—æ®µï¼ˆç›®æ ‡æƒé‡ï¼ŒDict[str, float]ï¼‰
    pass


class Context(TypedDict, total=False):
    """ç»„åˆä¸Šä¸‹æ–‡ï¼ˆå…¨éƒ¨å¯é€‰ï¼‰"""
    # TODO: å®šä¹‰ current_positions å­—æ®µï¼ˆå½“å‰æŒä»“æƒé‡ï¼ŒDict[str, float]ï¼‰
    # TODO: å®šä¹‰ current_positions_date å­—æ®µï¼ˆæŒä»“æ—¥æœŸï¼Œstrï¼‰
    # TODO: å®šä¹‰ universe å­—æ®µï¼ˆETF ä»£ç åˆ—è¡¨ï¼ŒList[str]ï¼‰
    # TODO: å®šä¹‰ trade_calendar å­—æ®µï¼ˆäº¤æ˜“æ—¥å†æ ‡è¯†ï¼Œstrï¼‰
    # TODO: å®šä¹‰ account_type å­—æ®µï¼ˆè´¦æˆ·ç±»å‹ï¼Œstrï¼‰
    # TODO: å®šä¹‰ jurisdiction å­—æ®µï¼ˆå¸æ³•ç®¡è¾–åŒºï¼Œstrï¼‰
    # TODO: å®šä¹‰ policy_profile å­—æ®µï¼ˆè§„åˆ™é…ç½®åï¼Œstrï¼‰
    # TODO: å®šä¹‰ aum å­—æ®µï¼ˆèµ„äº§ç®¡ç†è§„æ¨¡ï¼Œfloatï¼‰
    pass


class Finding(TypedDict, total=False):
    """å•ä¸ªåˆ†æç»´åº¦çš„ç»“æœ"""
    # TODO: å®šä¹‰ agent å­—æ®µï¼ˆäº§ç”Ÿè¯¥ç»“æœçš„ Agent åç§°ï¼Œstrï¼‰
    # TODO: å®šä¹‰ risk_type å­—æ®µï¼ˆé£é™©ç±»å‹ï¼Œstrï¼‰
    # TODO: å®šä¹‰ severity å­—æ®µï¼ˆé£é™©ç­‰çº§ 0-3ï¼Œintï¼‰
    # TODO: å®šä¹‰ summary å­—æ®µï¼ˆä¸€å¥è¯æ€»ç»“ï¼Œstrï¼‰
    # TODO: å®šä¹‰ evidence å­—æ®µï¼ˆè¯æ®åˆ—è¡¨ï¼ŒList[Dict[str, Any]]ï¼‰
    # TODO: å®šä¹‰ metrics å­—æ®µï¼ˆæŒ‡æ ‡å­—å…¸ï¼ŒDict[str, Any]ï¼‰
    # TODO: å®šä¹‰ recommendations å­—æ®µï¼ˆå»ºè®®åˆ—è¡¨ï¼ŒList[str]ï¼‰
    # TODO: å®šä¹‰ policy_ids å­—æ®µï¼ˆå…³è”çš„æ”¿ç­– IDï¼ŒList[str]ï¼‰
    pass


class RiskState(TypedDict, total=False):
    """
    é£æ§å·¥ä½œæµçš„çŠ¶æ€ç»“æ„

    ä½¿ç”¨ total=False è¡¨ç¤ºæ‰€æœ‰å­—æ®µéƒ½æ˜¯å¯é€‰çš„
    """

    # ========== è¾“å…¥å­—æ®µ ==========
    # TODO: å®šä¹‰ intent å­—æ®µï¼ˆIntent ç±»å‹ï¼‰
    # TODO: å®šä¹‰ context å­—æ®µï¼ˆContext ç±»å‹ï¼‰

    # ========== é¢„å¤„ç†å­—æ®µ ==========
    # TODO: å®šä¹‰ normalized å­—æ®µï¼ˆDict[str, Any]ï¼‰
    # TODO: å®šä¹‰ validation å­—æ®µï¼ˆDict[str, Any]ï¼‰

    # ========== ç¡®å®šæ€§å·¥å…·å­—æ®µ ==========
    # TODO: å®šä¹‰ data_quality å­—æ®µï¼ˆDict[str, Any]ï¼‰
    # TODO: å®šä¹‰ data_gaps å­—æ®µï¼ˆList[Dict[str, Any]]ï¼‰
    # TODO: å®šä¹‰ snapshot_metrics å­—æ®µï¼ˆDict[str, Any]ï¼‰
    # TODO: å®šä¹‰ rule_findings å­—æ®µï¼ˆList[Dict[str, Any]]ï¼‰
    # TODO: å®šä¹‰ compliance_blocklist å­—æ®µï¼ˆList[str]ï¼‰
    # TODO: å®šä¹‰ compliance_blocklist_soft å­—æ®µï¼ˆList[str]ï¼‰
    # TODO: å®šä¹‰ compliance_blocklist_meta å­—æ®µï¼ˆDict[str, Any]ï¼‰

    # ========== è·¯ç”±å­—æ®µ ==========
    # TODO: å®šä¹‰ candidate_nodes å­—æ®µï¼ˆList[str]ï¼‰
    # TODO: å®šä¹‰ nodes_to_run å­—æ®µï¼ˆList[str]ï¼‰
    # TODO: å®šä¹‰ pending_agents å­—æ®µï¼ˆList[str]ï¼‰
    # TODO: å®šä¹‰ next_agent å­—æ®µï¼ˆstrï¼‰
    # TODO: å®šä¹‰ stop_condition å­—æ®µï¼ˆboolï¼‰
    # TODO: å®šä¹‰ cost_budget å­—æ®µï¼ˆDict[str, Any]ï¼‰
    # TODO: å®šä¹‰ gatekeeper_used å­—æ®µï¼ˆboolï¼‰
    # TODO: å®šä¹‰ gatekeeper_rationale å­—æ®µï¼ˆstrï¼‰
    # TODO: å®šä¹‰ supervisor_used å­—æ®µï¼ˆboolï¼‰
    # TODO: å®šä¹‰ supervisor_rationale å­—æ®µï¼ˆstrï¼‰
    # TODO: å®šä¹‰ supervisor_model å­—æ®µï¼ˆstrï¼‰

    # ========== å¹¶è¡Œåˆ†æå­—æ®µ ==========
    # TODO: å®šä¹‰ finding_market å­—æ®µï¼ˆFinding ç±»å‹ï¼‰

    # TODO: å®šä¹‰ finding_concentration å­—æ®µï¼ˆFinding ç±»å‹ï¼‰
    # TODO: å®šä¹‰ finding_diversification å­—æ®µï¼ˆFinding ç±»å‹ï¼‰
    # TODO: å®šä¹‰ finding_liquidity å­—æ®µï¼ˆFinding ç±»å‹ï¼‰
    # TODO: å®šä¹‰ finding_macro å­—æ®µï¼ˆFinding ç±»å‹ï¼‰
    # TODO: å®šä¹‰ finding_compliance å­—æ®µï¼ˆFinding ç±»å‹ï¼‰
    # TODO: å®šä¹‰ tool_calls_macro å­—æ®µï¼ˆList[Dict[str, Any]]ï¼‰
    # TODO: å®šä¹‰ tool_calls_compliance å­—æ®µï¼ˆList[Dict[str, Any]]ï¼‰
    # TODO: å®šä¹‰ llm_used_macro å­—æ®µï¼ˆboolï¼‰
    # TODO: å®šä¹‰ llm_used_compliance å­—æ®µï¼ˆboolï¼‰
    # TODO: å®šä¹‰ llm_model_macro å­—æ®µï¼ˆstrï¼‰
    # TODO: å®šä¹‰ llm_model_compliance å­—æ®µï¼ˆstrï¼‰

    # ========== æ±‡æ€»å­—æ®µ ==========
    # TODO: å®šä¹‰ findings å­—æ®µï¼ˆList[Finding]ï¼‰
    # TODO: å®šä¹‰ risk_report å­—æ®µï¼ˆDict[str, Any]ï¼‰

    # ========== å†³ç­–å­—æ®µ ==========
    # TODO: å®šä¹‰ decision å­—æ®µï¼ˆDict[str, Any]ï¼‰
    # TODO: å®šä¹‰ binding_constraints å­—æ®µï¼ˆList[Dict[str, Any]]ï¼‰
    # TODO: å®šä¹‰ recommended_actions å­—æ®µï¼ˆList[Dict[str, Any]]ï¼‰

    # ========== å®¡è®¡å­—æ®µ ==========
    # TODO: å®šä¹‰ audit å­—æ®µï¼ˆDict[str, Any]ï¼‰
    pass


def new_state(intent: Intent, context: Optional[Context] = None) -> RiskState:
    """åˆ›å»ºåˆå§‹çŠ¶æ€ï¼Œåªå¡«å…¥ intent å’Œ context"""
    # TODO: è¿”å›ä¸€ä¸ªåªåŒ…å« intent å’Œ context çš„ RiskState
    pass
```

</details>

<details>
<summary><b>ğŸ§ª æµ‹è¯•æ£€æŸ¥</b></summary>

å®Œæˆåè¯·æ£€æŸ¥ï¼š
- [ ] `Intent`ã€`Context`ã€`Finding` ä¸‰ä¸ªå­ TypedDict å·²å®šä¹‰
- [ ] `RiskState` ä½¿ç”¨ `total=False`ï¼Œæ‰€æœ‰å­—æ®µå¯é€‰
- [ ] `Finding` ç±»å‹ç”¨äº `finding_*` å­—æ®µï¼ˆè€Œé `dict`ï¼‰
- [ ] `new_state()` å·¥å‚å‡½æ•°å·²å®ç°
- [ ] å¯ä»¥æˆåŠŸå¯¼å…¥æ‰€æœ‰ç±»å‹

**æµ‹è¯•å‘½ä»¤ï¼š**
```bash
# æµ‹è¯•å¯¼å…¥
uv run --env-file .env -- python -u -c "
from src.state import RiskState, Intent, Context, Finding, new_state
s = new_state({'date': '2025-01-06', 'mode': 'target', 'targets': {'510300': 0.5}})
assert 'intent' in s
print('âœ… çŠ¶æ€å®šä¹‰æ­£ç¡®')
"
```

**é¢„æœŸè¾“å‡ºï¼š**
```
âœ… çŠ¶æ€å®šä¹‰æ­£ç¡®
```

</details>

<details>
<summary><b>ğŸ’¡ æç¤ºä¸æŠ€å·§</b></summary>

**å…³é”®ç‚¹ï¼š**
1. ä½¿ç”¨ `NotRequired` è€Œä¸æ˜¯ `Optional`
   - `NotRequired` è¡¨ç¤ºå­—æ®µå¯ä»¥ä¸å­˜åœ¨
   - `Optional` è¡¨ç¤ºå­—æ®µå€¼å¯ä»¥æ˜¯ None

2. å­—æ®µå‘½åè§„èŒƒ
   - åˆ†æç»“æœå­—æ®µç»Ÿä¸€ä½¿ç”¨ `finding_` å‰ç¼€
   - ä¿æŒä¸åç»­æ¨¡å—çš„å‘½åä¸€è‡´

3. ç±»å‹æ ‡æ³¨
   - ä½¿ç”¨ `dict[str, Any]` è€Œä¸æ˜¯ `dict`
   - ä½¿ç”¨ `list[dict[str, Any]]` è€Œä¸æ˜¯ `list`

**å¸¸è§é”™è¯¯ï¼š**
- âŒ å¿˜è®°å¯¼å…¥ `NotRequired`
- âŒ ä½¿ç”¨ `Optional` ä»£æ›¿ `NotRequired`
- âŒ å­—æ®µåæ‹¼å†™é”™è¯¯

**å‚è€ƒèµ„æºï¼š**
- [TypedDict å®˜æ–¹æ–‡æ¡£](https://docs.python.org/3/library/typing.html#typing.TypedDict)
- [NotRequired è¯´æ˜](https://peps.python.org/pep-0655/)

</details>

---

## ğŸ“ æ¨¡å—2: è¾“å…¥è§„èŒƒåŒ–

<details open>
<summary><b>ğŸ“‹ æ¨¡å—ä¿¡æ¯</b></summary>

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **éš¾åº¦** | â­â­ è¿›é˜¶ |
| **æ–‡ä»¶æ•°** | 1ä¸ª |
| **æ–‡ä»¶è·¯å¾„** | `src/tools/validate.py` |
| **ä¾èµ–æ¨¡å—** | æ¨¡å—1 |
| **è¢«ä¾èµ–** | æ¨¡å—3, 4 |

</details>

<details>
<summary><b>ğŸ¯ å­¦ä¹ ç›®æ ‡</b></summary>

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ å°†æŒæ¡ï¼š
- [ ] è¾“å…¥éªŒè¯çš„æœ€ä½³å®è·µ
- [ ] æ•°æ®å½’ä¸€åŒ–æŠ€æœ¯
- [ ] é”™è¯¯å¤„ç†å’Œè­¦å‘Šæœºåˆ¶
- [ ] çŠ¶æ€æ›´æ–°æ¨¡å¼

</details>

<details>
<summary><b>ğŸ“– åŠŸèƒ½è¯´æ˜</b></summary>

éªŒè¯å’Œè§„èŒƒåŒ–ç”¨æˆ·è¾“å…¥ï¼Œç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®ã€‚

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
1. **è¾“å…¥éªŒè¯**ï¼šæ£€æŸ¥å¿…å¡«å­—æ®µå’Œæ•°æ®æ ¼å¼
2. **æƒé‡å½’ä¸€åŒ–**ï¼šç¡®ä¿æƒé‡å’Œä¸º1
3. **æ¨¡å¼è½¬æ¢**ï¼šå¤„ç† delta æ¨¡å¼åˆ° target æ¨¡å¼çš„è½¬æ¢
4. **é”™è¯¯æ”¶é›†**ï¼šè®°å½•æ‰€æœ‰éªŒè¯é—®é¢˜

**ä¸ºä»€ä¹ˆé‡è¦ï¼š**
è¾“å…¥éªŒè¯æ˜¯ç³»ç»Ÿçš„ç¬¬ä¸€é“é˜²çº¿ï¼Œç¡®ä¿åç»­èŠ‚ç‚¹æ¥æ”¶åˆ°çš„æ•°æ®æ˜¯åˆæ³•çš„ã€‚

</details>

<details>
<summary><b>âœ… å®ç°è¦æ±‚</b></summary>

å®ç° `validate_and_normalize(state, config) -> Dict[str, Any]` å‡½æ•°åŠ 4 ä¸ªè¾…åŠ©å‡½æ•°ï¼š

**1. è¾…åŠ©å‡½æ•°**
- `_sum_weights(weights)`: è®¡ç®—æƒé‡ä¹‹å’Œ
- `_normalize_weights(weights)`: å½’ä¸€åŒ–æƒé‡ä½¿å…¶å’Œä¸º 1
- `_coerce_weights(weights, errors, label)`: å°†æƒé‡å€¼å¼ºåˆ¶è½¬ä¸º floatï¼Œæ— æ³•è½¬æ¢çš„è®°å½•é”™è¯¯
- `_validate_date(date_str)`: æ ¡éªŒæ—¥æœŸæ ¼å¼ `YYYY-MM-DD`ï¼Œè¿”å› `(bool, str)`

**2. ä¸»å‡½æ•° `validate_and_normalize`**
- éªŒè¯ `intent.date`ï¼ˆå¿…å¡«ï¼ŒYYYY-MM-DD æ ¼å¼ï¼‰
- éªŒè¯ `intent.mode`ï¼ˆå¿…å¡«ï¼Œtarget æˆ– deltaï¼‰
- ç”¨ `_coerce_weights` è½¬æ¢ `intent.targets` å’Œ `context.current_positions`
- å¤„ç† `mode=delta`ï¼šå°† delta å åŠ åˆ° current_positions åå½’ä¸€åŒ–
- å¤„ç† `mode=target`ï¼šæƒé‡å’Œä¸ä¸º 1 æ—¶è‡ªåŠ¨å½’ä¸€åŒ–å¹¶è®°å½• warning
- è°ƒç”¨ `previous_trading_date(date_str, cfg)` è·å– `asof_date`ï¼ˆæ¥è‡ª csv_data æ¨¡å—ï¼‰
- æ„å»º `universe` åˆ—è¡¨ï¼ˆåˆå¹¶ context.universe + current_positions.keys + target_weights.keysï¼Œå»é‡ï¼‰

**3. è¿”å›å€¼**ï¼ˆDictï¼Œä¸æ˜¯ RiskStateï¼‰
```python
{
    "normalized": {
        "asof_date": str,        # ä¸Šä¸€ä¸ªäº¤æ˜“æ—¥
        "mode": str,
        "targets": dict,         # åŸå§‹ç›®æ ‡
        "current_positions": dict,
        "current_positions_date": str,
        "target_weights": dict,  # å½’ä¸€åŒ–åçš„ç›®æ ‡æƒé‡
        "universe": list,        # ETF ä»£ç åˆ—è¡¨
        "account_type": str,
        "jurisdiction": str,
        "policy_profile": str,   # é»˜è®¤ "default"
        "aum": float,
    },
    "validation": {
        "is_valid": bool,
        "errors": list,
        "warnings": list,
    },
}
```

</details>

<details>
<summary><b>ğŸ’» ä»£ç æ¨¡æ¿</b></summary>

```python
"""
è¾“å…¥éªŒè¯ä¸è§„èŒƒåŒ–æ¨¡å—
"""
from __future__ import annotations

from datetime import datetime
from typing import Dict, Any, Tuple

from ..state import RiskState
from ..config import RuntimeConfig, DEFAULT_CONFIG
from ..tools.csv_data import previous_trading_date


def _sum_weights(weights: Dict[str, float]) -> float:
    """è®¡ç®—æƒé‡ä¹‹å’Œ"""
    # TODO: å®ç°
    pass


def _normalize_weights(weights: Dict[str, float]) -> Dict[str, float]:
    """å½’ä¸€åŒ–æƒé‡ä½¿å…¶å’Œä¸º 1"""
    # TODO: è°ƒç”¨ _sum_weightsï¼Œè‹¥ total <= 0 è¿”å›åŸå€¼ï¼Œå¦åˆ™æ¯ä¸ªå€¼é™¤ä»¥ total
    pass


def _coerce_weights(weights: Dict[str, Any], errors: list[str], label: str) -> Dict[str, float]:
    """å°†æƒé‡å€¼å¼ºåˆ¶è½¬ä¸º floatï¼Œæ— æ³•è½¬æ¢çš„è®°å½•åˆ° errors"""
    # TODO: éå† weightsï¼Œå°è¯• float(value)ï¼Œå¤±è´¥åˆ™ append é”™è¯¯ä¿¡æ¯
    pass


def _validate_date(date_str: str) -> Tuple[bool, str]:
    """æ ¡éªŒæ—¥æœŸæ ¼å¼ YYYY-MM-DD"""
    # TODO: ç”¨ datetime.strptime æ ¡éªŒï¼Œè¿”å› (True, "") æˆ– (False, é”™è¯¯ä¿¡æ¯)
    pass


def validate_and_normalize(state: RiskState, config: RuntimeConfig | None = None) -> Dict[str, Any]:
    """
    éªŒè¯å’Œè§„èŒƒåŒ–ç”¨æˆ·è¾“å…¥

    Args:
        state: å½“å‰çŠ¶æ€
        config: è¿è¡Œæ—¶é…ç½®

    Returns:
        åŒ…å« "normalized" å’Œ "validation" ä¸¤ä¸ªé”®çš„å­—å…¸
    """
    cfg = config or DEFAULT_CONFIG
    intent = state.get("intent") or {}
    context = state.get("context") or {}

    errors = []
    warnings = []

    # TODO: éªŒè¯ intent.date å­—æ®µ
    # - æ£€æŸ¥æ˜¯å¦å­˜åœ¨ï¼ˆç©ºå­—ç¬¦ä¸²è§†ä¸ºç¼ºå¤±ï¼‰
    # - ç”¨ _validate_date æ£€æŸ¥æ ¼å¼

    # TODO: éªŒè¯ intent.mode å­—æ®µ
    # - é»˜è®¤å€¼ "target"ï¼Œåªå…è®¸ "target" æˆ– "delta"

    # TODO: ç”¨ _coerce_weights è½¬æ¢ targets å’Œ current_positions
    # - targets ä¸ºç©ºæ—¶è®°å½•é”™è¯¯

    # TODO: å¤„ç† mode=delta
    # - å°† delta å åŠ åˆ° current_positionsï¼Œç„¶å _normalize_weights
    # TODO: å¤„ç† mode=target
    # - æƒé‡å’Œä¸ä¸º 1 æ—¶å½’ä¸€åŒ–å¹¶è®°å½• warning

    # TODO: æ„å»º universe åˆ—è¡¨
    # - åˆå¹¶ context.universe + current.keys() + target_weights.keys()
    # - å»é‡ä¿åºï¼ˆç”¨ dict.fromkeysï¼‰

    # TODO: è°ƒç”¨ previous_trading_date(date_str, cfg) è·å– asof_date

    # TODO: æ„å»º normalized å­—å…¸ï¼ˆåŒ…å« asof_date, mode, targets, current_positions,
    #        current_positions_date, target_weights, universe, account_type,
    #        jurisdiction, policy_profile, aumï¼‰

    # TODO: è¿”å› {"normalized": normalized, "validation": {"is_valid": ..., "errors": ..., "warnings": ...}}
    pass
```

</details>

<details>
<summary><b>ğŸ§ª æµ‹è¯•æ£€æŸ¥</b></summary>

å®Œæˆåè¯·æ£€æŸ¥ï¼š
- [ ] èƒ½æ­£ç¡®éªŒè¯æ—¥æœŸæ ¼å¼
- [ ] èƒ½æ­£ç¡®éªŒè¯ mode å­—æ®µ
- [ ] èƒ½æ­£ç¡®å½’ä¸€åŒ–æƒé‡
- [ ] èƒ½æ­£ç¡®å¤„ç† delta æ¨¡å¼
- [ ] é”™è¯¯å’Œè­¦å‘Šè¢«æ­£ç¡®è®°å½•

**æµ‹è¯•å‘½ä»¤ï¼š**
```bash
# åˆ›å»ºæµ‹è¯•æ–‡ä»¶ test_validate.py
uv run --env-file .env -- python -u test_validate.py
```

**æµ‹è¯•ç”¨ä¾‹ï¼š**
```python
from src.tools.validate import (
    validate_and_normalize,
    _sum_weights, _normalize_weights, _coerce_weights, _validate_date,
)

# æµ‹è¯•1: è¾…åŠ©å‡½æ•°
assert _sum_weights({"A": 0.3, "B": 0.7}) == 1.0
assert _sum_weights({}) == 0.0
nw = _normalize_weights({"A": 3, "B": 7})
assert abs(nw["A"] - 0.3) < 1e-9
ok, msg = _validate_date("2025-11-15")
assert ok and msg == ""
ok2, msg2 = _validate_date("bad-date")
assert not ok2

errs = []
cw = _coerce_weights({"A": "0.5", "B": "abc"}, errs, "test")
assert cw == {"A": 0.5} and len(errs) == 1

# æµ‹è¯•2: æ­£å¸¸è¾“å…¥
state = {
    "intent": {
        "date": "2025-11-15",
        "mode": "target",
        "targets": {"159213": 0.5, "159959": 0.5},
    },
    "context": {},
}
result = validate_and_normalize(state)
assert result["validation"]["is_valid"] is True
assert result["normalized"]["mode"] == "target"
assert abs(sum(result["normalized"]["target_weights"].values()) - 1.0) < 1e-6

# æµ‹è¯•3: æƒé‡ä¸ä¸º1 â†’ è‡ªåŠ¨å½’ä¸€åŒ– + warning
state2 = {
    "intent": {
        "date": "2025-11-15",
        "mode": "target",
        "targets": {"159213": 0.3, "159959": 0.4},
    },
    "context": {},
}
result2 = validate_and_normalize(state2)
assert len(result2["validation"]["warnings"]) > 0
assert abs(sum(result2["normalized"]["target_weights"].values()) - 1.0) < 1e-6

# æµ‹è¯•4: delta æ¨¡å¼
state3 = {
    "intent": {
        "date": "2025-11-15",
        "mode": "delta",
        "targets": {"159213": 0.1},
    },
    "context": {"current_positions": {"159213": 0.6, "159959": 0.4}},
}
result3 = validate_and_normalize(state3)
assert result3["validation"]["is_valid"] is True
assert "159213" in result3["normalized"]["target_weights"]

# æµ‹è¯•5: ç¼ºå¤±æ—¥æœŸ â†’ é”™è¯¯
state4 = {"intent": {"targets": {"A": 1.0}}, "context": {}}
result4 = validate_and_normalize(state4)
assert result4["validation"]["is_valid"] is False

print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡")
```

</details>

<details>
<summary><b>ğŸ’¡ æç¤ºä¸æŠ€å·§</b></summary>

**å…³é”®ç‚¹ï¼š**
1. æ—¥æœŸéªŒè¯å¯ä»¥ä½¿ç”¨ `datetime.strptime`
2. æƒé‡å½’ä¸€åŒ–å…¬å¼ï¼š`w_i_new = w_i / sum(w)`
3. delta æ¨¡å¼å¤„ç†ï¼š`target = current + delta`

**å¸¸è§é”™è¯¯ï¼š**
- âŒ å¿˜è®°å¤„ç†ç©ºå­—å…¸çš„æƒ…å†µ
- âŒ æƒé‡å½’ä¸€åŒ–åç²¾åº¦é—®é¢˜
- âŒ delta æ¨¡å¼ä¸‹æ²¡æœ‰æ£€æŸ¥ current_positions æ˜¯å¦å­˜åœ¨

**ä¼˜åŒ–å»ºè®®ï¼š**
- ä½¿ç”¨ `get()` æ–¹æ³•é¿å… KeyError
- å¯¹æµ®ç‚¹æ•°æ¯”è¾ƒä½¿ç”¨å®¹å·®ï¼ˆå¦‚ `abs(x - 1.0) < 1e-6`ï¼‰

</details>

---

## ğŸ“ æ¨¡å—3: æ•°æ®ä¸æŒ‡æ ‡

<details open>
<summary><b>ğŸ“‹ æ¨¡å—ä¿¡æ¯</b></summary>

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **éš¾åº¦** | â­â­â­ ä¸­çº§ |
| **æ–‡ä»¶æ•°** | 4ä¸ª |
| **æ–‡ä»¶è·¯å¾„** | `src/tools/utils.py`<br>`src/tools/csv_data.py`<br>`src/tools/data_quality.py`<br>`src/tools/snapshot.py` |
| **ä¾èµ–æ¨¡å—** | æ¨¡å—1, 2 |
| **è¢«ä¾èµ–** | æ¨¡å—4, 5, 6, 7 |

</details>

<details>
<summary><b>ğŸ¯ å­¦ä¹ ç›®æ ‡</b></summary>

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ å°†æŒæ¡ï¼š
- [ ] é‡‘èæŒ‡æ ‡è®¡ç®—ï¼ˆHHIã€æœ‰æ•ˆæŒä»“æ•°ã€æ³¢åŠ¨ç‡ï¼‰
- [ ] CSV æ•°æ®å¤„ç†æŠ€æœ¯
- [ ] æ•°æ®è´¨é‡æ£€æŸ¥æ–¹æ³•
- [ ] Pandas æ•°æ®åˆ†æåº”ç”¨

</details>

<details>
<summary><b>ğŸ“– åŠŸèƒ½è¯´æ˜</b></summary>

æœ¬æ¨¡å—è´Ÿè´£æ•°æ®åŠ è½½ã€è´¨é‡æ£€æŸ¥å’ŒæŒ‡æ ‡è®¡ç®—ï¼Œæ˜¯ç³»ç»Ÿçš„æ•°æ®åŸºç¡€å±‚ã€‚

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
1. **å·¥å…·å‡½æ•°**ï¼šæä¾›æƒé‡å½’ä¸€åŒ–ã€HHI è®¡ç®—ç­‰å…±äº«å‡½æ•°
2. **æ•°æ®åŠ è½½**ï¼šä» CSV è¯»å– ETF è¡Œæƒ…ã€åˆè§„æ–‡æœ¬ç­‰
3. **è´¨é‡æ£€æŸ¥**ï¼šæ£€æŸ¥æ•°æ®å®Œæ•´æ€§å’Œæ–°é²œåº¦
4. **æŒ‡æ ‡å¿«ç…§**ï¼šè®¡ç®—ç»„åˆçš„é£é™©æŒ‡æ ‡

**ä¸ºä»€ä¹ˆé‡è¦ï¼š**
æ•°æ®è´¨é‡ç›´æ¥å½±å“é£é™©åˆ†æçš„å‡†ç¡®æ€§ï¼ŒæŒ‡æ ‡è®¡ç®—æ˜¯åç»­å†³ç­–çš„åŸºç¡€ã€‚

</details>

<details>
<summary><b>âœ… å®ç°è¦æ±‚</b></summary>

### æ–‡ä»¶1ï¼š`src/tools/utils.py`

**å¸¸é‡ï¼š**
- `WEIGHT_TOLERANCE = 1e-6`ï¼šæƒé‡æ¯”è¾ƒå®¹å·®
- `EPSILON = 1e-12`ï¼šé˜²é™¤é›¶å¸¸é‡

**å®ç° 4 ä¸ªå·¥å…·å‡½æ•°ï¼š**

1. `normalize_weights(weights: Dict[str, float]) -> Dict[str, float]`
   - å°†æƒé‡å½’ä¸€åŒ–ä¸ºå’Œä¸º 1
   - å…ˆç”¨ `float()` æ¸…æ´—å€¼ï¼Œ`total <= 0` æ—¶è¿”å›æ¸…æ´—åçš„åŸå€¼

2. `compute_hhi(weights: Dict[str, float], already_normalized: bool = False) -> float`
   - è®¡ç®— HHI æŒ‡æ•°ï¼ˆHerfindahl-Hirschman Indexï¼‰
   - å…¬å¼ï¼šHHI = Î£(w_iÂ²)
   - `already_normalized=True` æ—¶è·³è¿‡å½’ä¸€åŒ–ï¼Œç›´æ¥å¯¹ values æ±‚å¹³æ–¹å’Œ
   - `already_normalized=False` æ—¶å…ˆé™¤ä»¥ total å†æ±‚å¹³æ–¹å’Œ

3. `compute_effective_n(weights: Dict[str, float], already_normalized: bool = False) -> float`
   - è®¡ç®—æœ‰æ•ˆæŒä»“æ•°ï¼ˆinverse HHIï¼‰
   - å…¬å¼ï¼š1 / HHIï¼ˆHHI â‰¤ EPSILON æ—¶è¿”å› 0.0ï¼‰

4. `weights_sum_to_one(weights: Dict[str, float]) -> bool`
   - æ£€æŸ¥æƒé‡å’Œæ˜¯å¦åœ¨ WEIGHT_TOLERANCE å†…ç­‰äº 1.0

### æ–‡ä»¶2ï¼š`src/tools/csv_data.py`

**å®ç°æ•°æ®åŠ è½½ä¸æŸ¥è¯¢å‡½æ•°ï¼ˆå…± 18 ä¸ªå‡½æ•°ï¼‰ï¼š**

**åŸºç¡€è®¾æ–½ï¼ˆ3 ä¸ªï¼‰ï¼š**
1. `_data_dir(config) -> Path`ï¼šæ ¹æ® `config.csv_data_dir` è¿”å›æ•°æ®ç›®å½•ï¼Œé»˜è®¤ `cufel_practice_data/`
2. `_load_csv(path, *, usecols=None) -> pd.DataFrame`ï¼šé€šç”¨ CSV åŠ è½½ï¼Œæ–‡ä»¶ä¸å­˜åœ¨è¿”å›ç©º DataFrame
3. `_load_etf_prices_cached(path_str) -> pd.DataFrame`ï¼šå¸¦ `@lru_cache` çš„ ETF è¡Œæƒ…åŠ è½½ï¼Œè½¬æ¢ code/date/æ•°å€¼åˆ—ç±»å‹

**æ•°æ®åŠ è½½ï¼ˆ5 ä¸ªï¼‰ï¼š**
4. `load_etf_prices(config) -> pd.DataFrame`ï¼šåŠ è½½ `etf_2025_data.csv`
5. `load_etf_basic(config) -> pd.DataFrame`ï¼šåŠ è½½ `sampled_etf_basic.csv`ï¼ˆETF åŸºæœ¬ä¿¡æ¯ï¼‰
6. `load_compliance_docs(config) -> pd.DataFrame`ï¼šåŠ è½½ `csrc_2025.csv`ï¼ˆåˆè§„æ–‡æœ¬ï¼‰
7. `load_macro_docs(config) -> pd.DataFrame`ï¼šåŠ è½½å®è§‚æ–‡æœ¬ï¼ˆä¼˜å…ˆ `govcn_2025_results.json`ï¼Œé™çº§åˆ° `govcn_2025.csv`ï¼‰
8. `etf_industry_map(config) -> Dict[str, str]`ï¼šä» ETF åŸºæœ¬ä¿¡æ¯æ„å»º `{code: industry}` æ˜ å°„

**æŸ¥è¯¢å‡½æ•°ï¼ˆ6 ä¸ªï¼‰ï¼š**
9. `etf_codes_by_industry(industry_names, config) -> Dict[str, List[str]]`ï¼šæŒ‰è¡Œä¸šåæŸ¥ ETF ä»£ç 
10. `security_master_codes(config) -> Tuple[set, str]`ï¼šè¿”å›æ‰€æœ‰å·²çŸ¥ ETF ä»£ç é›†åˆåŠæ¥æºæ–‡ä»¶å
11. `sample_universe(asof_date, size, seed, config) -> List[str]`ï¼šéšæœºé‡‡æ · ETF ä»£ç ï¼ˆç”¨äºè’™ç‰¹å¡æ´›ï¼‰
12. `previous_trading_date(asof_date, config) -> str`ï¼šæŸ¥æ‰¾ asof_date ä¹‹å‰æœ€è¿‘çš„äº¤æ˜“æ—¥
13. `lookback_start_date(asof_date, lookback_days) -> str`ï¼šè®¡ç®—å›æº¯èµ·å§‹æ—¥æœŸ

**è¡Œæƒ…æŒ‡æ ‡ï¼ˆ2 ä¸ªï¼‰ï¼š**
14. `market_metrics(codes, start_date, end_date, config) -> Dict[str, Dict[str, float]]`ï¼šè®¡ç®—æ¯åª ETF çš„ volatility / adv / spread_bps
15. `market_metrics_by_range(start_date, end_date, config) -> Tuple[List[str], Dict]`ï¼šæŒ‰æ—¥æœŸèŒƒå›´æŸ¥å…¨éƒ¨ ETF æŒ‡æ ‡

**æ–‡æœ¬æœç´¢ï¼ˆ2 ä¸ªï¼‰ï¼š**
16. `macro_search_hits(query, limit, asof_date, config) -> List[Dict]`ï¼šå…³é”®è¯æœç´¢å®è§‚æ–‡æœ¬
17. `compliance_search_hits(query, limit, config) -> List[str]`ï¼šå…³é”®è¯æœç´¢åˆè§„æ–‡æœ¬

**å¯ç”¨æ€§æ£€æŸ¥ï¼ˆ2 ä¸ªï¼‰ï¼š**
18. `macro_docs_available(config) -> bool` / `compliance_docs_available(config) -> bool`
19. `macro_latest_date(asof_date, config) -> str`ï¼šå®è§‚æ–‡æœ¬æœ€æ–°æ—¥æœŸ

### æ–‡ä»¶3ï¼š`src/tools/data_quality.py`

**å®ç° `check_data_quality(state, config) -> Dict[str, Any]`**ï¼ˆæ³¨æ„ï¼šå‡½æ•°åä¸æ˜¯ `data_quality_node`ï¼Œè¿”å› Dict ä¸æ˜¯ RiskStateï¼‰

**è¾…åŠ©å‡½æ•°ï¼š**
- `_append_gap(data_gaps, status, *, gap_type, severity, message, affect_status=True) -> str`
  - å‘ data_gaps åˆ—è¡¨è¿½åŠ ä¸€æ¡è®°å½•
  - severity ä¸º `"block"` æ—¶ status å‡çº§ä¸º `"blocked"`
  - severity ä¸º `"warn"` ä¸”å½“å‰ status ä¸º `"ok"` æ—¶å‡çº§ä¸º `"degraded"`
  - `affect_status=False` æ—¶ä¸æ”¹å˜ status

**æ£€æŸ¥é¡¹ï¼š**
- ETF ä¸»è¡¨æ˜¯å¦ç¼ºå¤±ï¼ˆ`security_master_codes`ï¼‰
- universe ä¸­å“ªäº› ETF ç¼ºå°‘ä¸»è¡¨ / ç¼ºå°‘è¡Œæƒ…æ•°æ®
- å®è§‚æ—¶åºå¯ç”¨æ€§ï¼ˆ`bool(cfg.tushare_token)`ï¼‰
- å®è§‚æ–‡æœ¬å¯ç”¨æ€§ä¸æ–°é²œåº¦ï¼ˆ`macro_docs_available` + `macro_latest_date`ï¼‰
- æ–°é²œåº¦çŠ¶æ€ï¼š`ok` / `stale` / `future` / `unknown`
- åˆè§„æ–‡æœ¬å¯ç”¨æ€§ï¼ˆ`compliance_docs_available`ï¼‰
- æŒä»“æ•°æ®æ–°é²œåº¦ï¼ˆ`current_positions_date` ä¸ `asof_date` çš„å¤©æ•°å·®ï¼‰

**è¿”å›å€¼ï¼š**
```python
{
    "data_quality": {
        "status": "ok" | "degraded" | "blocked",
        "market": {"missing_etf_master": [...], "missing_market": [...]},
        "macro": {
            "timeseries_available": bool,
            "text_available": bool,
            "latest_date": str,
            "freshness_days": int | None,
            "freshness_status": "ok" | "stale" | "future" | "unknown",
        },
        "compliance": {"text_available": bool},
        "positions": {"freshness_days": int | None},
    },
    "data_gaps": [{"type": str, "severity": str, "message": str}, ...],
}
```

### æ–‡ä»¶4ï¼š`src/tools/snapshot.py`

**å®ç° `risk_snapshot_bundle(state, config) -> Dict[str, Any]`**ï¼ˆæ³¨æ„ï¼šå‡½æ•°åä¸æ˜¯ `snapshot_node`ï¼Œè¿”å› Dict ä¸æ˜¯ RiskStateï¼‰

**è®¡ç®—æŒ‡æ ‡ï¼ˆè¿”å› `{"snapshot_metrics": metrics}` å­—å…¸ï¼‰ï¼š**

| æŒ‡æ ‡ | è¯´æ˜ |
|:---|:---|
| `portfolio_volatility` | ç›®æ ‡æƒé‡çš„åŠ æƒæ³¢åŠ¨ç‡ |
| `current_portfolio_volatility` | å½“å‰æŒä»“çš„åŠ æƒæ³¢åŠ¨ç‡ |
| `delta_portfolio_volatility` | ç›®æ ‡ - å½“å‰æ³¢åŠ¨ç‡å·®å€¼ |
| `weighted_spread_bps` | åŠ æƒä¹°å–ä»·å·®ï¼ˆåŸºç‚¹ï¼‰ |
| `weighted_adv` | åŠ æƒæ—¥å‡æˆäº¤é¢ |
| `hhi` | ç›®æ ‡æƒé‡çš„ HHI é›†ä¸­åº¦ |
| `effective_n` | ç›®æ ‡æƒé‡çš„æœ‰æ•ˆæŒä»“æ•° |
| `top_weight` | ç›®æ ‡æƒé‡ä¸­æœ€å¤§å•ä¸€æƒé‡ |
| `current_hhi` / `current_effective_n` / `current_top_weight` | å½“å‰æŒä»“çš„å¯¹åº”æŒ‡æ ‡ |
| `delta_hhi` / `delta_effective_n` | ç›®æ ‡ - å½“å‰çš„å·®å€¼ |
| `turnover` | æ¢æ‰‹ç‡ = 0.5 Ã— Î£|target_w - current_w| |
| `max_position_delta` | æœ€å¤§å•ä¸€æŒä»“å˜åŠ¨ |
| `max_adv_ratio` | æœ€å¤§äº¤æ˜“é¢/ADV æ¯”ç‡ï¼ˆéœ€è¦ aumï¼‰ |
| `adv_by_symbol` | æ¯åª ETF çš„ ADV å­—å…¸ |
| `macro_severity` | åˆå§‹å€¼ 0ï¼ˆåç»­ç”±å®è§‚ Agent æ›´æ–°ï¼‰ |
| `missing_market_rows` | ç¼ºå°‘è¡Œæƒ…æ•°æ®çš„ ETF åˆ—è¡¨ |

**å…³é”®å®ç°ç»†èŠ‚ï¼š**
- ä½¿ç”¨ `normalize_weights` / `compute_hhi` / `compute_effective_n`ï¼ˆæ¥è‡ª utils.pyï¼‰
- ä½¿ç”¨ `market_metrics` / `lookback_start_date`ï¼ˆæ¥è‡ª csv_data.pyï¼‰
- aum ä¼˜å…ˆå– `normalized.aum`ï¼Œä¸º None æ—¶å– `cfg.default_aum`

</details>

<details>
<summary><b>ğŸ’» ä»£ç æ¨¡æ¿</b></summary>

### æ–‡ä»¶1ï¼š`src/tools/utils.py`

```python
"""Common utility functions for risk tools."""
from __future__ import annotations

from typing import Dict

# Tolerance constants for floating-point comparisons
WEIGHT_TOLERANCE = 1e-6
EPSILON = 1e-12


def normalize_weights(weights: Dict[str, float]) -> Dict[str, float]:
    """Normalize weights to sum to 1.0."""
    # TODO: ç”¨ float() æ¸…æ´—æ¯ä¸ªå€¼
    # TODO: è®¡ç®— totalï¼Œè‹¥ total <= 0 è¿”å›æ¸…æ´—åçš„åŸå€¼
    # TODO: å¦åˆ™æ¯ä¸ªå€¼é™¤ä»¥ total
    pass


def compute_hhi(weights: Dict[str, float], already_normalized: bool = False) -> float:
    """Compute HHI = Î£(w_iÂ²).

    Args:
        weights: {symbol: weight}
        already_normalized: True æ—¶è·³è¿‡å½’ä¸€åŒ–ï¼Œç›´æ¥å¯¹ values æ±‚å¹³æ–¹å’Œ
    """
    # TODO: already_normalized=True â†’ ç›´æ¥ sum(w**2 for w in weights.values())
    # TODO: already_normalized=False â†’ å…ˆç®— totalï¼Œå† sum((v/total)**2)
    # TODO: total <= 0 æ—¶è¿”å› 0.0
    pass


def compute_effective_n(weights: Dict[str, float], already_normalized: bool = False) -> float:
    """Compute effective number of holdings = 1 / HHI."""
    # TODO: è°ƒç”¨ compute_hhi
    # TODO: hhi > EPSILON æ—¶è¿”å› 1.0/hhiï¼Œå¦åˆ™è¿”å› 0.0
    pass


def weights_sum_to_one(weights: Dict[str, float]) -> bool:
    """Check if weights sum to 1.0 within WEIGHT_TOLERANCE."""
    # TODO: abs(sum - 1.0) <= WEIGHT_TOLERANCE
    pass
```

### æ–‡ä»¶2ï¼š`src/tools/csv_data.py`

```python
"""CSV æ•°æ®è¯»å–ä¸æŸ¥è¯¢æ¨¡å—"""
from __future__ import annotations

import json
import random
from functools import lru_cache
from pathlib import Path
from typing import Any, Dict, Iterable, List, Tuple

import numpy as np
import pandas as pd

from ..config import RuntimeConfig, DEFAULT_CONFIG

_ROOT = Path(__file__).resolve().parents[2]


def _data_dir(config: RuntimeConfig | None = None) -> Path:
    """è¿”å›æ•°æ®ç›®å½•è·¯å¾„"""
    cfg = config or DEFAULT_CONFIG
    # TODO: è‹¥ cfg.csv_data_dir æœ‰å€¼åˆ™ç”¨å®ƒï¼Œå¦åˆ™ç”¨ _ROOT / "cufel_practice_data"
    pass


def _load_csv(path: Path, *, usecols: Iterable[str] | None = None) -> pd.DataFrame:
    """é€šç”¨ CSV åŠ è½½ï¼Œæ–‡ä»¶ä¸å­˜åœ¨è¿”å›ç©º DataFrame"""
    # TODO: path ä¸å­˜åœ¨è¿”å› pd.DataFrame()ï¼Œå¦åˆ™ pd.read_csv
    pass


@lru_cache(maxsize=4)
def _load_etf_prices_cached(path_str: str) -> pd.DataFrame:
    """å¸¦ç¼“å­˜çš„ ETF è¡Œæƒ…åŠ è½½"""
    # TODO: è°ƒç”¨ _load_csv
    # TODO: è½¬æ¢åˆ—ç±»å‹ï¼šcodeâ†’str, dateâ†’datetime, æ•°å€¼åˆ—â†’numeric
    #   æ•°å€¼åˆ—: open, high, low, close, vol, amount, pre_close, change, pct_chg, adj_factor
    # TODO: dropna(subset=["date", "code"])
    pass


def load_etf_prices(config: RuntimeConfig | None = None) -> pd.DataFrame:
    """åŠ è½½ ETF è¡Œæƒ…æ•°æ®"""
    # TODO: æ‹¼æ¥è·¯å¾„ _data_dir(config) / "etf_2025_data.csv"ï¼Œè°ƒç”¨ cached ç‰ˆæœ¬
    pass


@lru_cache(maxsize=4)
def _load_etf_basic_cached(path_str: str) -> pd.DataFrame:
    """å¸¦ç¼“å­˜çš„ ETF åŸºæœ¬ä¿¡æ¯åŠ è½½"""
    # TODO: åŠ è½½ CSVï¼Œcode åˆ—è½¬ str
    pass


def load_etf_basic(config: RuntimeConfig | None = None) -> pd.DataFrame:
    """åŠ è½½ ETF åŸºæœ¬ä¿¡æ¯"""
    # TODO: æ‹¼æ¥è·¯å¾„ "sampled_etf_basic.csv"
    pass


@lru_cache(maxsize=4)
def _etf_industry_map_cached(path_str: str) -> Dict[str, str]:
    """æ„å»º {code: industry} æ˜ å°„"""
    # TODO: ä» _load_etf_basic_cached å– code + indx_csname ä¸¤åˆ—
    # TODO: éå†æ„å»ºæ˜ å°„å­—å…¸
    pass


def etf_industry_map(config: RuntimeConfig | None = None) -> Dict[str, str]:
    """è¿”å› ETF ä»£ç åˆ°è¡Œä¸šçš„æ˜ å°„"""
    pass


def etf_codes_by_industry(
    industry_names: Iterable[str], config: RuntimeConfig | None = None
) -> Dict[str, List[str]]:
    """æŒ‰è¡Œä¸šåæŸ¥ ETF ä»£ç ï¼Œè¿”å› {industry: [codes]}"""
    # TODO: è°ƒç”¨ etf_industry_mapï¼Œåè½¬æ˜ å°„
    pass


@lru_cache(maxsize=4)
def _load_compliance_docs_cached(path_str: str) -> pd.DataFrame:
    """å¸¦ç¼“å­˜çš„åˆè§„æ–‡æœ¬åŠ è½½"""
    # TODO: åŠ è½½ csrc_2025.csvï¼Œdateâ†’datetimeï¼Œtitle/content/fromâ†’str
    pass


def load_compliance_docs(config: RuntimeConfig | None = None) -> pd.DataFrame:
    pass


@lru_cache(maxsize=4)
def _load_macro_docs_cached(results_str: str, csv_str: str) -> pd.DataFrame:
    """åŠ è½½å®è§‚æ–‡æœ¬ï¼ˆä¼˜å…ˆ JSONï¼Œé™çº§ CSVï¼‰"""
    # TODO: ä¼˜å…ˆå°è¯• govcn_2025_results.json
    #   - è§£æ results åˆ—è¡¨ï¼Œæå– date/title/content/industry_name/sentiment_score
    # TODO: JSON ä¸å¯ç”¨æ—¶é™çº§åˆ° govcn_2025.csv
    pass


def load_macro_docs(config: RuntimeConfig | None = None) -> pd.DataFrame:
    pass


def security_master_codes(config: RuntimeConfig | None = None) -> Tuple[set, str]:
    """è¿”å› (æ‰€æœ‰å·²çŸ¥ ETF ä»£ç é›†åˆ, æ¥æºæ–‡ä»¶å)"""
    # TODO: ä¼˜å…ˆä» load_etf_basic å–ï¼Œé™çº§åˆ° load_etf_prices
    # TODO: éƒ½æ²¡æœ‰æ—¶è¿”å› (set(), "missing")
    pass


def sample_universe(
    asof_date: str, size: int, seed: str | None, config: RuntimeConfig | None = None
) -> List[str]:
    """éšæœºé‡‡æ · ETF ä»£ç ï¼ˆç”¨äºè’™ç‰¹å¡æ´›æ ¡å‡†ï¼‰"""
    # TODO: ä» load_etf_prices å– asof_date ä¹‹å‰çš„æ‰€æœ‰ code
    # TODO: ç”¨ random.Random(seed) é‡‡æ · size ä¸ª
    pass


def lookback_start_date(asof_date: str, lookback_days: int) -> str:
    """è®¡ç®—å›æº¯èµ·å§‹æ—¥æœŸ = asof_date - lookback_days"""
    pass


def previous_trading_date(asof_date: str, config: RuntimeConfig | None = None) -> str:
    """æŸ¥æ‰¾ asof_date ä¹‹å‰æœ€è¿‘çš„äº¤æ˜“æ—¥"""
    # TODO: ä» load_etf_prices ä¸­æ‰¾ date < asof_date çš„æœ€å¤§æ—¥æœŸ
    # TODO: æ‰¾ä¸åˆ°æ—¶è¿”å› asof_date æœ¬èº«
    pass


def market_metrics(
    codes: Iterable[str],
    start_date: str | None,
    end_date: str | None,
    config: RuntimeConfig | None = None,
) -> Dict[str, Dict[str, float]]:
    """è®¡ç®—æ¯åª ETF çš„ volatility / adv / spread_bps"""
    # TODO: ä» load_etf_prices ç­›é€‰ codes + æ—¥æœŸèŒƒå›´
    # TODO: è®¡ç®—å¤æƒæ”¶ç›Šç‡ ret = groupby("code")[adj_close].pct_change()
    # TODO: spread_bps = (high - low) / close * 10000
    # TODO: volatility = grouped["ret"].std(ddof=0)
    # TODO: adv = grouped["amount"].mean()
    # TODO: spread_bps = grouped["spread_bps"].mean()
    pass


def market_metrics_by_range(
    start_date: str, end_date: str, config: RuntimeConfig | None = None
) -> Tuple[List[str], Dict[str, Dict[str, float]]]:
    """æŒ‰æ—¥æœŸèŒƒå›´æŸ¥å…¨éƒ¨ ETF æŒ‡æ ‡"""
    # TODO: ç­›é€‰æ—¥æœŸèŒƒå›´å†…çš„ codesï¼Œè°ƒç”¨ market_metrics
    pass


def macro_docs_available(config: RuntimeConfig | None = None) -> bool:
    """å®è§‚æ–‡æœ¬æ˜¯å¦å¯ç”¨"""
    pass


def compliance_docs_available(config: RuntimeConfig | None = None) -> bool:
    """åˆè§„æ–‡æœ¬æ˜¯å¦å¯ç”¨"""
    pass


def _text_mask(df: pd.DataFrame, query: str, columns: Iterable[str]) -> pd.Series:
    """å…³é”®è¯åŒ¹é…æ©ç ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰"""
    # TODO: å¯¹æ¯ä¸ª column åš str.contains(query.lower())ï¼Œç”¨ | åˆå¹¶
    pass


def macro_search_hits(
    query: str,
    limit: int = 5,
    asof_date: str | None = None,
    config: RuntimeConfig | None = None,
) -> List[Dict[str, Any]]:
    """å…³é”®è¯æœç´¢å®è§‚æ–‡æœ¬ï¼Œè¿”å› [{date, title, summary, sentiment_score}]"""
    # TODO: åŠ è½½ macro_docsï¼ŒæŒ‰ asof_date æˆªæ–­ï¼Œç”¨ _text_mask è¿‡æ»¤
    pass


def compliance_search_hits(
    query: str, limit: int = 5, config: RuntimeConfig | None = None
) -> List[str]:
    """å…³é”®è¯æœç´¢åˆè§„æ–‡æœ¬ï¼Œè¿”å› content åˆ—è¡¨"""
    pass


def macro_latest_date(
    asof_date: str | None = None, config: RuntimeConfig | None = None
) -> str:
    """å®è§‚æ–‡æœ¬æœ€æ–°æ—¥æœŸ"""
    pass
```

### æ–‡ä»¶3ï¼š`src/tools/data_quality.py`

```python
"""æ•°æ®è´¨é‡æ£€æŸ¥æ¨¡å—"""
from __future__ import annotations

from datetime import datetime
from typing import Any, Dict, List

from ..config import RuntimeConfig, DEFAULT_CONFIG
from ..state import RiskState
from .csv_data import (
    compliance_docs_available,
    macro_docs_available,
    macro_latest_date,
    market_metrics,
    lookback_start_date,
    security_master_codes,
)


def _append_gap(
    data_gaps: List[Dict[str, Any]],
    status: str,
    *,
    gap_type: str,
    severity: str,
    message: str,
    affect_status: bool = True,
) -> str:
    """å‘ data_gaps è¿½åŠ ä¸€æ¡è®°å½•ï¼Œå¹¶æ ¹æ® severity å‡çº§ status"""
    # TODO: append {"type": gap_type, "severity": severity, "message": message}
    # TODO: affect_status=False æ—¶ç›´æ¥è¿”å›åŸ status
    # TODO: severity=="block" â†’ è¿”å› "blocked"
    # TODO: severity=="warn" ä¸” status=="ok" â†’ è¿”å› "degraded"
    # TODO: å…¶ä»–æƒ…å†µè¿”å›åŸ status
    pass


def check_data_quality(state: RiskState, config: RuntimeConfig | None = None) -> Dict[str, Any]:
    """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§ä¸æ–°é²œåº¦ï¼Œç”Ÿæˆ data_quality ä¸ data_gapsã€‚"""
    cfg = config or DEFAULT_CONFIG
    normalized = state.get("normalized") or {}
    universe = normalized.get("universe") or []
    asof_date = normalized.get("asof_date") or ""
    positions_date = normalized.get("current_positions_date") or ""

    data_gaps: List[Dict[str, Any]] = []
    status = "ok"

    # TODO: æ£€æŸ¥ ETF ä¸»è¡¨ï¼ˆsecurity_master_codesï¼‰
    # - ä¸»è¡¨ä¸ºç©ºæ—¶ _append_gap(severity="warn")

    # TODO: æ£€æŸ¥è¡Œæƒ…æ•°æ®ï¼ˆmarket_metricsï¼‰
    # - æ‰¾å‡º universe ä¸­ç¼ºå°‘ä¸»è¡¨ / ç¼ºå°‘è¡Œæƒ…çš„ ETF
    # - åˆ†åˆ« _append_gap

    # TODO: æ£€æŸ¥å®è§‚æ•°æ®
    # - timeseries_available = bool(cfg.tushare_token)
    # - text_available = macro_docs_available(cfg)
    # - latest_date = macro_latest_date(asof_date, cfg)
    # - è®¡ç®— freshness_days å’Œ freshness_status

    # TODO: æ£€æŸ¥åˆè§„æ•°æ®
    # - compliance_docs_available(cfg)

    # TODO: å…¨éƒ¨ universe ç¼ºè¡Œæƒ…æ—¶ severity="block"

    # TODO: æ£€æŸ¥æŒä»“æ•°æ®æ–°é²œåº¦
    # - asof_date ä¸ positions_date çš„å¤©æ•°å·®

    # TODO: ç»„è£… data_quality å­—å…¸å¹¶è¿”å›
    # return {"data_quality": {...}, "data_gaps": data_gaps}
    pass
```

### æ–‡ä»¶4ï¼š`src/tools/snapshot.py`

```python
"""æŒ‡æ ‡å¿«ç…§è®¡ç®—æ¨¡å—"""
from __future__ import annotations

from typing import Dict, Any

from ..state import RiskState
from ..config import RuntimeConfig, DEFAULT_CONFIG
from .csv_data import market_metrics, lookback_start_date
from .utils import normalize_weights, compute_hhi, compute_effective_n


def risk_snapshot_bundle(state: RiskState, config: RuntimeConfig | None = None) -> Dict[str, Any]:
    """åŸºäºè¾“å…¥æƒé‡ä¸è¡Œæƒ…æ•°æ®è®¡ç®—æŒ‡æ ‡å¿«ç…§ã€‚"""
    cfg = config or DEFAULT_CONFIG
    normalized = state.get("normalized") or {}
    target_weights = normalized.get("target_weights") or {}
    current_weights = normalized.get("current_positions") or {}
    asof_date = normalized.get("asof_date") or ""
    aum = normalized.get("aum")
    if aum is None:
        aum = cfg.default_aum
    lookback_days = int(cfg.market_lookback_days)
    start_date = lookback_start_date(asof_date, lookback_days)

    # TODO: è·å–è¡Œæƒ…æ•°æ®
    # codes = set(target_weights) | set(current_weights)
    # market = market_metrics(codes, start_date or asof_date, asof_date, cfg)

    # TODO: å½’ä¸€åŒ–æƒé‡
    # target_norm = normalize_weights(target_weights)
    # current_norm = normalize_weights(current_weights)

    # TODO: è®¡ç®—é›†ä¸­åº¦æŒ‡æ ‡ï¼ˆhhi, effective_n, top_weightï¼‰
    # - åˆ†åˆ«å¯¹ target_norm å’Œ current_norm è®¡ç®—

    # TODO: è®¡ç®—æ¢æ‰‹ç‡å’Œæœ€å¤§æŒä»“å˜åŠ¨
    # turnover = 0.5 * sum(|target_w - current_w|)

    # TODO: è®¡ç®—åŠ æƒæ³¢åŠ¨ç‡ã€ä»·å·®ã€ADV
    # - éå† target_normï¼Œä» market å–æ¯åª ETF çš„æŒ‡æ ‡
    # - weighted_vol += weight * volatility
    # - weighted_spread += weight * spread_bps
    # - weighted_adv += weight * adv

    # TODO: è®¡ç®— max_adv_ratioï¼ˆéœ€è¦ aumï¼‰
    # ratio = (trade_amount * aum) / adv

    # TODO: è®¡ç®—å½“å‰æŒä»“çš„æ³¢åŠ¨ç‡ current_vol

    # TODO: ç»„è£… metrics å­—å…¸ï¼ˆåŒ…å«ä¸Šè¿°æ‰€æœ‰æŒ‡æ ‡ + delta æŒ‡æ ‡ï¼‰
    # return {"snapshot_metrics": metrics}
    pass
```

</details>

<details>
<summary><b>ğŸ§ª æµ‹è¯•æ£€æŸ¥</b></summary>

### æµ‹è¯• utils.py
```python
from src.tools.utils import (
    normalize_weights, compute_hhi, compute_effective_n, weights_sum_to_one,
    WEIGHT_TOLERANCE, EPSILON,
)

# æµ‹è¯•å½’ä¸€åŒ–
weights = {"A": 3, "B": 7}
normalized = normalize_weights(weights)
assert abs(sum(normalized.values()) - 1.0) < 1e-6
assert abs(normalized["A"] - 0.3) < 1e-9

# æµ‹è¯• HHIï¼ˆå·²å½’ä¸€åŒ–ï¼‰
hhi = compute_hhi({"A": 0.5, "B": 0.5}, already_normalized=True)
assert abs(hhi - 0.5) < 1e-6

# æµ‹è¯• HHIï¼ˆæœªå½’ä¸€åŒ–ï¼‰
hhi2 = compute_hhi({"A": 3, "B": 7}, already_normalized=False)
assert abs(hhi2 - 0.58) < 1e-6  # (0.3^2 + 0.7^2)

# æµ‹è¯•æœ‰æ•ˆæŒä»“æ•°
effective_n = compute_effective_n({"A": 0.5, "B": 0.5}, already_normalized=True)
assert abs(effective_n - 2.0) < 1e-6

# æµ‹è¯• weights_sum_to_one
assert weights_sum_to_one({"A": 0.5, "B": 0.5}) is True
assert weights_sum_to_one({"A": 0.3, "B": 0.4}) is False

print("âœ… utils.py æµ‹è¯•é€šè¿‡")
```

### æµ‹è¯• csv_data.py
```python
from src.tools.csv_data import (
    load_etf_prices, load_etf_basic, load_compliance_docs, load_macro_docs,
    security_master_codes, previous_trading_date, market_metrics,
    macro_docs_available, compliance_docs_available,
)

# æµ‹è¯•æ•°æ®åŠ è½½
df = load_etf_prices()
assert not df.empty
assert "code" in df.columns and "date" in df.columns

# æµ‹è¯• ETF åŸºæœ¬ä¿¡æ¯
basic = load_etf_basic()
assert not basic.empty

# æµ‹è¯• security_master_codes
codes, source = security_master_codes()
assert len(codes) > 0

# æµ‹è¯• previous_trading_date
prev = previous_trading_date("2025-11-15")
assert prev  # åº”è¿”å›éç©ºå­—ç¬¦ä¸²

# æµ‹è¯• market_metrics
metrics = market_metrics(list(codes)[:3], "2025-01-01", "2025-11-15")
for code, m in metrics.items():
    assert "volatility" in m and "adv" in m and "spread_bps" in m

# æµ‹è¯•å¯ç”¨æ€§æ£€æŸ¥
print(f"macro_docs_available: {macro_docs_available()}")
print(f"compliance_docs_available: {compliance_docs_available()}")

print("âœ… csv_data.py æµ‹è¯•é€šè¿‡")
```

### æµ‹è¯•å®Œæ•´æµç¨‹
```bash
uv run --env-file .env -- python -u -m pytest tests/test_module3.py -v
```

</details>

<details>
<summary><b>ğŸ’¡ æç¤ºä¸æŠ€å·§</b></summary>

**å…³é”®ç‚¹ï¼š**
1. **HHI è®¡ç®—**ï¼šæ³¨æ„æƒé‡éœ€è¦å…ˆå½’ä¸€åŒ–
2. **æ³¢åŠ¨ç‡è®¡ç®—**ï¼šä½¿ç”¨ `pandas.std()` è®¡ç®—æ ‡å‡†å·®
3. **æ•°æ®æˆªæ–­**ï¼šä½¿ç”¨ `df[df['date'] <= asof_date]`

**å¸¸è§é”™è¯¯ï¼š**
- âŒ å¿˜è®°å¤„ç†ç©º DataFrame
- âŒ æ—¥æœŸæ¯”è¾ƒæ—¶ç±»å‹ä¸åŒ¹é…ï¼ˆstr vs datetimeï¼‰
- âŒ é™¤é›¶é”™è¯¯ï¼ˆHHI ä¸º 0 æ—¶ï¼‰

**æ€§èƒ½ä¼˜åŒ–ï¼š**
- ä½¿ç”¨ Pandas å‘é‡åŒ–æ“ä½œè€Œä¸æ˜¯å¾ªç¯
- ç¼“å­˜ ETF æ•°æ®é¿å…é‡å¤è¯»å–

**å‚è€ƒèµ„æºï¼š**
- [Pandas å®˜æ–¹æ–‡æ¡£](https://pandas.pydata.org/docs/)
- [HHI æŒ‡æ•°è¯´æ˜](https://en.wikipedia.org/wiki/Herfindahl%E2%80%93Hirschman_index)

</details>

---

## ğŸ“ æ¨¡å—4: ç¼–æ’è°ƒåº¦

<details open>
<summary><b>ğŸ“‹ æ¨¡å—ä¿¡æ¯</b></summary>

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **éš¾åº¦** | â­â­â­â­ é«˜çº§ |
| **æ–‡ä»¶æ•°** | 3ä¸ª |
| **æ–‡ä»¶è·¯å¾„** | `src/graph.py`<br>`src/chains/gatekeeper.py`<br>`src/chains/supervisor.py` |
| **ä¾èµ–æ¨¡å—** | æ¨¡å—1, 2, 3 |
| **è¢«ä¾èµ–** | æ¨¡å—5, 6 |

</details>

<details>
<summary><b>ğŸ¯ å­¦ä¹ ç›®æ ‡</b></summary>

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ å°†æŒæ¡ï¼š
- [ ] LangGraph å·¥ä½œæµç¼–æ’
- [ ] DAGï¼ˆæœ‰å‘æ— ç¯å›¾ï¼‰è®¾è®¡
- [ ] Send API å®ç°å¹¶è¡Œæ‰§è¡Œ
- [ ] LLM ä½œä¸ºè°ƒåº¦å™¨çš„åº”ç”¨
- [ ] æ¡ä»¶è·¯ç”±é€»è¾‘

</details>

<details>
<summary><b>ğŸ“– åŠŸèƒ½è¯´æ˜</b></summary>

æœ¬æ¨¡å—æ˜¯ç³»ç»Ÿçš„æ ¸å¿ƒç¼–æ’å±‚ï¼Œè´Ÿè´£åè°ƒæ‰€æœ‰èŠ‚ç‚¹çš„æ‰§è¡Œé¡ºåºå’Œå¹¶è¡Œè°ƒåº¦ã€‚

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
1. **å·¥ä½œæµç¼–æ’**ï¼šä½¿ç”¨ LangGraph æ„å»º DAG
2. **æ•°æ®å¯ç”¨æ€§æ£€æŸ¥**ï¼šGatekeeper åŸºäºæ•°æ®å¯ç”¨æ€§è£å‰ªå€™é€‰èŠ‚ç‚¹
3. **ä¸šåŠ¡é€»è¾‘è°ƒåº¦**ï¼šSupervisor åŸºäºä¸šåŠ¡é€»è¾‘ä»å€™é€‰èŠ‚ç‚¹ä¸­é€‰æ‹©éœ€è¦è¿è¡Œçš„èŠ‚ç‚¹
4. **å¹¶è¡Œæ‰§è¡Œ**ï¼šä½¿ç”¨ Send API å®ç°çœŸæ­£çš„å¹¶è¡Œ

**ä¸ºä»€ä¹ˆé‡è¦ï¼š**
ç¼–æ’å±‚å†³å®šäº†ç³»ç»Ÿçš„æ‰§è¡Œæ•ˆç‡å’Œçµæ´»æ€§ï¼Œæ˜¯ MAS æ¶æ„çš„å…³é”®ã€‚

</details>

<details>
<summary><b>âœ… å®ç°è¦æ±‚</b></summary>

### æ–‡ä»¶1ï¼š`src/graph.py`

**å®ç° `build_graph(llm=None, config=None)` å‡½æ•°**ï¼ˆæ³¨æ„ï¼šå‡½æ•°åæ˜¯ `build_graph` ä¸æ˜¯ `create_graph`ï¼‰

**å…³é”®å†…éƒ¨å‡½æ•°ï¼š**
- `_should_run_node(state, name) -> bool`ï¼šæ£€æŸ¥ `state["pending_agents"]` ä¸­æ˜¯å¦åŒ…å«è¯¥èŠ‚ç‚¹ï¼Œä¸” `stop_condition` ä¸º False
- `_guarded_node(name, fn)`ï¼šåŒ…è£…åˆ†æèŠ‚ç‚¹ï¼Œä¸åœ¨ pending åˆ—è¡¨ä¸­æ—¶è¿”å› `{f"finding_{name}": None}`
- `dispatch_to_parallel(state) -> List[Send]`ï¼šæ ¹æ® `pending_agents` åˆ—è¡¨ç”¨ Send API å¹¶è¡Œåˆ†å‘

**èŠ‚ç‚¹æ³¨å†Œæ–¹å¼ï¼š**
- æ‰€æœ‰èŠ‚ç‚¹ç”¨ `RunnableLambda` åŒ…è£…å `add_node`
- åˆ†æèŠ‚ç‚¹ï¼ˆ6ä¸ªï¼‰ç»Ÿä¸€ç”¨ `_guarded_node` åŒ…è£…
- æ¯ä¸ªèŠ‚ç‚¹å‡½æ•°æ˜¯ä¸€ä¸ª lambdaï¼Œå°† `cfg` / `llm` é€šè¿‡é—­åŒ…ä¼ å…¥å¯¹åº”çš„ chain/agent å‡½æ•°

**è¾¹çš„è¿æ¥ï¼š**
- ä¸²è¡Œç®¡é“ï¼švalidate â†’ data_quality â†’ snapshot â†’ gatekeeper â†’ supervisor
- æ¡ä»¶è¾¹ï¼šsupervisor â†’ `dispatch_to_parallel`ï¼ˆSend API å¹¶è¡Œåˆ†å‘ï¼‰
- æ‰€æœ‰åˆ†æèŠ‚ç‚¹ â†’ reducer
- åå¤„ç†ç®¡é“ï¼šreducer â†’ constraints â†’ decision â†’ solver â†’ audit â†’ END

### æ–‡ä»¶2ï¼š`src/chains/gatekeeper.py`

**å®ç° `gatekeeper_chain(state) -> Dict[str, Any]`**ï¼ˆæ³¨æ„ï¼šå‡½æ•°åæ˜¯ `gatekeeper_chain`ï¼Œè¿”å› Dictï¼‰

**é€»è¾‘ï¼š**
1. æ£€æŸ¥ `validation.is_valid`ï¼šä¸º False æ—¶è®¾ `stop_condition=True`
2. æ£€æŸ¥ `data_quality.status`ï¼šä¸º `"blocked"` æ—¶è®¾ `stop_condition=True`
3. éåœæ­¢æ—¶æ„å»ºå€™é€‰åˆ—è¡¨ï¼š4 ä¸ªç¡®å®šæ€§é“¾å§‹ç»ˆåŠ å…¥ï¼Œmacro/compliance æŒ‰æ•°æ®å¯ç”¨æ€§æ¡ä»¶åŠ å…¥
4. è¿”å› `{candidate_nodes, stop_condition, gatekeeper_used, gatekeeper_rationale}`

### æ–‡ä»¶3ï¼š`src/chains/supervisor.py`

**å®ç° `supervisor_chain(state, llm, candidates, config) -> Dict[str, Any]`**ï¼ˆæ³¨æ„ï¼šå‡½æ•°åå’Œå‚æ•°ï¼‰

**è¾…åŠ©å‡½æ•°ï¼š**
- `_fallback_result(candidates, *, used, rationale) -> Dict`ï¼šé™çº§ç»“æœï¼Œç›´æ¥ç”¨å…¨éƒ¨å€™é€‰
- `_normalize_nodes(nodes, candidates) -> List[str]`ï¼šè¿‡æ»¤ LLM è¾“å‡ºï¼Œåªä¿ç•™å€™é€‰ä¸­çš„èŠ‚ç‚¹

**é€»è¾‘ï¼š**
1. `cfg.enable_supervisor` ä¸º False æ—¶è¿”å› fallbackï¼ˆå…¨éƒ¨å€™é€‰ï¼‰
2. `llm is None` æ—¶è¿”å› fallback
3. åŠ è½½ `load_skill("supervisor-router")`ï¼Œæ„å»º system_prompt
4. å°† candidates + validation + data_quality + snapshot_metrics + rule_findings + policy_profile æ‰“åŒ…ä¸º JSON å‘ç»™ LLM
5. è§£æ LLM è¾“å‡ºçš„ `nodes_to_run` å’Œ `rationale`
6. ç”¨ `_normalize_nodes` è¿‡æ»¤ï¼Œç©ºåˆ—è¡¨æ—¶é™çº§å›å…¨éƒ¨å€™é€‰
7. è¿”å› `{nodes_to_run, pending_agents, supervisor_used, supervisor_rationale, supervisor_model}`

</details>

<details>
<summary><b>ğŸ’» ä»£ç æ¨¡æ¿</b></summary>

### æ–‡ä»¶1ï¼š`src/graph.py`

```python
"""LangGraph å·¥ä½œæµç¼–æ’æ¨¡å—"""
from __future__ import annotations

from typing import Any, Dict, List, Literal

from langgraph.graph import StateGraph, END
from langgraph.types import Send
from langchain_core.runnables import RunnableLambda

from .state import RiskState
from .config import RuntimeConfig, DEFAULT_CONFIG
from .tools import (
    validate_and_normalize,
    check_data_quality,
    risk_snapshot_bundle,
    constraints_evaluator,
    decision_engine,
    constraint_solver,
    audit_log,
)
from .chains import (
    gatekeeper_chain,
    supervisor_chain,
    market_risk_chain,
    concentration_chain,
    diversification_chain,
    liquidity_chain,
    reducer_chain,
)
from .agents import run_macro_agent, run_compliance_agent


def _should_run_node(state: RiskState, name: str) -> bool:
    """Check if a node should run based on pending_agents list."""
    # TODO: stop_condition ä¸º True æ—¶è¿”å› False
    # TODO: æ£€æŸ¥ name æ˜¯å¦åœ¨ state["pending_agents"] ä¸­
    pass


def build_graph(llm=None, config: RuntimeConfig | None = None):
    """æ„å»ºå¹¶è¿”å›ä¸»å·¥ä½œæµå›¾ã€‚"""
    cfg = config or DEFAULT_CONFIG
    g = StateGraph(RiskState)

    # TODO: å®šä¹‰ pipeline èŠ‚ç‚¹å‡½æ•°ï¼ˆç”¨é—­åŒ…ä¼ å…¥ cfgï¼‰
    # def validate_node(state): return validate_and_normalize(state, cfg)
    # def data_quality_node(state): return check_data_quality(state, cfg)
    # def snapshot_node(state): return risk_snapshot_bundle(state, cfg)
    # ... ç±»ä¼¼å®šä¹‰ constraints_node, gatekeeper_node, supervisor_node

    # TODO: å®šä¹‰åˆ†æèŠ‚ç‚¹å­—å…¸
    # analysis_nodes = {"market": lambda s: market_risk_chain(s, cfg), ...}
    # agent_nodes = {"macro": lambda s: run_macro_agent(s, llm, cfg), ...}
    # all_analysis_nodes = {**analysis_nodes, **agent_nodes}

    # TODO: å®ç° _guarded_node(name, fn) åŒ…è£…å™¨
    # - ä¸åœ¨ pending_agents ä¸­æ—¶è¿”å› {f"finding_{name}": None}
    # - åœ¨åˆ—è¡¨ä¸­æ—¶æ­£å¸¸è°ƒç”¨ fn(state)

    # TODO: å®šä¹‰åå¤„ç†èŠ‚ç‚¹ï¼ˆreducer, decision, solver, auditï¼‰

    # TODO: å®ç° dispatch_to_parallel(state) -> List[Send]
    # - stop_condition æˆ– pending ä¸ºç©ºæ—¶ â†’ [Send("reducer", state)]
    # - å¦åˆ™ä¸ºæ¯ä¸ª pending èŠ‚ç‚¹åˆ›å»º Send å¯¹è±¡

    # TODO: æ³¨å†Œæ‰€æœ‰èŠ‚ç‚¹ï¼ˆç”¨ RunnableLambda åŒ…è£…ï¼‰
    # g.add_node("validate", RunnableLambda(validate_node))
    # åˆ†æèŠ‚ç‚¹ç”¨ _guarded_node åŒ…è£…

    # TODO: æ·»åŠ è¾¹
    # g.set_entry_point("validate")
    # g.add_edge("validate", "data_quality")
    # ... ä¸²è¡Œç®¡é“
    # g.add_conditional_edges("supervisor", dispatch_to_parallel)
    # for name in all_analysis_nodes: g.add_edge(name, "reducer")
    # ... åå¤„ç†ç®¡é“ â†’ END

    return g.compile()
```

### æ–‡ä»¶2ï¼š`src/chains/gatekeeper.py`

```python
"""å‰ç½®æ£€æŸ¥æ¨¡å—ï¼ˆGatekeeperï¼‰"""
from __future__ import annotations

from typing import Any, Dict, List

from ..state import RiskState


def gatekeeper_chain(state: RiskState) -> Dict[str, Any]:
    """å‰ç½®æ£€æŸ¥ï¼ŒåŸºäºæ•°æ®å¯ç”¨æ€§å’ŒéªŒè¯ç»“æœè£å‰ªå€™é€‰èŠ‚ç‚¹ã€‚"""
    validation = state.get("validation") or {}
    data_quality = state.get("data_quality") or {}

    stop_condition = False
    rationale: List[str] = []

    # TODO: æ£€æŸ¥ validation.is_valid
    # - ä¸º False æ—¶è®¾ stop_condition=Trueï¼Œrationale è¿½åŠ  "validation_failed"

    # TODO: æ£€æŸ¥ data_quality.status
    # - ä¸º "blocked" æ—¶è®¾ stop_condition=Trueï¼Œrationale è¿½åŠ  "data_quality_blocked"

    # TODO: éåœæ­¢æ—¶æ„å»ºå€™é€‰åˆ—è¡¨
    # - 4 ä¸ªç¡®å®šæ€§é“¾å§‹ç»ˆåŠ å…¥: market, concentration, diversification, liquidity
    # - macro: ä»…å½“ data_quality["macro"]["timeseries_available"] ä¸º True
    # - compliance: ä»…å½“ data_quality["compliance"]["text_available"] ä¸º True

    # TODO: è¿”å›å­—å…¸
    # return {
    #     "candidate_nodes": candidates,
    #     "stop_condition": stop_condition,
    #     "gatekeeper_used": True,
    #     "gatekeeper_rationale": "; ".join(rationale) if rationale else "ok",
    # }
    pass
```

### æ–‡ä»¶3ï¼š`src/chains/supervisor.py`

```python
"""ä¸šåŠ¡é€»è¾‘èŠ‚ç‚¹é€‰æ‹©æ¨¡å—ï¼ˆSupervisorï¼‰"""
from __future__ import annotations

import json
from typing import Any, Dict, List

from langchain_core.messages import HumanMessage, SystemMessage

from ..state import RiskState
from ..config import RuntimeConfig, DEFAULT_CONFIG
from ..skills_runtime import load_skill, build_system_prompt, validate_output

_BASE_PROMPT = (
    "ä½ æ˜¯ç³»ç»Ÿè°ƒåº¦å‘˜ï¼Œè´Ÿè´£å†³å®šè¿è¡Œå“ªäº›åˆ†æèŠ‚ç‚¹ã€‚"
    "åªèƒ½ä»æä¾›çš„å€™é€‰åˆ—è¡¨ä¸­é€‰æ‹©ã€‚"
    "å§‹ç»ˆè¿”å› JSONï¼ŒåŒ…å« keys: nodes_to_run (list of strings) å’Œ rationale (string)ã€‚"
    "rationale å¿…é¡»ä¸ºä¸­æ–‡ã€‚"
)


def _llm_model_name(llm) -> str:
    return str(getattr(llm, "model_name", None) or getattr(llm, "model", None) or "")


def _fallback_result(candidates: List[str], *, used: bool, rationale: str) -> Dict[str, Any]:
    """é™çº§ç»“æœï¼šç›´æ¥ç”¨å…¨éƒ¨å€™é€‰èŠ‚ç‚¹"""
    # TODO: è¿”å› {nodes_to_run, pending_agents, supervisor_used, supervisor_rationale}
    pass


def _normalize_nodes(nodes: List[str], candidates: List[str]) -> List[str]:
    """è¿‡æ»¤ LLM è¾“å‡ºï¼Œåªä¿ç•™å€™é€‰ä¸­çš„èŠ‚ç‚¹"""
    # TODO: åªä¿ç•™ nodes ä¸­å­˜åœ¨äº candidates çš„å…ƒç´ 
    pass


def supervisor_chain(
    state: RiskState, llm, candidates: List[str], config: RuntimeConfig | None = None
) -> Dict[str, Any]:
    """åŸºäºä¸šåŠ¡é€»è¾‘ä»å€™é€‰èŠ‚ç‚¹ä¸­é€‰æ‹©éœ€è¦è¿è¡Œçš„èŠ‚ç‚¹ã€‚"""
    cfg = config or DEFAULT_CONFIG

    # TODO: stop_condition æ—¶è¿”å›ç©ºå­—å…¸
    # TODO: enable_supervisor ä¸º False æ—¶è¿”å› _fallback_result
    # TODO: llm is None æ—¶è¿”å› _fallback_result

    # TODO: åŠ è½½ skill = load_skill("supervisor-router")
    # TODO: æ„å»º system_prompt = build_system_prompt(_BASE_PROMPT, skill)

    # TODO: æ„å»º payloadï¼ˆcandidates + validation + data_quality + snapshot_metrics + ...ï¼‰
    # TODO: è°ƒç”¨ llm.invoke([SystemMessage(...), HumanMessage(...)])

    # TODO: è§£æ JSON è¾“å‡ºï¼Œvalidate_output æ ¡éªŒ
    # TODO: _normalize_nodes è¿‡æ»¤ï¼Œç©ºåˆ—è¡¨æ—¶é™çº§å›å…¨éƒ¨å€™é€‰

    # TODO: è¿”å› {nodes_to_run, pending_agents, supervisor_used, supervisor_rationale, supervisor_model}
    pass
```

</details>

<details>
<summary><b>ğŸ§ª æµ‹è¯•æ£€æŸ¥</b></summary>

### æµ‹è¯• Gatekeeper
```python
from src.chains.gatekeeper import gatekeeper_chain

# æµ‹è¯•1: macro ä¸å¯ç”¨æ—¶è¢«è£å‰ª
state = {
    "validation": {"is_valid": True},
    "data_quality": {
        "status": "ok",
        "macro": {"timeseries_available": False},
        "compliance": {"text_available": True},
    },
}
result = gatekeeper_chain(state)
assert "macro" not in result["candidate_nodes"]
assert "compliance" in result["candidate_nodes"]
assert result["stop_condition"] is False

# æµ‹è¯•2: validation å¤±è´¥æ—¶ stop
state2 = {
    "validation": {"is_valid": False},
    "data_quality": {"status": "ok"},
}
result2 = gatekeeper_chain(state2)
assert result2["stop_condition"] is True
assert result2["candidate_nodes"] == []

print("âœ… Gatekeeper æµ‹è¯•é€šè¿‡")
```

### æµ‹è¯•å®Œæ•´å·¥ä½œæµ
```bash
# è¿è¡Œå®Œæ•´å·¥ä½œæµ
uv run --env-file .env -- python -u -m src.app
```

**æ£€æŸ¥é¡¹ï¼š**
- [ ] æ‰€æœ‰èŠ‚ç‚¹éƒ½å·²æ·»åŠ 
- [ ] è¾¹çš„è¿æ¥æ­£ç¡®
- [ ] å¹¶è¡Œæ‰§è¡Œæ­£å¸¸å·¥ä½œ
- [ ] æ¡ä»¶è·¯ç”±æ­£ç¡®

</details>

<details>
<summary><b>ğŸ’¡ æç¤ºä¸æŠ€å·§</b></summary>

**å…³é”®ç‚¹ï¼š**
1. **Send API ç”¨æ³•**ï¼š
   ```python
   return [Send("node_name", state) for node in nodes_to_run]
   ```

2. **æ¡ä»¶è¾¹**ï¼šä½¿ç”¨ `add_conditional_edges` å®ç°æ¡ä»¶è·¯ç”±

3. **LLM è°ƒç”¨**ï¼šä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºç¡®ä¿è¿”å›æ ¼å¼æ­£ç¡®

**å¸¸è§é”™è¯¯ï¼š**
- âŒ å¿˜è®°æ·»åŠ  END èŠ‚ç‚¹
- âŒ å¾ªç¯ä¾èµ–å¯¼è‡´ DAG æ— æ•ˆ
- âŒ Send API ä½¿ç”¨ä¸å½“å¯¼è‡´å¹¶è¡Œå¤±è´¥

**è°ƒè¯•æŠ€å·§ï¼š**
- ä½¿ç”¨ `graph.get_graph().draw_mermaid()` å¯è§†åŒ–å·¥ä½œæµ
- æ‰“å°æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å…¥è¾“å‡º
- ä½¿ç”¨ LangSmith è¿½è¸ªæ‰§è¡Œè¿‡ç¨‹

**å‚è€ƒèµ„æºï¼š**
- [LangGraph å®˜æ–¹æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)
- [Send API ç¤ºä¾‹](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/)

</details>

---

## ğŸ“ æ¨¡å—5: åˆ†æé“¾è·¯

<details open>
<summary><b>ğŸ“‹ æ¨¡å—ä¿¡æ¯</b></summary>

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **éš¾åº¦** | â­â­â­ ä¸­çº§ |
| **æ–‡ä»¶æ•°** | 5ä¸ª |
| **æ–‡ä»¶è·¯å¾„** | `src/chains/common.py`<br>`src/chains/market.py`<br>`src/chains/concentration.py`<br>`src/chains/diversification.py`<br>`src/chains/liquidity.py` |
| **ä¾èµ–æ¨¡å—** | æ¨¡å—1, 3, 7, 8 |
| **è¢«ä¾èµ–** | æ¨¡å—4ï¼ˆgraph ä¸­æ³¨å†Œï¼‰ |

</details>

<details>
<summary><b>ğŸ¯ å­¦ä¹ ç›®æ ‡</b></summary>

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ å°†æŒæ¡ï¼š
- [ ] ç¡®å®šæ€§é£é™©åˆ†æé“¾çš„è®¾è®¡æ¨¡å¼
- [ ] è§„åˆ™é˜ˆå€¼çš„åº”ç”¨
- [ ] é£é™©ç­‰çº§åˆ¤æ–­é€»è¾‘
- [ ] è¯æ®æ”¶é›†å’Œå»ºè®®ç”Ÿæˆ

</details>

<details>
<summary><b>ğŸ“– åŠŸèƒ½è¯´æ˜</b></summary>

æœ¬æ¨¡å—å®ç°4æ¡ç¡®å®šæ€§åˆ†æé“¾ï¼ŒåŸºäºè§„åˆ™å’ŒæŒ‡æ ‡è¿›è¡Œé£é™©è¯„ä¼°ã€‚

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
1. **å¸‚åœºé£é™©**ï¼šåŸºäºç»„åˆæ³¢åŠ¨ç‡
2. **é›†ä¸­åº¦é£é™©**ï¼šåŸºäº HHI æŒ‡æ•°
3. **åˆ†æ•£åº¦é£é™©**ï¼šåŸºäºæœ‰æ•ˆæŒä»“æ•°
4. **æµåŠ¨æ€§é£é™©**ï¼šåŸºäºä¹°å–ä»·å·®å’Œæˆäº¤é‡

**ä¸ºä»€ä¹ˆé‡è¦ï¼š**
ç¡®å®šæ€§é“¾æä¾›å¿«é€Ÿã€å¯é çš„é£é™©è¯„ä¼°ï¼Œæ˜¯ç³»ç»Ÿçš„åŸºç¡€åˆ†æå±‚ã€‚

</details>

<details>
<summary><b>âœ… å®ç°è¦æ±‚</b></summary>

### é€šç”¨è¦æ±‚

æ¯ä¸ªåˆ†æé“¾éƒ½éµå¾ªç›¸åŒæ¨¡å¼ï¼š
1. ä» `state["snapshot_metrics"]` è·å–æŒ‡æ ‡
2. ä» `state["normalized"]["policy_profile"]` è·å–è§„åˆ™é…ç½®åï¼ˆé»˜è®¤ `"default"`ï¼‰
3. ç”¨ `load_rules_cached(profile, config)` åŠ è½½é˜ˆå€¼
4. æ¯”è¾ƒæŒ‡æ ‡ä¸é˜ˆå€¼ï¼Œåˆ¤æ–­ severityï¼ˆ0/1/2ï¼‰
5. æ„å»º `Finding` TypedDictï¼ˆåŒ…å« agent/risk_type/severity/summary/metrics/evidenceï¼‰
6. è°ƒç”¨ `validate_finding(skill_name, finding, label)` åš Schema æ ¡éªŒ
7. è¿”å› `{f"finding_{risk_type}": finding}`ï¼ˆDictï¼Œä¸æ˜¯ RiskStateï¼‰

### æ–‡ä»¶0ï¼š`src/chains/common.py`

**å®ç° 2 ä¸ªå…±äº«å‡½æ•°ï¼š**
- `load_rules_cached(profile, config) -> Dict`ï¼šè°ƒç”¨ `load_rules(profile, config)` å¹¶è¿”å›è§„åˆ™å­—å…¸
- `validate_finding(skill_name, finding, label) -> None`ï¼šè°ƒç”¨ `validate_output(load_skill(skill_name), finding)`ï¼Œæœ‰é”™è¯¯æ—¶ raise RuntimeError

### æ–‡ä»¶1ï¼š`src/chains/market.py`

**å®ç° `market_risk_chain(state, config) -> Dict[str, Any]`**

- æŒ‡æ ‡ï¼š`portfolio_volatility`
- é˜ˆå€¼ï¼š`volatility_warn`, `volatility_restrict`
- åˆ¤æ–­ï¼švol >= restrict â†’ severity=2, vol >= warn â†’ severity=1, å¦åˆ™ 0

### æ–‡ä»¶2ï¼š`src/chains/concentration.py`

**å®ç° `concentration_chain(state, config) -> Dict[str, Any]`**

- æŒ‡æ ‡ï¼š`hhi` + `top_weight`ï¼ˆä¸¤ä¸ªæŒ‡æ ‡å–ä¸¥æ ¼çš„ï¼‰
- é˜ˆå€¼ï¼š`hhi_warn/restrict`, `top_weight_warn/restrict`
- åˆ¤æ–­ï¼šä»»ä¸€ >= restrict â†’ 2, ä»»ä¸€ >= warn â†’ 1

### æ–‡ä»¶3ï¼š`src/chains/diversification.py`

**å®ç° `diversification_chain(state, config) -> Dict[str, Any]`**

- æŒ‡æ ‡ï¼š`effective_n`ï¼ˆåå‘æŒ‡æ ‡ï¼Œè¶Šå°è¶Šå±é™©ï¼‰
- é˜ˆå€¼ï¼š`effective_n_warn`, `effective_n_restrict`
- åˆ¤æ–­ï¼šn <= restrict â†’ 2, n <= warn â†’ 1

### æ–‡ä»¶4ï¼š`src/chains/liquidity.py`

**å®ç° `liquidity_chain(state, config) -> Dict[str, Any]`**

- æŒ‡æ ‡ï¼š`weighted_spread_bps`ï¼ˆæ­£å‘ï¼‰+ `weighted_adv`ï¼ˆåå‘ï¼‰
- é˜ˆå€¼ï¼š`spread_warn/restrict`, `adv_warn/restrict`
- åˆ¤æ–­ï¼šspread >= restrict æˆ– adv <= restrict â†’ 2

</details>

<details>
<summary><b>ğŸ’» ä»£ç æ¨¡æ¿</b></summary>

ç”±äº4ä¸ªåˆ†æé“¾ç»“æ„ç›¸ä¼¼ï¼Œè¿™é‡Œå…ˆæä¾› common.py å’Œ market.py å®Œæ•´æ¨¡æ¿ï¼Œå…¶ä½™3ä¸ªå‚ç…§ä¿®æ”¹ã€‚

### æ–‡ä»¶0ï¼š`src/chains/common.py`

```python
"""åˆ†æé“¾å…±äº«å·¥å…·"""
from __future__ import annotations

from typing import Any, Dict

from ..skills_runtime import load_skill, validate_output
from ..tools.rules import load_rules
from ..config import RuntimeConfig, DEFAULT_CONFIG


def load_rules_cached(profile: str, config: RuntimeConfig | None = None) -> Dict[str, Any]:
    """åŠ è½½æŒ‡å®š profile çš„è§„åˆ™é˜ˆå€¼"""
    # TODO: è°ƒç”¨ load_rules(profile, config or DEFAULT_CONFIG)ï¼Œè¿”å›è§„åˆ™å­—å…¸
    pass


def validate_finding(skill_name: str, finding: Dict[str, Any], label: str) -> None:
    """ç”¨ Skill çš„ output.schema.json æ ¡éªŒ Finding ç»“æ„"""
    # TODO: errors = validate_output(load_skill(skill_name), finding)
    # TODO: æœ‰é”™è¯¯æ—¶ raise RuntimeError(f"{label} skill output invalid: {errors}")
    pass
```

### æ–‡ä»¶1ï¼š`src/chains/market.py`ï¼ˆå®Œæ•´æ¨¡æ¿ï¼‰

```python
"""å¸‚åœºé£é™©åˆ†ææ¨¡å—"""
from __future__ import annotations

from typing import Dict, Any

from ..state import RiskState, Finding
from .common import load_rules_cached, validate_finding
from ..config import RuntimeConfig


def market_risk_chain(state: RiskState, config: RuntimeConfig | None = None) -> Dict[str, Any]:
    """å¸‚åœºé£é™©åˆ†æï¼ˆåŸºäºæ³¢åŠ¨ç‡ï¼‰"""
    metrics = state.get("snapshot_metrics") or {}
    vol = float(metrics.get("portfolio_volatility", 0.0))

    # TODO: è·å– policy_profileï¼ŒåŠ è½½è§„åˆ™é˜ˆå€¼
    # profile = (state.get("normalized") or {}).get("policy_profile", "default")
    # rules = load_rules_cached(profile, config)
    # vol_warn = float(rules.get("volatility_warn", 0.15))
    # vol_restrict = float(rules.get("volatility_restrict", 0.25))

    # TODO: åˆ¤æ–­ severityï¼ˆ0/1/2ï¼‰

    # TODO: æ„å»º Findingï¼ˆåŒ…å« agent, risk_type, severity, summary, metrics, evidenceï¼‰

    # TODO: validate_finding("risk-market-assessor", finding, "market")

    # TODO: return {"finding_market": finding}
    pass
```

**å…¶ä½™3ä¸ªæ–‡ä»¶ï¼ˆconcentration / diversification / liquidityï¼‰ç»“æ„å®Œå…¨ç›¸åŒï¼Œåªéœ€ä¿®æ”¹ï¼š**
- å‡½æ•°åï¼š`concentration_chain` / `diversification_chain` / `liquidity_chain`
- æŒ‡æ ‡åå’Œé˜ˆå€¼åï¼ˆå‚è§å®ç°è¦æ±‚ï¼‰
- Finding çš„ `agent` / `risk_type` å­—æ®µ
- liquidity çš„ skill_name ä¸º `"liquidity-execution-assessor"`ï¼Œå…¶ä½™ä¸º `"risk-market-assessor"`

</details>

<details>
<summary><b>ğŸ§ª æµ‹è¯•æ£€æŸ¥</b></summary>

### å•å…ƒæµ‹è¯•ç¤ºä¾‹

```python
from src.chains.market import market_risk_chain
from src.chains.concentration import concentration_chain

# æµ‹è¯• market_risk_chain
state = {
    "snapshot_metrics": {"portfolio_volatility": 0.10},
    "normalized": {"policy_profile": "default"},
}
result = market_risk_chain(state)
assert "finding_market" in result
assert result["finding_market"]["risk_type"] == "market"
assert result["finding_market"]["severity"] >= 0

# æµ‹è¯• concentration_chain
state2 = {
    "snapshot_metrics": {"hhi": 0.8, "top_weight": 0.9},
    "normalized": {"policy_profile": "default"},
}
result2 = concentration_chain(state2)
assert result2["finding_concentration"]["severity"] == 2

print("âœ… åˆ†æé“¾è·¯æµ‹è¯•é€šè¿‡")
```

### é›†æˆæµ‹è¯•

```bash
# æµ‹è¯•æ‰€æœ‰åˆ†æé“¾
uv run --env-file .env -- python -u -m pytest tests/test_module5.py -v
```

**æ£€æŸ¥é¡¹ï¼š**
- [ ] æ‰€æœ‰4ä¸ªæ–‡ä»¶éƒ½å·²å®ç°
- [ ] é£é™©ç­‰çº§åˆ¤æ–­æ­£ç¡®
- [ ] è¯æ®æ•°æ®å®Œæ•´
- [ ] å»ºè®®æ–‡æœ¬åˆç†

</details>

<details>
<summary><b>ğŸ’¡ æç¤ºä¸æŠ€å·§</b></summary>

**å…³é”®ç‚¹ï¼š**
1. **é£é™©ç­‰çº§æ˜ å°„**ï¼š
   - 0: passï¼ˆæ— é£é™©ï¼‰
   - 1: warnï¼ˆé¢„è­¦ï¼‰
   - 2: restrictï¼ˆé™åˆ¶ï¼‰
   - 3: blockï¼ˆé˜»æ–­ï¼Œé€šå¸¸ç”±åˆè§„è§¦å‘ï¼‰

2. **åå‘æŒ‡æ ‡å¤„ç†**ï¼š
   - æœ‰æ•ˆæŒä»“æ•°ã€æˆäº¤é‡æ˜¯"è¶Šå¤§è¶Šå¥½"
   - åˆ¤æ–­é€»è¾‘éœ€è¦åå‘ï¼š`< threshold` è€Œä¸æ˜¯ `>`

3. **è¯æ®æ”¶é›†**ï¼š
   - è®°å½•å½“å‰å€¼å’Œé˜ˆå€¼
   - æä¾›è®¡ç®—ä¾æ®

**å¸¸è§é”™è¯¯ï¼š**
- âŒ åå‘æŒ‡æ ‡åˆ¤æ–­é€»è¾‘é”™è¯¯
- âŒ å¿˜è®°å¤„ç†æŒ‡æ ‡ç¼ºå¤±çš„æƒ…å†µ
- âŒ é£é™©æ‘˜è¦ä¸å¤Ÿå…·ä½“

**ä¼˜åŒ–å»ºè®®ï¼š**
- ä½¿ç”¨è¾…åŠ©å‡½æ•°ç»Ÿä¸€åˆ¤æ–­é€»è¾‘
- é£é™©æ‘˜è¦ä½¿ç”¨æ¨¡æ¿å­—ç¬¦ä¸²
- å»ºè®®è¦å…·ä½“å¯æ‰§è¡Œ

**å‚è€ƒèµ„æºï¼š**
- æ ·æœ¬ç­”æ¡ˆä¸­çš„ `src/chains/` ç›®å½•
- `rules.yaml` ä¸­çš„é˜ˆå€¼é…ç½®

</details>

---

## ğŸ“ æ¨¡å—6: Agent æ¨¡å—

<details open>
<summary><b>ğŸ“‹ æ¨¡å—ä¿¡æ¯</b></summary>

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **éš¾åº¦** | â­â­â­â­ é«˜çº§ |
| **æ–‡ä»¶æ•°** | 3ä¸ª |
| **æ–‡ä»¶è·¯å¾„** | `src/agents/agent_utils.py`<br>`src/agents/macro_agent.py`<br>`src/agents/compliance_agent.py` |
| **ä¾èµ–æ¨¡å—** | æ¨¡å—1, 3, 7 |
| **è¢«ä¾èµ–** | æ¨¡å—4ï¼ˆgraph ä¸­æ³¨å†Œï¼‰ |

</details>

<details>
<summary><b>ğŸ¯ å­¦ä¹ ç›®æ ‡</b></summary>

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ å°†æŒæ¡ï¼š
- [ ] LangChain Agent çš„æ„å»ºæ–¹æ³•
- [ ] å·¥å…·è°ƒç”¨ï¼ˆTool Callingï¼‰çš„å®ç°
- [ ] ReAct Agent æ¨¡å¼
- [ ] ç»“æ„åŒ–è¾“å‡ºçš„åº”ç”¨
- [ ] RAG æ£€ç´¢æŠ€æœ¯

</details>

<details>
<summary><b>ğŸ“– åŠŸèƒ½è¯´æ˜</b></summary>

æœ¬æ¨¡å—å®ç°2ä¸ª LLM Agentï¼Œä½¿ç”¨å·¥å…·è°ƒç”¨è¿›è¡Œæ™ºèƒ½åˆ†æã€‚

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
1. **å®è§‚ Agent**ï¼šè°ƒç”¨ Tushare API å’Œæ–‡æœ¬æ£€ç´¢ï¼Œè¯„ä¼°å®è§‚é£é™©
2. **åˆè§„ Agent**ï¼šä½¿ç”¨ RAG æ£€ç´¢æ”¿ç­–æ–‡æœ¬ï¼Œ**å¿…é¡»å¼•ç”¨æ–‡æ¡£**åè¾“å‡ºç»“æ„åŒ–åˆè§„ç»“è®º

**ä¸ºä»€ä¹ˆé‡è¦ï¼š**
Agent èƒ½å¤Ÿå¤„ç†éç»“æ„åŒ–æ•°æ®å’Œå¤æ‚æ¨ç†ï¼Œæ˜¯ç³»ç»Ÿçš„æ™ºèƒ½åˆ†æå±‚ã€‚

</details>

<details>
<summary><b>âœ… å®ç°è¦æ±‚</b></summary>

### æ–‡ä»¶0ï¼š`src/agents/agent_utils.py`

**å®ç° 4 ä¸ª Agent å…±äº«å·¥å…·å‡½æ•°ï¼š**

1. `extract_tool_calls(messages) -> List[Dict]`
   - ä» LangChain æ¶ˆæ¯åˆ—è¡¨ä¸­æå–å·¥å…·è°ƒç”¨è®°å½•
   - éå† AIMessage æ”¶é›† pending è°ƒç”¨ï¼Œéå† ToolMessage åŒ¹é…ç»“æœ
   - è¿”å› `[{tool, input, output, latency_ms, error}]`

2. `last_ai_content(messages) -> str`
   - ä»æ¶ˆæ¯åˆ—è¡¨ä¸­å–æœ€åä¸€æ¡ AIMessage çš„ content
   - content ä¸º list æ—¶ json.dumpsï¼Œå¦åˆ™ str

3. `wrap_tool(name, fn)`
   - ç”¨ `@tool(name)` è£…é¥°å™¨åŒ…è£…æ™®é€šå‡½æ•°ä¸º LangChain Tool
   - è‡ªåŠ¨è®°å½• latency_ms å’Œ error åˆ° `result["tool_meta"]`

4. `_parse_tool_output(content) -> Any`ï¼ˆå†…éƒ¨è¾…åŠ©ï¼‰
   - å°è¯• json.loads è§£æå­—ç¬¦ä¸²ï¼Œå¤±è´¥åˆ™åŸæ ·è¿”å›

### æ–‡ä»¶1ï¼š`src/agents/macro_agent.py`

**å®ç°å®è§‚ç»æµåˆ†æ Agentï¼ˆçº¦ 19 ä¸ªå‡½æ•°ï¼‰ï¼š**

**å·¥å…·å®ç°å±‚ï¼ˆ5 ä¸ªï¼‰ï¼š**
- `_macro_timeseries_impl(series, asof_date, runtime) -> dict`ï¼šè°ƒç”¨ Tushare API è·å–æ—¶åºæ•°æ®
- `_macro_search_impl(query, asof_date, runtime) -> dict`ï¼šå…³é”®è¯æœç´¢å®è§‚æ–‡æœ¬
- `_create_tools_with_asof_date(asof_date, runtime)`ï¼šå·¥å‚å‡½æ•°ï¼Œç”¨é—­åŒ…ç»‘å®š asof_date åˆ›å»ºçº¿ç¨‹å®‰å…¨çš„å·¥å…·
- `_tushare_timeseries_from_config(series, config, asof_date, runtime)`ï¼šä» macro_series.yaml é…ç½®é©±åŠ¨ Tushare è°ƒç”¨
- `_load_macro_series_config(path_str)`ï¼šå¸¦ `@lru_cache` åŠ è½½ YAML é…ç½®

**ç¡®å®šæ€§è®¡ç®—å±‚ï¼ˆ5 ä¸ªï¼‰ï¼š**
- `_prefetch_macro_timeseries(asof_date, runtime)`ï¼šé¢„å–æ‰€æœ‰æ—¶åºæ•°æ®
- `_compute_macro_severity(tool_results, runtime)`ï¼šåŸºäºå˜åŒ–ç‡è®¡ç®—ç¡®å®šæ€§ severity
- `_nlp_severity_from_tool_calls(tool_calls)`ï¼šä»æ–‡æœ¬æœç´¢ç»“æœçš„ sentiment_score è®¡ç®— NLP severity
- `_blend_severity(macro_severity, nlp_severity, runtime)`ï¼šåŠ æƒæ··åˆä¸¤ä¸ª severity
- `_fallback_finding(severity) -> Finding`ï¼šé™çº§ç»“æœ

**èŠ‚ç‚¹å‡½æ•°ï¼š**
- `run_macro_agent(state, llm, config) -> Dict[str, Any]`ï¼šå®Œæ•´èŠ‚ç‚¹å…¥å£

### æ–‡ä»¶2ï¼š`src/agents/compliance_agent.py`

**å®ç°åˆè§„é£é™©åˆ†æ Agentï¼ˆçº¦ 25 ä¸ªå‡½æ•°ï¼‰ï¼š**

**RAG æ£€ç´¢å±‚ï¼ˆ8 ä¸ªï¼‰ï¼š**
- `_get_embedding_model(runtime)`ï¼šè·å– embedding æ¨¡å‹ï¼ˆtext-embedding-v4 æˆ–å…¼å®¹æ¥å£ï¼‰
- `_embed_texts(texts, model) -> List[List[float]]`ï¼šæ‰¹é‡æ–‡æœ¬å‘é‡åŒ–
- `_cosine_similarity(a, b) -> float`ï¼šä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
- `_vector_search(query, docs, model, top_k) -> List`ï¼šå‘é‡æ£€ç´¢
- `_keyword_search(query, docs, top_k) -> List`ï¼šå…³é”®è¯é™çº§æ£€ç´¢
- `_policy_search_impl(query, asof_date, runtime) -> dict`ï¼šåˆè§„æ–‡æœ¬ RAG æ£€ç´¢ï¼ˆå‘é‡ä¼˜å…ˆï¼Œå…³é”®è¯é™çº§ï¼‰
- `_allowlist_check_impl(codes, runtime) -> dict`ï¼šç¦æŠ•æ¸…å•æ£€æŸ¥
- `_create_tools_with_context(asof_date, runtime)`ï¼šå·¥å‚å‡½æ•°åˆ›å»ºå·¥å…·

**ç¡®å®šæ€§è®¡ç®—å±‚ï¼ˆ4 ä¸ªï¼‰ï¼š**
- `_compute_compliance_severity(blocklist_result, runtime) -> int`ï¼šåŸºäºç¦æŠ•æ¸…å•è®¡ç®— severity
- `_infer_industry_from_universe(universe, runtime) -> List[str]`ï¼šä» ETF ä»£ç æ¨æ–­è¡Œä¸š
- `_fallback_finding(severity) -> Finding`ï¼šé™çº§ç»“æœ
- `_prefetch_compliance_data(state, runtime)`ï¼šé¢„å–åˆè§„æ•°æ®

**èŠ‚ç‚¹å‡½æ•°ï¼š**
- `run_compliance_agent(state, llm, config) -> Dict[str, Any]`ï¼šå®Œæ•´èŠ‚ç‚¹å…¥å£
- **ç¡¬çº¦æŸ**ï¼šAgent å¿…é¡»è°ƒç”¨ `policy_search`ï¼Œå¦åˆ™ç»“æœæ— æ•ˆï¼Œé™çº§å› fallback

</details>

<details>
<summary><b>ğŸ’» ä»£ç æ¨¡æ¿</b></summary>

### æ–‡ä»¶0ï¼š`src/agents/agent_utils.py`

```python
"""Agent å…±äº«å·¥å…·å‡½æ•°"""
from __future__ import annotations

import json
import time
from typing import Any, Dict, List, Sequence, Callable

from langchain_core.messages import AIMessage, ToolMessage
from langchain_core.tools import tool


def _parse_tool_output(content: Any) -> Any:
    """å°è¯• JSON è§£æå­—ç¬¦ä¸²ï¼Œå¤±è´¥åˆ™åŸæ ·è¿”å›"""
    # TODO: isinstance(content, str) æ—¶å°è¯• json.loads
    pass


def extract_tool_calls(messages: Sequence[Any]) -> List[Dict[str, Any]]:
    """ä»æ¶ˆæ¯åˆ—è¡¨ä¸­æå–å·¥å…·è°ƒç”¨è®°å½•"""
    # TODO: éå† messages
    # - AIMessage: æ”¶é›† tool_calls åˆ° pending å­—å…¸ï¼ˆkey=call_idï¼‰
    # - ToolMessage: ç”¨ tool_call_id åŒ¹é… pendingï¼Œç»„è£… {tool, input, output, latency_ms, error}
    pass


def last_ai_content(messages: Sequence[Any]) -> str:
    """å–æœ€åä¸€æ¡ AIMessage çš„ content"""
    # TODO: reversed éå†ï¼Œæ‰¾åˆ° AIMessage è¿”å›å…¶ content
    # TODO: content ä¸º list æ—¶ json.dumps
    pass


def wrap_tool(name: str, fn: Callable[..., Dict[str, Any]]):
    """åŒ…è£…æ™®é€šå‡½æ•°ä¸º LangChain Toolï¼Œè‡ªåŠ¨è®°å½• latency å’Œ error"""
    @tool(name)
    def _wrapped(*args, **kwargs) -> Dict[str, Any]:
        # TODO: è®°å½• start = time.monotonic()
        # TODO: try/except è°ƒç”¨ fnï¼Œæ•è·å¼‚å¸¸è®°å½• error
        # TODO: è®¡ç®— latency_ms
        # TODO: å†™å…¥ result["tool_meta"] = {"latency_ms": ..., "error": ...}
        pass
    return _wrapped
```

### æ–‡ä»¶1ï¼š`src/agents/macro_agent.py`

```python
"""å®è§‚ç»æµåˆ†æ Agent"""
from __future__ import annotations

import hashlib
import json
import time
from datetime import datetime, timezone, timedelta
from functools import lru_cache
from pathlib import Path
from typing import Any

from langchain.agents import create_agent
import tushare as ts
import yaml

from .agent_utils import extract_tool_calls, last_ai_content, wrap_tool
from ..state import RiskState, Finding
from ..tools.csv_data import macro_search_hits
from ..config import RuntimeConfig, DEFAULT_CONFIG
from ..skills_runtime import load_skill, build_system_prompt, filter_tools, validate_output

_ROOT = Path(__file__).resolve().parents[2]


def _provenance(source: str, params: dict[str, Any]) -> dict[str, Any]:
    """ç”Ÿæˆæ•°æ®æº¯æºä¿¡æ¯ï¼ˆsource + timestamp + params_hashï¼‰"""
    # TODO: å®ç°
    pass


def _macro_series_path(config: RuntimeConfig) -> Path:
    """è¿”å› macro_series.yaml çš„è·¯å¾„"""
    # TODO: ä¼˜å…ˆ config.macro_series_configï¼Œé™çº§åˆ° csv_data_dir / "macro_series.yaml"
    pass


@lru_cache(maxsize=8)
def _load_macro_series_config(path_str: str) -> dict[str, Any]:
    """åŠ è½½ macro_series.yaml é…ç½®"""
    # TODO: è¯»å– YAMLï¼Œæ ¡éªŒ "series" é”®å­˜åœ¨ä¸”éç©º
    pass


def _parse_date(value: Any) -> datetime | None:
    """è§£ææ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ”¯æŒ YYYY-MM-DD / YYYYMMDD / ISO æ ¼å¼"""
    pass


def _tushare_timeseries_from_config(
    series: str, config: dict[str, Any], asof_date: str, runtime: RuntimeConfig
) -> tuple[dict[str, Any], str | None]:
    """ä» Tushare API è·å–æ—¶åºæ•°æ®ï¼ˆé…ç½®é©±åŠ¨ï¼‰"""
    # TODO: ä» config æå– api/params/fields/date_field/value_field ç­‰
    # TODO: è°ƒç”¨ tushare pro_api
    # TODO: å¤„ç† bid/ask ä¸­é—´ä»·ã€æ—¥æœŸåç§»ã€é™ˆæ—§æ£€æµ‹
    # TODO: è¿”å› (payload, error)
    pass


def _macro_timeseries_impl(series: str, asof_date: str, runtime: RuntimeConfig) -> dict[str, Any]:
    """å·¥å…·å®ç°ï¼šè·å–å®è§‚æ—¶åºæ•°æ®"""
    # TODO: åŠ è½½é…ç½®ï¼Œè°ƒç”¨ _tushare_timeseries_from_config
    # TODO: é™„åŠ  provenance ä¿¡æ¯
    pass


def _macro_search_impl(query: str, asof_date: str, runtime: RuntimeConfig) -> dict[str, Any]:
    """å·¥å…·å®ç°ï¼šæœç´¢å®è§‚æ–‡æœ¬"""
    # TODO: è°ƒç”¨ macro_search_hits(query, limit=5, asof_date=asof_date, config=runtime)
    pass


def _create_tools_with_asof_date(asof_date: str, runtime: RuntimeConfig):
    """å·¥å‚å‡½æ•°ï¼šç”¨é—­åŒ…ç»‘å®š asof_date åˆ›å»ºçº¿ç¨‹å®‰å…¨çš„å·¥å…·"""
    # TODO: å®šä¹‰ timeseries_impl å’Œ search_impl é—­åŒ…
    # TODO: ç”¨ wrap_tool åŒ…è£…å¹¶è¿”å›
    pass


def _fallback_finding(severity: int) -> Finding:
    """é™çº§ç»“æœ"""
    # TODO: è¿”å›åŒ…å« agent/risk_type/severity/summary/metrics/evidence/recommendations çš„ Finding
    pass


def _prefetch_macro_timeseries(
    asof_date: str, runtime: RuntimeConfig
) -> tuple[list[dict[str, Any]], dict[str, Any]]:
    """é¢„å–æ‰€æœ‰æ—¶åºæ•°æ®ï¼Œè¿”å› (tool_calls, results)"""
    # TODO: éå† macro_series.yaml ä¸­çš„æ‰€æœ‰ series
    # TODO: é€ä¸ªè°ƒç”¨ _macro_timeseries_implï¼Œè®°å½• tool_calls
    pass


def _compute_macro_severity(tool_results: dict[str, Any], runtime: RuntimeConfig) -> int:
    """åŸºäºæ—¶åºæ•°æ®å˜åŒ–ç‡è®¡ç®—ç¡®å®šæ€§ severity"""
    # TODO: éå†æ¯ä¸ª series çš„ values
    # TODO: è®¡ç®—æœ€è¿‘ä¸¤ä¸ªå€¼çš„å˜åŒ–ç‡ï¼ˆpct æˆ– abs æ¨¡å¼ï¼‰
    # TODO: ä¸ warn_pct_change / restrict_pct_change æ¯”è¾ƒ
    pass


def _nlp_severity_from_tool_calls(tool_calls: list[dict[str, Any]]) -> int | None:
    """ä»æ–‡æœ¬æœç´¢ç»“æœçš„ sentiment_score è®¡ç®— NLP severity"""
    # TODO: éå† macro_search çš„ tool_calls
    # TODO: æå– hits ä¸­çš„ sentiment_score
    # TODO: score >= 70 æˆ– <= 30 â†’ severity=2, 60-70 æˆ– 30-40 â†’ severity=1
    pass


def _blend_severity(macro_severity: int, nlp_severity: int | None, runtime: RuntimeConfig) -> int:
    """åŠ æƒæ··åˆæ—¶åº severity å’Œ NLP severity"""
    # TODO: nlp_severity ä¸º None æ—¶ç›´æ¥è¿”å› macro_severity
    # TODO: å¦åˆ™ round(weight * macro + (1-weight) * nlp)ï¼Œclamp åˆ° [0,3]
    pass


def run_macro_agent(state: RiskState, llm, config: RuntimeConfig | None = None) -> dict[str, Any]:
    """å®è§‚ Agent èŠ‚ç‚¹å…¥å£"""
    runtime = config or DEFAULT_CONFIG

    # TODO: 1. æ£€æŸ¥ data_quality.macro.timeseries_availableï¼Œä¸å¯ç”¨æ—¶è¿”å› fallback

    # TODO: 2. é¢„å–æ—¶åºæ•°æ®ï¼Œè®¡ç®—ç¡®å®šæ€§ severity
    # prefetched_calls, prefetched_results = _prefetch_macro_timeseries(asof_date, runtime)
    # macro_severity = _compute_macro_severity(prefetched_results, runtime)

    # TODO: 3. llm is None æ—¶è¿”å›ç¡®å®šæ€§ç»“æœ

    # TODO: 4. åŠ è½½ Skillï¼Œåˆ›å»ºå·¥å…·ï¼Œåˆ›å»º Agent
    # skill = load_skill("macro-tool-calling")
    # tools = filter_tools([macro_timeseries, macro_search], skill.allowlist)
    # agent = create_agent(llm, tools, system_prompt=build_system_prompt("", skill))

    # TODO: 5. è°ƒç”¨ Agentï¼Œè§£æ JSON è¾“å‡º
    # TODO: 6. validate_output æ ¡éªŒï¼Œå¤±è´¥æ—¶é™çº§
    # TODO: 7. è®¡ç®— nlp_severityï¼Œblend_severity
    # TODO: 8. ç»„è£… Findingï¼Œè¿”å› {finding_macro, tool_calls_macro, llm_used_macro, ...}
    pass
```

### æ–‡ä»¶2ï¼š`src/agents/compliance_agent.py`

```python
"""åˆè§„é£é™©åˆ†æ Agent"""
from __future__ import annotations

import json
import time
from typing import Any, Dict, List

from langchain.agents import create_agent

from .agent_utils import extract_tool_calls, last_ai_content, wrap_tool
from ..state import RiskState, Finding
from ..tools.csv_data import (
    compliance_search_hits,
    load_compliance_docs,
    etf_industry_map,
)
from ..config import RuntimeConfig, DEFAULT_CONFIG
from ..skills_runtime import load_skill, build_system_prompt, filter_tools, validate_output


def _get_embedding_model(runtime: RuntimeConfig):
    """è·å– embedding æ¨¡å‹ï¼ˆç”¨äºå‘é‡æ£€ç´¢ï¼‰"""
    # TODO: æ ¹æ® runtime.openai_base_url å’Œ runtime.embedding_model åˆ›å»ºæ¨¡å‹
    # TODO: æ— é…ç½®æ—¶è¿”å› None
    pass


def _embed_texts(texts: List[str], model) -> List[List[float]]:
    """æ‰¹é‡æ–‡æœ¬å‘é‡åŒ–"""
    # TODO: è°ƒç”¨ model.embed_documents(texts)
    pass


def _cosine_similarity(a: List[float], b: List[float]) -> float:
    """ä½™å¼¦ç›¸ä¼¼åº¦"""
    # TODO: dot(a,b) / (norm(a) * norm(b))
    pass


def _vector_search(query: str, docs: List[Dict], model, top_k: int) -> List[Dict]:
    """å‘é‡æ£€ç´¢"""
    # TODO: embed queryï¼Œè®¡ç®—ä¸æ¯ä¸ª doc çš„ç›¸ä¼¼åº¦ï¼Œå– top_k
    pass


def _keyword_search(query: str, docs: List[Dict], top_k: int) -> List[Dict]:
    """å…³é”®è¯é™çº§æ£€ç´¢"""
    # TODO: è°ƒç”¨ compliance_search_hits
    pass


def _policy_search_impl(query: str, asof_date: str, runtime: RuntimeConfig) -> dict:
    """åˆè§„æ–‡æœ¬ RAG æ£€ç´¢ï¼ˆå‘é‡ä¼˜å…ˆï¼Œå…³é”®è¯é™çº§ï¼‰"""
    # TODO: å°è¯• _vector_searchï¼Œå¤±è´¥æ—¶é™çº§åˆ° _keyword_search
    # TODO: è¿”å› {query, hits: [{title, content, score}], provenance}
    pass


def _allowlist_check_impl(codes: List[str], runtime: RuntimeConfig) -> dict:
    """ç¦æŠ•æ¸…å•æ£€æŸ¥"""
    # TODO: ä» rules.yaml åŠ è½½ blocklist
    # TODO: æ£€æŸ¥ codes ä¸­å“ªäº›åœ¨ blocklist ä¸­
    # TODO: è¿”å› {blocked: [...], clean: [...], provenance}
    pass


def _create_tools_with_context(asof_date: str, runtime: RuntimeConfig):
    """å·¥å‚å‡½æ•°ï¼šç”¨é—­åŒ…åˆ›å»ºå·¥å…·"""
    # TODO: ç±»ä¼¼ macro_agent çš„ _create_tools_with_asof_date
    pass


def _fallback_finding(severity: int) -> Finding:
    """é™çº§ç»“æœ"""
    pass


def _compute_compliance_severity(blocklist_result: dict, runtime: RuntimeConfig) -> int:
    """åŸºäºç¦æŠ•æ¸…å•è®¡ç®— severity"""
    # TODO: æœ‰ blocked é¡¹æ—¶ severity=3(block)
    pass


def run_compliance_agent(state: RiskState, llm, config: RuntimeConfig | None = None) -> dict[str, Any]:
    """åˆè§„ Agent èŠ‚ç‚¹å…¥å£"""
    runtime = config or DEFAULT_CONFIG

    # TODO: 1. æ£€æŸ¥ data_quality.compliance.text_availableï¼Œä¸å¯ç”¨æ—¶è¿”å› fallback

    # TODO: 2. é¢„å–åˆè§„æ•°æ®ï¼ˆç¦æŠ•æ¸…å•æ£€æŸ¥ï¼‰

    # TODO: 3. llm is None æ—¶è¿”å›ç¡®å®šæ€§ç»“æœ

    # TODO: 4. åŠ è½½ Skillï¼Œåˆ›å»ºå·¥å…·ï¼Œåˆ›å»º Agent
    # skill = load_skill("compliance-evidence")
    # tools = filter_tools([policy_search, allowlist_check], skill.allowlist)

    # TODO: 5. è°ƒç”¨ Agentï¼Œè§£æ JSON è¾“å‡º

    # TODO: 6. ç¡¬çº¦æŸæ£€æŸ¥ï¼šAgent å¿…é¡»è°ƒç”¨ policy_search
    # if not any(call["tool"] == "policy_search" for call in tool_calls):
    #     return fallback_finding(...)

    # TODO: 7. validate_output æ ¡éªŒ
    # TODO: 8. ç»„è£… Findingï¼Œè¿”å› {finding_compliance, tool_calls_compliance, ...}
    pass
```

</details>

<details>
<summary><b>ğŸ§ª æµ‹è¯•æ£€æŸ¥</b></summary>

### æµ‹è¯•å·¥å…·å‡½æ•°

```python
from src.agents.macro_agent import macro_timeseries, macro_search

# æµ‹è¯•æ—¶åºæ•°æ®è·å–
data = macro_timeseries("SHIBOR3M", "2025-01-01", "2025-11-15")
assert data is not None

# æµ‹è¯•æ–‡æœ¬æœç´¢
results = macro_search("åˆ©ç‡æ”¿ç­–", top_k=3)
assert len(results) <= 3

print("âœ… å·¥å…·å‡½æ•°æµ‹è¯•é€šè¿‡")
```

### æµ‹è¯• Agent

```python
from src.agents.macro_agent import run_macro_agent

state = {
    "normalized": {
        "asof_date": "2025-11-14",
        "target_weights": {"159213": 0.5, "159959": 0.5},
        "policy_profile": "default",
    },
    "data_quality": {
        "macro": {"timeseries_available": True},
    },
    "snapshot_metrics": {},
}

# æ—  LLM æ—¶åº”è¿”å›ç¡®å®šæ€§ç»“æœ
result = run_macro_agent(state, llm=None)
assert "finding_macro" in result
assert result["finding_macro"]["risk_type"] == "macro"
assert result["llm_used_macro"] is False

print("âœ… Agent æµ‹è¯•é€šè¿‡")
```

**æ£€æŸ¥é¡¹ï¼š**
- [ ] å·¥å…·å‡½æ•°æ­£å¸¸å·¥ä½œ
- [ ] Agent èƒ½æ­£ç¡®è°ƒç”¨å·¥å…·
- [ ] è¾“å‡ºæ ¼å¼ç¬¦åˆè¦æ±‚
- [ ] severity åˆ¤æ–­åˆç†

</details>

<details>
<summary><b>ğŸ’¡ æç¤ºä¸æŠ€å·§</b></summary>

**å…³é”®ç‚¹ï¼š**
1. **å·¥å…·å®šä¹‰**ï¼šä½¿ç”¨ `@tool` è£…é¥°å™¨ï¼Œæä¾›æ¸…æ™°çš„ docstring
2. **Agent åˆ›å»º**ï¼š
   ```python
   from langgraph.prebuilt import create_react_agent
   agent = create_react_agent(llm, tools=[tool1, tool2])
   ```
3. **ç»“æ„åŒ–è¾“å‡º**ï¼šä½¿ç”¨ Pydantic æ¨¡å‹æˆ– JSON Schema çº¦æŸè¾“å‡ºæ ¼å¼

**å¸¸è§é”™è¯¯ï¼š**
- âŒ å·¥å…· docstring ä¸æ¸…æ™°å¯¼è‡´ LLM è¯¯ç”¨
- âŒ å¿˜è®°å¤„ç† Agent æ‰§è¡Œå¤±è´¥çš„æƒ…å†µ
- âŒ è¾“å‡ºè§£æé€»è¾‘ä¸å¥å£®

**ä¼˜åŒ–å»ºè®®ï¼š**
- ä½¿ç”¨ Skills ä½“ç³»ç®¡ç†æç¤ºè¯
- æ·»åŠ å·¥å…·è°ƒç”¨æ—¥å¿—
- è®¾ç½® Agent æœ€å¤§è¿­ä»£æ¬¡æ•°

**å‚è€ƒèµ„æºï¼š**
- [LangChain Agent æ–‡æ¡£](https://python.langchain.com/docs/modules/agents/)
- [Tool Calling æŒ‡å—](https://python.langchain.com/docs/modules/agents/tools/)
- æ ·æœ¬ç­”æ¡ˆä¸­çš„ `src/agents/` ç›®å½•

</details>

---

## ğŸ“ æ¨¡å—7: Skills ä½“ç³»

<details open>
<summary><b>ğŸ“‹ æ¨¡å—ä¿¡æ¯</b></summary>

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **éš¾åº¦** | â­â­ è¿›é˜¶ |
| **æ–‡ä»¶æ•°** | 1ä¸ª Python + é…ç½®æ–‡ä»¶ |
| **æ–‡ä»¶è·¯å¾„** | `src/skills_runtime.py`<br>`skills/*/SKILL.md`ï¼ˆå·²æä¾›ï¼Œæ— éœ€ç¼–å†™ï¼‰<br>`skills/*/output.schema.json`ï¼ˆå·²æä¾›ï¼‰<br>`skills/tools/tool_interfaces.yaml`ï¼ˆå·²æä¾›ï¼‰<br>`skills/snippets/*.md`ï¼ˆå·²æä¾›ï¼‰ |
| **ä¾èµ–æ¨¡å—** | æ—  |
| **è¢«ä¾èµ–** | æ¨¡å—4ï¼ˆsupervisor ç”¨ load_skillï¼‰, æ¨¡å—6ï¼ˆAgent ç”¨å…¨éƒ¨å‡½æ•°ï¼‰ |

</details>

<details>
<summary><b>ğŸ¯ å­¦ä¹ ç›®æ ‡</b></summary>

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ å°†æŒæ¡ï¼š
- [ ] é…ç½®é©±åŠ¨çš„ç³»ç»Ÿè®¾è®¡ï¼ˆSKILL.md + YAML frontmatterï¼‰
- [ ] Markdown frontmatter è§£æï¼ˆYAML å…ƒæ•°æ® + æ­£æ–‡åˆ†ç¦»ï¼‰
- [ ] `dataclass(frozen=True)` ä¸å¯å˜æ•°æ®ç»“æ„
- [ ] JSON Schema éªŒè¯ï¼ˆ`jsonschema.Draft202012Validator`ï¼‰
- [ ] å·¥å…·ç™½åå•ä¸æ³¨å†Œè¡¨äº¤å‰è¿‡æ»¤
- [ ] `lru_cache` ç¼“å­˜ä¼˜åŒ–

</details>

<details>
<summary><b>ğŸ“– åŠŸèƒ½è¯´æ˜</b></summary>

æœ¬æ¨¡å—å®ç° Skills é…ç½®ä½“ç³»ï¼Œå°†æç¤ºè¯ã€å·¥å…·æƒé™ã€è¾“å‡ºç»“æ„åšæˆå¯é…ç½®çš„æŠ€èƒ½åŒ…ã€‚

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
1. **æŠ€èƒ½åŠ è½½**ï¼šä» `SKILL.md` è§£æ YAML frontmatter + Markdown æ­£æ–‡ï¼Œè¿”å› `SkillSpec` å†»ç»“æ•°æ®ç±»
2. **Schema ç®¡ç†**ï¼šåŠ è½½ `output.schema.json` å¹¶ç”¨ `jsonschema` æ ¡éªŒ Agent è¾“å‡º
3. **å·¥å…·ç™½åå•è¿‡æ»¤**ï¼šç»“åˆ `tool_interfaces.yaml` æ³¨å†Œè¡¨äº¤å‰è¿‡æ»¤ï¼Œåªæ”¾è¡Œåˆæ³•å·¥å…·
4. **æç¤ºè¯æ„å»º**ï¼šæ‹¼æ¥ base prompt + skill body + snippets + schema çº¦æŸ
5. **ç‰‡æ®µåŠ è½½**ï¼šä» `skills/snippets/` åŠ è½½å¯å¤ç”¨çš„æç¤ºè¯ç‰‡æ®µ
6. **å·¥å…·æ³¨å†Œè¡¨**ï¼šä» `tools/tool_interfaces.yaml` åŠ è½½å…¨å±€å·¥å…·å®šä¹‰

**SKILL.md æ–‡ä»¶æ ¼å¼ï¼š**
```
---
name: macro-tool-calling          # æŠ€èƒ½åç§°
type: agent                       # æŠ€èƒ½ç±»å‹
inputs: [snapshot_metrics, ...]   # è¾“å…¥å­—æ®µ
outputs: [finding_macro]          # è¾“å‡ºå­—æ®µ
tools:
  allowlist: [macro_timeseries, macro_search]  # å·¥å…·ç™½åå•
  max_calls: 3                    # æœ€å¤§è°ƒç”¨æ¬¡æ•°
  timeout_ms: 8000                # è¶…æ—¶æ—¶é—´
snippets:                         # å¯å¤ç”¨ç‰‡æ®µå¼•ç”¨
  - snippets/evidence_rules.md
---
# æ­£æ–‡éƒ¨åˆ†ï¼ˆç³»ç»Ÿæç¤ºè¯ï¼‰
ä½ æ˜¯ MacroToolCallingAgentï¼Œè´Ÿè´£è¯„ä¼°å®è§‚ç»æµç¯å¢ƒ...
```

**ä¸ºä»€ä¹ˆé‡è¦ï¼š**
æ”¹æç¤ºè¯åªéœ€æ”¹ SKILL.mdï¼Œä¸éœ€è¦åŠ¨ Python ä»£ç ï¼›æ”¹å·¥å…·æƒé™åªéœ€æ”¹ allowlistï¼Œä¸éœ€è¦æ”¹ Agent é€»è¾‘ã€‚

</details>

<details>
<summary><b>âœ… å®ç°è¦æ±‚</b></summary>

### æ–‡ä»¶ï¼š`src/skills_runtime.py`

**æ•°æ®ç»“æ„ï¼š**

`SkillSpec` â€” å†»ç»“æ•°æ®ç±»ï¼ˆ`@dataclass(frozen=True)`ï¼‰ï¼Œ16 ä¸ªå­—æ®µï¼š

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|:---|:---|:---|
| `name` | `str` | æŠ€èƒ½åç§° |
| `type` | `str` | æŠ€èƒ½ç±»å‹ï¼ˆå¦‚ `"agent"`ï¼‰ |
| `inputs` | `List[str]` | è¾“å…¥å­—æ®µåˆ—è¡¨ |
| `outputs` | `List[str]` | è¾“å‡ºå­—æ®µåˆ—è¡¨ |
| `allowlist` | `List[str]` | å·¥å…·ç™½åå• |
| `snippets` | `List[str]` | ç‰‡æ®µå¼•ç”¨è·¯å¾„åˆ—è¡¨ |
| `body` | `str` | SKILL.md æ­£æ–‡ï¼ˆç³»ç»Ÿæç¤ºè¯ï¼‰ |
| `schema` | `Optional[Dict]` | output.schema.json å†…å®¹ |
| `policy_version` | `str` | ç­–ç•¥ç‰ˆæœ¬ |
| `skills_hash` | `str` | æŠ€èƒ½å†…å®¹å“ˆå¸Œï¼ˆSHA256 å‰ 16 ä½ï¼‰ |
| `max_calls` | `int` | å·¥å…·æœ€å¤§è°ƒç”¨æ¬¡æ•° |
| `timeout_ms` | `int` | å·¥å…·è¶…æ—¶æ¯«ç§’ |
| `require_evidence` | `bool` | æ˜¯å¦å¼ºåˆ¶è¦æ±‚è¯æ® |
| `evidence_prefixes` | `List[str]` | åˆæ³•è¯æ®å‰ç¼€åˆ—è¡¨ |
| `limits` | `Dict[str, Any]` | é™åˆ¶é…ç½® |
| `stop_condition` | `List[str]` | åœæ­¢æ¡ä»¶ |
| `cost_budget` | `Dict[str, Any]` | æˆæœ¬é¢„ç®— |

**å®ç°å‡½æ•°ï¼ˆ8 ä¸ªï¼‰ï¼š**

1. `_parse_frontmatter(text: str) -> tuple[Dict, str]`ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰
   - è§£æ `---` åˆ†éš”çš„ YAML frontmatter
   - è¿”å› `(meta_dict, body_text)` å…ƒç»„
   - å¦‚æœæ²¡æœ‰ frontmatterï¼Œè¿”å› `({}, text)`

2. `_hash_text(text: str) -> str`ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰
   - SHA256 å“ˆå¸Œï¼Œå–å‰ 16 ä½åå…­è¿›åˆ¶

3. `load_skill(skill_dir: str) -> SkillSpec`ï¼ˆ`@lru_cache`ï¼‰
   - è¯»å– `skills/{skill_dir}/SKILL.md`
   - ç”¨ `_parse_frontmatter` è§£æ YAML å…ƒæ•°æ®å’Œæ­£æ–‡
   - è¯»å– `skills/{skill_dir}/output.schema.json`ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
   - ä» `meta["tools"]` æå– allowlist / max_calls / timeout_ms / require_evidence
   - æ„é€ å¹¶è¿”å› `SkillSpec` å®ä¾‹

4. `load_tool_registry() -> Set[str]`ï¼ˆ`@lru_cache`ï¼‰
   - è¯»å– `skills/tools/tool_interfaces.yaml`
   - æå–æ‰€æœ‰å·¥å…·åç§°ï¼Œè¿”å› `Set[str]`
   - æ–‡ä»¶ä¸å­˜åœ¨æ—¶è¿”å›ç©ºé›†åˆ

5. `load_snippet(path: str) -> str`
   - å…ˆå°è¯• `skills/{path}`ï¼Œå†å°è¯• `skills/snippets/{path}`
   - è¿”å›æ–‡ä»¶å†…å®¹ï¼ˆstripï¼‰ï¼Œæ‰¾ä¸åˆ°è¿”å›ç©ºå­—ç¬¦ä¸²

6. `build_system_prompt(base: str, skill: SkillSpec) -> str`
   - æ‹¼æ¥ï¼šbase + skill.body + snippets å†…å®¹ + schema çº¦æŸ
   - ç”¨ `"\n\n".join(parts).strip()` è¿æ¥
   - schema éƒ¨åˆ†æ ¼å¼ï¼š`"Output must conform to schema:\n" + json.dumps(schema)`

7. `filter_tools(tools: Sequence, allowlist: Sequence[str]) -> List`
   - å¦‚æœ allowlist ä¸ºç©ºï¼Œè¿”å›å…¨éƒ¨å·¥å…·
   - ç”¨ `load_tool_registry()` äº¤å‰è¿‡æ»¤ï¼šåªä¿ç•™æ³¨å†Œè¡¨ä¸­å­˜åœ¨çš„ç™½åå•é¡¹
   - æŒ‰å·¥å…·çš„ `name` å±æ€§åŒ¹é…

8. `validate_output(skill: SkillSpec, data: Dict) -> List[str]`
   - å¦‚æœ skill.schema ä¸º None æˆ– jsonschema æœªå®‰è£…ï¼Œè¿”å›ç©ºåˆ—è¡¨
   - ç”¨ `jsonschema.Draft202012Validator` æ ¡éªŒ
   - è¿”å›é”™è¯¯æ¶ˆæ¯åˆ—è¡¨

**è·¯å¾„å¸¸é‡ï¼ˆæ¨¡å—çº§ï¼‰ï¼š**
- `_ROOT`ï¼šé¡¹ç›®æ ¹ç›®å½•ï¼ˆ`Path(__file__).resolve().parents[1]`ï¼‰
- `_SKILLS_ROOT`ï¼š`_ROOT / "skills"`
- `_SNIPPETS_ROOT`ï¼š`_SKILLS_ROOT / "snippets"`
- `_TOOL_REGISTRY`ï¼š`_SKILLS_ROOT / "tools" / "tool_interfaces.yaml"`

</details>

<details>
<summary><b>ğŸ’» ä»£ç æ¨¡æ¿</b></summary>

### æ–‡ä»¶ï¼š`src/skills_runtime.py`

```python
"""
Skills è¿è¡Œæ—¶æ¨¡å— â€” é…ç½®é©±åŠ¨çš„æŠ€èƒ½åŠ è½½ã€æç¤ºè¯æ„å»ºã€å·¥å…·è¿‡æ»¤ã€è¾“å‡ºæ ¡éªŒã€‚
"""
from __future__ import annotations

from dataclasses import dataclass
from functools import lru_cache
import json
from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence, Set

import yaml
import hashlib

try:
    import jsonschema
except ImportError:
    jsonschema = None


_ROOT = Path(__file__).resolve().parents[1]
_SKILLS_ROOT = _ROOT / "skills"
_SNIPPETS_ROOT = _SKILLS_ROOT / "snippets"
_TOOL_REGISTRY = _SKILLS_ROOT / "tools" / "tool_interfaces.yaml"


# ---------------------------------------------------------------------------
# æ•°æ®ç»“æ„
# ---------------------------------------------------------------------------
@dataclass(frozen=True)
class SkillSpec:
    """æŠ€èƒ½è§„æ ¼ï¼Œä¸å¯å˜ã€‚"""
    name: str
    type: str
    inputs: List[str]
    outputs: List[str]
    allowlist: List[str]
    snippets: List[str]
    body: str
    schema: Optional[Dict[str, Any]]
    policy_version: str
    skills_hash: str
    max_calls: int
    timeout_ms: int
    require_evidence: bool
    evidence_prefixes: List[str]
    limits: Dict[str, Any]
    stop_condition: List[str]
    cost_budget: Dict[str, Any]


# ---------------------------------------------------------------------------
# å†…éƒ¨å·¥å…·å‡½æ•°
# ---------------------------------------------------------------------------
def _parse_frontmatter(text: str) -> tuple[Dict[str, Any], str]:
    """è§£æ YAML frontmatterï¼Œè¿”å› (meta, body)ã€‚"""
    # TODO: æ£€æŸ¥ text æ˜¯å¦ä»¥ "---" å¼€å¤´
    # TODO: ç”¨ "---" åˆ†å‰²ï¼Œå–ç¬¬äºŒæ®µåš yaml.safe_load
    # TODO: ç¬¬ä¸‰æ®µä½œä¸º bodyï¼ˆå»æ‰å¼€å¤´æ¢è¡Œï¼‰
    # TODO: æ²¡æœ‰ frontmatter æ—¶è¿”å› ({}, text)
    pass


def _hash_text(text: str) -> str:
    """SHA256 å“ˆå¸Œï¼Œå–å‰ 16 ä½ã€‚"""
    # TODO: hashlib.sha256 â†’ hexdigest()[:16]
    pass


# ---------------------------------------------------------------------------
# æ ¸å¿ƒå…¬å¼€å‡½æ•°
# ---------------------------------------------------------------------------
@lru_cache(maxsize=None)
def load_skill(skill_dir: str) -> SkillSpec:
    """åŠ è½½æŠ€èƒ½é…ç½®ï¼Œè¿”å› SkillSpecã€‚å¸¦ lru_cache ç¼“å­˜ã€‚"""
    # TODO: æ‹¼æ¥ skill_path = _SKILLS_ROOT / skill_dir / "SKILL.md"
    # TODO: æ–‡ä»¶ä¸å­˜åœ¨æ—¶ raise FileNotFoundError

    # TODO: è¯»å–æ–‡ä»¶å†…å®¹ï¼Œè°ƒç”¨ _parse_frontmatter è§£æ
    # raw = skill_path.read_text(encoding="utf-8")
    # meta, body = _parse_frontmatter(raw)

    # TODO: åŠ è½½ output.schema.jsonï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    # schema_path = _SKILLS_ROOT / skill_dir / "output.schema.json"

    # TODO: ä» meta["tools"] æå–å·¥å…·é…ç½®
    # tools_meta = meta.get("tools") or {}
    # allowlist = list(tools_meta.get("allowlist") or meta.get("allowlist") or [])
    # max_calls = int(tools_meta.get("max_calls") or 0)
    # timeout_ms = int(tools_meta.get("timeout_ms") or 0)
    # require_evidence = bool(tools_meta.get("require_evidence") or False)

    # TODO: æå–å…¶ä»–å­—æ®µï¼ševidence_prefixes, limits, stop_condition, cost_budget, policy_version
    # TODO: è®¡ç®— skills_hashï¼ˆä¼˜å…ˆç”¨ meta ä¸­çš„å€¼ï¼Œå¦åˆ™ _hash_text(raw)ï¼‰

    # TODO: æ„é€ å¹¶è¿”å› SkillSpec(name=..., type=..., ...)
    pass


@lru_cache(maxsize=None)
def load_tool_registry() -> Set[str]:
    """ä» tool_interfaces.yaml åŠ è½½å…¨å±€å·¥å…·æ³¨å†Œè¡¨ã€‚"""
    # TODO: å¦‚æœ _TOOL_REGISTRY ä¸å­˜åœ¨ï¼Œè¿”å› set()
    # TODO: yaml.safe_load è¯»å–ï¼Œæå– tools[].name
    # TODO: è¿”å› Set[str]
    pass


def load_snippet(path: str) -> str:
    """åŠ è½½å¯å¤ç”¨æç¤ºè¯ç‰‡æ®µã€‚"""
    # TODO: å…ˆå°è¯• _SKILLS_ROOT / path
    # TODO: å†å°è¯• _SNIPPETS_ROOT / path
    # TODO: è¿”å› strip() åçš„å†…å®¹ï¼Œæ‰¾ä¸åˆ°è¿”å› ""
    pass


def build_system_prompt(base: str, skill: SkillSpec) -> str:
    """æ‹¼æ¥ç³»ç»Ÿæç¤ºè¯ï¼šbase + skill.body + snippets + schemaã€‚"""
    # TODO: parts = [base]
    # TODO: å¦‚æœ skill.body éç©ºï¼Œè¿½åŠ 
    # TODO: éå† skill.snippetsï¼Œload_snippet å¹¶è¿½åŠ 
    # TODO: å¦‚æœ skill.schema é Noneï¼Œè¿½åŠ  schema çº¦æŸæ–‡æœ¬
    # TODO: return "\n\n".join(parts).strip()
    pass


def filter_tools(tools: Sequence[Any], allowlist: Sequence[str]) -> List[Any]:
    """æŒ‰ç™½åå•è¿‡æ»¤å·¥å…·åˆ—è¡¨ï¼Œä¸æ³¨å†Œè¡¨äº¤å‰éªŒè¯ã€‚"""
    # TODO: allow = set(allowlist or [])
    # TODO: å¦‚æœ allow ä¸ºç©ºï¼Œè¿”å› list(tools)
    # TODO: åŠ è½½ registry = load_tool_registry()
    # TODO: å¦‚æœ registry éç©ºï¼Œallow åªä¿ç•™ registry ä¸­å­˜åœ¨çš„
    # TODO: éå† toolsï¼ŒæŒ‰ name å±æ€§åŒ¹é…
    pass


def validate_output(skill: SkillSpec, data: Dict[str, Any]) -> List[str]:
    """ç”¨ JSON Schema æ ¡éªŒ Agent è¾“å‡ºï¼Œè¿”å›é”™è¯¯åˆ—è¡¨ã€‚"""
    # TODO: å¦‚æœ skill.schema ä¸º None æˆ– jsonschema æœªå®‰è£…ï¼Œè¿”å› []
    # TODO: ç”¨ jsonschema.Draft202012Validator æ ¡éªŒ
    # TODO: æ”¶é›† iter_errors çš„ message åˆ—è¡¨
    pass
```

### å‚è€ƒï¼š`skills/macro-tool-calling/SKILL.md`ï¼ˆå·²æä¾›ï¼Œæ— éœ€ç¼–å†™ï¼‰

```markdown
---
name: macro-tool-calling
description: å®è§‚å·¥å…·è°ƒç”¨ä»£ç†ï¼Œç»“åˆæ—¶åºä¸æ–‡æœ¬ç”Ÿæˆå®è§‚é£é™©ç»“è®º
type: agent
inputs:
  - snapshot_metrics
  - data_quality
  - tool_results
outputs:
  - finding_macro
policy_version: "local-default"
skills_hash: ""
evidence_prefixes:
  - snapshot_metrics.
  - rules.
  - "tool:"
tools:
  allowlist:
    - macro_timeseries
    - macro_search
  max_calls: 3
  timeout_ms: 8000
  require_evidence: false
limits:
  max_retries: 1
cost_budget:
  llm_tokens: 1200
  tool_calls: 3
snippets:
  - snippets/evidence_rules.md
---

# MacroToolCallingAgent ç³»ç»Ÿæç¤ºè¯

ä½ æ˜¯ MacroToolCallingAgentï¼Œè´Ÿè´£è¯„ä¼°å®è§‚ç»æµç¯å¢ƒå¯¹æŠ•èµ„ç»„åˆçš„å½±å“ã€‚
...ï¼ˆæ­£æ–‡å³ç³»ç»Ÿæç¤ºè¯ï¼Œç”± build_system_prompt æ‹¼æ¥åˆ° Agentï¼‰
```

> **æ³¨æ„**ï¼š`skills/` ç›®å½•ä¸‹çš„æ‰€æœ‰é…ç½®æ–‡ä»¶ï¼ˆSKILL.mdã€output.schema.jsonã€tool_interfaces.yamlã€snippets/ï¼‰å‡å·²æä¾›ï¼Œä½ åªéœ€å®ç° `src/skills_runtime.py` æ¥è§£æå’Œä½¿ç”¨å®ƒä»¬ã€‚

</details>

<details>
<summary><b>ğŸ§ª æµ‹è¯•æ£€æŸ¥</b></summary>

### æµ‹è¯•1ï¼šSkillSpec æ•°æ®ç»“æ„

```python
from src.skills_runtime import load_skill, SkillSpec

skill = load_skill("macro-tool-calling")
assert isinstance(skill, SkillSpec)
assert skill.name == "macro-tool-calling"
assert skill.type == "agent"
assert "macro_timeseries" in skill.allowlist
assert "macro_search" in skill.allowlist
assert skill.max_calls == 3
assert skill.timeout_ms == 8000
assert len(skill.body) > 0  # æ­£æ–‡éç©º

print("âœ… SkillSpec æ•°æ®ç»“æ„æµ‹è¯•é€šè¿‡")
```

### æµ‹è¯•2ï¼šfrontmatter è§£æ

```python
from src.skills_runtime import _parse_frontmatter

text = "---\nname: test\ntype: agent\n---\n\n# Body here"
meta, body = _parse_frontmatter(text)
assert meta["name"] == "test"
assert meta["type"] == "agent"
assert "Body here" in body

# æ—  frontmatter
meta2, body2 = _parse_frontmatter("plain text")
assert meta2 == {}
assert body2 == "plain text"

print("âœ… frontmatter è§£ææµ‹è¯•é€šè¿‡")
```

### æµ‹è¯•3ï¼šbuild_system_prompt æ‹¼æ¥

```python
from src.skills_runtime import load_skill, build_system_prompt

skill = load_skill("macro-tool-calling")
prompt = build_system_prompt("Base prompt.", skill)
assert "Base prompt." in prompt
assert "MacroToolCallingAgent" in prompt  # skill.body å†…å®¹
# å¦‚æœæœ‰ snippetsï¼Œä¹Ÿä¼šè¢«æ‹¼æ¥è¿›å»
if skill.snippets:
    assert "è¯æ®" in prompt  # evidence_rules.md å†…å®¹

print("âœ… build_system_prompt æµ‹è¯•é€šè¿‡")
```

### æµ‹è¯•4ï¼šfilter_tools ç™½åå•è¿‡æ»¤

```python
from src.skills_runtime import filter_tools

class FakeTool:
    def __init__(self, name):
        self.name = name

tools = [FakeTool("macro_timeseries"), FakeTool("macro_search"), FakeTool("unknown")]
filtered = filter_tools(tools, ["macro_timeseries", "macro_search"])
assert len(filtered) <= 2
names = [t.name for t in filtered]
assert "unknown" not in names

# ç©ºç™½åå•è¿”å›å…¨éƒ¨
all_tools = filter_tools(tools, [])
assert len(all_tools) == 3

print("âœ… filter_tools æµ‹è¯•é€šè¿‡")
```

### æµ‹è¯•5ï¼švalidate_output Schema æ ¡éªŒ

```python
from src.skills_runtime import load_skill, validate_output

skill = load_skill("macro-tool-calling")
# åˆæ³•è¾“å‡º
valid = {"severity": 1, "summary": "test", "evidence": []}
errors = validate_output(skill, valid)
assert errors == []

# éæ³•è¾“å‡ºï¼ˆç¼ºå°‘ required å­—æ®µï¼‰
invalid = {"severity": "not_int"}
errors = validate_output(skill, invalid)
assert len(errors) > 0

print("âœ… validate_output æµ‹è¯•é€šè¿‡")
```

**æ£€æŸ¥é¡¹ï¼š**
- [ ] `load_skill` è¿”å› `SkillSpec` å®ä¾‹ï¼ˆé dictï¼‰
- [ ] frontmatter æ­£ç¡®è§£æ YAML å…ƒæ•°æ®å’Œæ­£æ–‡
- [ ] `build_system_prompt` æ‹¼æ¥ base + body + snippets + schema
- [ ] `filter_tools` ç©ºç™½åå•è¿”å›å…¨éƒ¨ï¼Œéç©ºæ—¶æŒ‰åç§°è¿‡æ»¤
- [ ] `validate_output` ç”¨ JSON Schema æ ¡éªŒï¼Œè¿”å›é”™è¯¯åˆ—è¡¨
- [ ] `load_skill` æœ‰ `lru_cache` ç¼“å­˜

</details>

<details>
<summary><b>ğŸ’¡ æç¤ºä¸æŠ€å·§</b></summary>

**å…³é”®ç‚¹ï¼š**
1. **Frontmatter è§£æ**ï¼šç”¨ `text.split("---", 2)` åˆ†å‰²ï¼Œç¬¬äºŒæ®µ `yaml.safe_load`ï¼Œç¬¬ä¸‰æ®µæ˜¯æ­£æ–‡
2. **`@dataclass(frozen=True)`**ï¼šSkillSpec åˆ›å»ºåä¸å¯ä¿®æ”¹ï¼Œä¿è¯çº¿ç¨‹å®‰å…¨
3. **`@lru_cache(maxsize=None)`**ï¼š`load_skill` å’Œ `load_tool_registry` éƒ½ç”¨ç¼“å­˜ï¼ŒåŒä¸€ skill åªè§£æä¸€æ¬¡
4. **å·¥å…·æ³¨å†Œè¡¨äº¤å‰è¿‡æ»¤**ï¼š`filter_tools` ä¸æ˜¯åªçœ‹ç™½åå•ï¼Œè¿˜è¦å’Œ `tool_interfaces.yaml` äº¤å‰éªŒè¯
5. **Schema æ ¡éªŒ**ï¼šç”¨ `Draft202012Validator`ï¼ˆä¸æ˜¯ `validate`ï¼‰ï¼Œé€šè¿‡ `iter_errors` æ”¶é›†æ‰€æœ‰é”™è¯¯

**å¸¸è§é”™è¯¯ï¼š**
- âŒ `load_skill` è¿”å› dict è€Œé SkillSpec â€” è°ƒç”¨æ–¹ç”¨ `skill.allowlist` å±æ€§è®¿é—®
- âŒ å¿˜è®°å¤„ç† `meta.get("tools")` åµŒå¥—ç»“æ„ â€” allowlist åœ¨ `tools.allowlist` ä¸‹
- âŒ `filter_tools` ç©ºç™½åå•æ—¶è¿‡æ»¤æ‰æ‰€æœ‰å·¥å…· â€” åº”è¯¥è¿”å›å…¨éƒ¨
- âŒ `jsonschema` æœªå®‰è£…æ—¶æŠ¥é”™ â€” åº”è¯¥ `try/except ImportError` å¹¶è¿”å›ç©ºåˆ—è¡¨

**å‚è€ƒèµ„æºï¼š**
- `skills/macro-tool-calling/SKILL.md` â€” å®é™…çš„ frontmatter æ ¼å¼
- `skills/tools/tool_interfaces.yaml` â€” å·¥å…·æ³¨å†Œè¡¨æ ¼å¼
- `skills/macro-tool-calling/output.schema.json` â€” Schema æ ¼å¼

</details>

---



## ğŸ“ æ¨¡å—8: è§„åˆ™ä¸çº¦æŸ

<details open>
<summary><b>ğŸ“‹ æ¨¡å—ä¿¡æ¯</b></summary>

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **éš¾åº¦** | â­â­â­ ä¸­çº§ |
| **æ–‡ä»¶æ•°** | 2ä¸ª |
| **æ–‡ä»¶è·¯å¾„** | `src/tools/rules.py`<br>`src/tools/constraints.py` |
| **ä¾èµ–æ¨¡å—** | æ¨¡å—1, 3, 10 |
| **è¢«ä¾èµ–** | æ¨¡å—5, 6, 9 |

</details>

<details>
<summary><b>ğŸ¯ å­¦ä¹ ç›®æ ‡</b></summary>

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ å°†æŒæ¡ï¼š
- [ ] YAML é…ç½®æ–‡ä»¶å¤„ç†
- [ ] è§„åˆ™å¼•æ“è®¾è®¡
- [ ] çº¦æŸè¯„ä¼°é€»è¾‘
- [ ] ç¡¬çº¦æŸä¸è½¯çº¦æŸçš„åŒºåˆ†

</details>

<details>
<summary><b>ğŸ“– åŠŸèƒ½è¯´æ˜</b></summary>

æœ¬æ¨¡å—è´Ÿè´£åŠ è½½è§„åˆ™é…ç½®å¹¶è¯„ä¼°çº¦æŸè¿è§„æƒ…å†µã€‚

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
1. **è§„åˆ™åŠ è½½**ï¼šä» YAML æ–‡ä»¶è¯»å–è§„åˆ™é…ç½®
2. **çº¦æŸè¯„ä¼°**ï¼šæ£€æŸ¥æŒ‡æ ‡æ˜¯å¦è¿åé˜ˆå€¼
3. **çº¦æŸåˆ†ç±»**ï¼šåŒºåˆ† hard å’Œ soft çº¦æŸ

**ä¸ºä»€ä¹ˆé‡è¦ï¼š**
è§„åˆ™å¼•æ“æ˜¯ç¡®å®šæ€§å†³ç­–çš„åŸºç¡€ï¼Œçº¦æŸè¯„ä¼°ç›´æ¥å½±å“å†³ç­–ç»“æœã€‚

</details>

<details>
<summary><b>âœ… å®ç°è¦æ±‚</b></summary>

### æ–‡ä»¶1ï¼š`src/tools/rules.py`

**æ¨¡å—çº§å¸¸é‡ï¼š**
- `_DEFAULT_RULES`ï¼šå†…ç½®é»˜è®¤è§„åˆ™ï¼ˆdefault + conservative ä¸¤ä¸ª profileï¼‰ï¼Œå½“ rules.yaml ä¸å­˜åœ¨æ—¶ä½¿ç”¨
- `_ROOT`ï¼šé¡¹ç›®æ ¹ç›®å½•

**å®ç°å‡½æ•°ï¼ˆ4 ä¸ªï¼‰ï¼š**

1. `_rules_path(config) -> Path`ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰
   - å¦‚æœ `config.csv_data_dir` æœ‰å€¼ï¼Œè¿”å› `csv_data_dir / "rules.yaml"`
   - å¦åˆ™è¿”å› `é¡¹ç›®æ ¹ç›®å½• / "cufel_practice_data" / "rules.yaml"`

2. `_load_rules_cached(profile: str, path_str: str) -> Tuple[Dict, str]`ï¼ˆ`@lru_cache`ï¼‰
   - è¯»å– YAML æ–‡ä»¶ï¼ŒæŒ‰ profile å–è§„åˆ™
   - å¦‚æœæŒ‡å®š profile ä¸å­˜åœ¨ï¼Œå›é€€åˆ° `"default"`
   - å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå›é€€åˆ° `_DEFAULT_RULES`
   - è¿”å› `(rules_dict, version_string)`ï¼Œversion ä¸º `"rules.yaml"` æˆ– `"local-default"`

3. `load_rules(profile: str, config) -> Tuple[Dict, str]`
   - è°ƒç”¨ `_rules_path` å’Œ `_load_rules_cached`
   - è¿”å› `(dict(rules), version)` â€” æ³¨æ„è¿”å›çš„æ˜¯**å…ƒç»„**ï¼Œä¸æ˜¯å•ä¸ª dict

4. `get_blocklist(profile: str, config) -> Tuple[list, str]`
   - è°ƒç”¨ `load_rules` æå– `rules["blocklist"]`
   - è¿”å› `(blocklist, version)` â€” åŒæ ·æ˜¯å…ƒç»„

### æ–‡ä»¶2ï¼š`src/tools/constraints.py`

**æ¨¡å—çº§å¸¸é‡ï¼š**
- `_LEVEL`ï¼šseverity åˆ°æ•°å€¼çš„æ˜ å°„ `{"pass": 0, "warn": 1, "restrict": 2, "block": 3}`

**å®ç°å‡½æ•°ï¼ˆ1 ä¸ªä¸»å‡½æ•° + 5 ä¸ªå†…éƒ¨è¾…åŠ©ï¼‰ï¼š**

`constraints_evaluator(state: RiskState, config) -> Dict[str, Any]`

è¿”å› `{"rule_findings": findings}`ï¼Œå…¶ä¸­ findings æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å«ï¼š

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|:---|:---|:---|
| `rule_id` | `str` | è§„åˆ™æ ‡è¯†ï¼ˆå¦‚ `"max_single_weight"`ï¼‰ |
| `severity` | `str` | `"warn"` / `"restrict"` / `"block"` |
| `level` | `int` | severity å¯¹åº”çš„æ•°å€¼ |
| `metric` | `str` | æŒ‡æ ‡åç§° |
| `value` | `float` | å½“å‰å€¼ |
| `limit` | `float` | é˜ˆå€¼ |
| `message` | `str` | æè¿°ä¿¡æ¯ |
| `evidence` | `list` | è¯æ®å¼•ç”¨ |

**è¯„ä¼°çš„è§„åˆ™ï¼ˆ10 æ¡ï¼‰ï¼š**
1. `max_single_weight` â€” top_weight è¶…ä¸Šé™ â†’ restrict
2. `max_hhi` â€” hhi è¶…ä¸Šé™ â†’ warn
3. `max_portfolio_volatility` â€” æ³¢åŠ¨ç‡è¶…ä¸Šé™ â†’ restrict
4. `max_weighted_spread_bps` â€” ä»·å·®è¶…ä¸Šé™ â†’ warn
5. `min_weighted_adv` â€” ADV ä½äºä¸‹é™ â†’ warnï¼ˆåå‘æŒ‡æ ‡ï¼‰
6. `max_turnover` â€” æ¢æ‰‹ç‡è¶…ä¸Šé™ â†’ warn
7. `max_position_delta` â€” å•ä¸€ä»“ä½å˜åŠ¨è¶…ä¸Šé™ â†’ warn
8. `max_adv_ratio` â€” äº¤æ˜“é‡/ADV æ¯”è¶…ä¸Šé™ â†’ warn
9. `max_delta_hhi` / `max_delta_volatility` â€” å¢é‡æŒ‡æ ‡è¶…ä¸Šé™ â†’ warn
10. `blocklist` â€” ç¦æŠ•æ¸…å•å‘½ä¸­ â†’ **block**ï¼ˆæœ€ä¸¥æ ¼ï¼‰

</details>

<details>
<summary><b>ğŸ’» ä»£ç æ¨¡æ¿</b></summary>

### æ–‡ä»¶1ï¼š`src/tools/rules.py`

```python
"""
è§„åˆ™åŠ è½½ä¸ç®¡ç†æ¨¡å— â€” ä» rules.yaml åŠ è½½é˜ˆå€¼é…ç½®ï¼Œæ”¯æŒå¤š profileã€‚
"""
from __future__ import annotations

from functools import lru_cache
from pathlib import Path
from typing import Any, Dict, Tuple

import yaml

from ..config import RuntimeConfig, DEFAULT_CONFIG


# ---------------------------------------------------------------------------
# å†…ç½®é»˜è®¤è§„åˆ™ï¼ˆrules.yaml ä¸å­˜åœ¨æ—¶çš„ fallbackï¼‰
# ---------------------------------------------------------------------------
_DEFAULT_RULES: Dict[str, Dict[str, Any]] = {
    "default": {
        "max_single_weight": 0.4,
        "max_hhi": 0.3,
        "max_portfolio_volatility": 0.24,
        "max_weighted_spread_bps": 50,
        "min_weighted_adv": 3000000,
        "blocklist": ["CCC"],
    },
    "conservative": {
        "max_single_weight": 0.3,
        "max_hhi": 0.25,
        "max_portfolio_volatility": 0.2,
        "max_weighted_spread_bps": 40,
        "min_weighted_adv": 5000000,
        "blocklist": ["CCC"],
    },
}

_ROOT = Path(__file__).resolve().parents[2]


# ---------------------------------------------------------------------------
# è·¯å¾„ä¸ç¼“å­˜
# ---------------------------------------------------------------------------
def _rules_path(config: RuntimeConfig | None = None) -> Path:
    """æ ¹æ®é…ç½®è¿”å› rules.yaml è·¯å¾„ã€‚"""
    # TODO: å¦‚æœ config.csv_data_dir æœ‰å€¼ï¼Œè¿”å› csv_data_dir / "rules.yaml"
    # TODO: å¦åˆ™è¿”å› _ROOT / "cufel_practice_data" / "rules.yaml"
    pass


@lru_cache(maxsize=32)
def _load_rules_cached(profile: str, path_str: str) -> Tuple[Dict[str, Any], str]:
    """å¸¦ç¼“å­˜çš„è§„åˆ™åŠ è½½ï¼ˆpath_str ç”¨äº cache keyï¼‰ã€‚"""
    # TODO: path = Path(path_str)
    # TODO: å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œyaml.safe_load è¯»å–
    # TODO: æŒ‰ profile å–è§„åˆ™ï¼Œprofile ä¸å­˜åœ¨åˆ™å›é€€ "default"
    # TODO: æ–‡ä»¶ä¸å­˜åœ¨æ—¶å›é€€ _DEFAULT_RULES
    # TODO: è¿”å› (rules_dict, version)ï¼Œversion ä¸º "rules.yaml" æˆ– "local-default"
    pass


# ---------------------------------------------------------------------------
# å…¬å¼€æ¥å£
# ---------------------------------------------------------------------------
def load_rules(profile: str, config: RuntimeConfig | None = None) -> Tuple[Dict[str, Any], str]:
    """åŠ è½½è§„åˆ™é…ç½®ï¼Œè¿”å› (rules, version) å…ƒç»„ã€‚"""
    # TODO: è°ƒç”¨ _rules_path å’Œ _load_rules_cached
    # TODO: è¿”å› dict(rules), version â€” æ³¨æ„æ‹·è´ä¸€ä»½é¿å…ç¼“å­˜è¢«ä¿®æ”¹
    pass


def get_blocklist(profile: str, config: RuntimeConfig | None = None) -> Tuple[list, str]:
    """æå–ç¦æŠ•æ¸…å•ï¼Œè¿”å› (blocklist, version) å…ƒç»„ã€‚"""
    # TODO: è°ƒç”¨ load_rulesï¼Œæå– rules["blocklist"]
    pass
```

### æ–‡ä»¶2ï¼š`src/tools/constraints.py`

```python
"""
çº¦æŸè¯„ä¼°æ¨¡å— â€” é€æ¡æ£€æŸ¥è§„åˆ™é˜ˆå€¼ï¼Œç”Ÿæˆ rule_findings åˆ—è¡¨ã€‚
"""
from __future__ import annotations

from typing import Dict, Any, List

from ..state import RiskState
from ..config import RuntimeConfig, DEFAULT_CONFIG
from .rules import load_rules


_LEVEL = {"pass": 0, "warn": 1, "restrict": 2, "block": 3}


def constraints_evaluator(state: RiskState, config: RuntimeConfig | None = None) -> Dict[str, Any]:
    """è¯„ä¼°æ‰€æœ‰è§„åˆ™çº¦æŸï¼Œè¿”å› {"rule_findings": [...]}ã€‚"""
    cfg = config or DEFAULT_CONFIG
    normalized = state.get("normalized") or {}
    metrics = state.get("snapshot_metrics") or {}

    rules, _ = load_rules(normalized.get("policy_profile", "default"), cfg)

    findings: List[Dict[str, Any]] = []

    # --- å†…éƒ¨è¾…åŠ©å‡½æ•° ---
    def add(rule_id: str, severity: str, metric: str,
            value: float, limit: float, message: str) -> None:
        """æ·»åŠ ä¸€æ¡è¿è§„è®°å½•ã€‚"""
        # TODO: æ„é€  finding dictï¼ŒåŒ…å« rule_id, severity, level, metric, value, limit, message, evidence
        # TODO: level = _LEVEL.get(severity, 0)
        # TODO: evidence = [{"ref": f"snapshot_metrics.{metric}", "value": value}]
        pass

    def _get_float(mapping: Dict[str, Any], key: str, default: float) -> float:
        # TODO: å®‰å…¨åœ°ä» mapping å– float å€¼
        pass

    def _metric(key: str) -> float:
        # TODO: ä» metrics å–æŒ‡æ ‡å€¼
        pass

    def _rule(key: str, default: float) -> float:
        # TODO: ä» rules å–é˜ˆå€¼
        pass

    def _check_max(rule_id: str, metric_key: str, limit: float,
                   severity: str, message: str) -> None:
        """æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦è¶…è¿‡ä¸Šé™ã€‚"""
        # TODO: value = _metric(metric_key)
        # TODO: if value > limit: add(...)
        pass

    def _check_min(rule_id: str, metric_key: str, limit: float,
                   severity: str, message: str) -> None:
        """æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦ä½äºä¸‹é™ï¼ˆåå‘æŒ‡æ ‡ï¼‰ã€‚"""
        # TODO: value = _metric(metric_key)
        # TODO: if limit > 0 and value < limit: add(...)
        pass

    # --- é€æ¡è¯„ä¼°è§„åˆ™ ---
    # TODO: _check_max("max_single_weight", "top_weight", ..., "restrict", ...)
    # TODO: _check_max("max_hhi", "hhi", ..., "warn", ...)
    # TODO: _check_max("max_portfolio_volatility", "portfolio_volatility", ..., "restrict", ...)
    # TODO: _check_max("max_weighted_spread_bps", "weighted_spread_bps", ..., "warn", ...)
    # TODO: _check_min("min_weighted_adv", "weighted_adv", ..., "warn", ...)
    # TODO: _check_max("max_turnover", "turnover", ..., "warn", ...)
    # TODO: _check_max("max_position_delta", "max_position_delta", ..., "warn", ...)

    # TODO: ç‰¹æ®Šå¤„ç† max_adv_ratioï¼ˆmetrics ä¸­å¯èƒ½ä¸º Noneï¼‰

    # TODO: _check_max("max_delta_hhi", "delta_hhi", ..., "warn", ...)
    # TODO: _check_max("max_delta_volatility", "delta_portfolio_volatility", ..., "warn", ...)

    # TODO: ç¦æŠ•æ¸…å•æ£€æŸ¥ â†’ severity = "block"
    # blocklist = set(state.get("compliance_blocklist") or rules.get("blocklist") or [])
    # blocked = [c for c, w in target_weights.items() if c in blocklist and w > 0]

    return {"rule_findings": findings}
```

</details>

<details>
<summary><b>ğŸ§ª æµ‹è¯•æ£€æŸ¥</b></summary>

### æµ‹è¯•1ï¼šè§„åˆ™åŠ è½½ï¼ˆè¿”å›å…ƒç»„ï¼‰

```python
from src.tools.rules import load_rules, get_blocklist

rules, version = load_rules("default")
assert isinstance(rules, dict)
assert isinstance(version, str)
assert version in ("rules.yaml", "local-default")
assert "max_single_weight" in rules or "blocklist" in rules

# conservative profile
rules_c, _ = load_rules("conservative")
assert isinstance(rules_c, dict)

print("âœ… è§„åˆ™åŠ è½½æµ‹è¯•é€šè¿‡")
```

### æµ‹è¯•2ï¼šç¦æŠ•æ¸…å•

```python
from src.tools.rules import get_blocklist

blocklist, version = get_blocklist("default")
assert isinstance(blocklist, list)
assert "CCC" in blocklist  # é»˜è®¤è§„åˆ™ä¸­æœ‰ CCC

print("âœ… ç¦æŠ•æ¸…å•æµ‹è¯•é€šè¿‡")
```

### æµ‹è¯•3ï¼šçº¦æŸè¯„ä¼°

```python
from src.tools.constraints import constraints_evaluator

state = {
    "normalized": {"policy_profile": "default", "target_weights": {"AAA": 0.9, "BBB": 0.1}},
    "snapshot_metrics": {
        "top_weight": 0.9,
        "hhi": 0.82,
        "portfolio_volatility": 0.03,
        "weighted_spread_bps": 300,
        "weighted_adv": 1000,
        "turnover": 0.1,
        "max_position_delta": 0.1,
        "delta_hhi": 0.01,
        "delta_portfolio_volatility": 0.001,
    },
}

result = constraints_evaluator(state)
assert "rule_findings" in result
findings = result["rule_findings"]
assert isinstance(findings, list)
# top_weight=0.9 åº”è¯¥è§¦å‘ max_single_weight
rule_ids = [f["rule_id"] for f in findings]
assert "max_single_weight" in rule_ids

print("âœ… çº¦æŸè¯„ä¼°æµ‹è¯•é€šè¿‡")
```

### æµ‹è¯•4ï¼šç¦æŠ•æ¸…å•è§¦å‘ block

```python
from src.tools.constraints import constraints_evaluator

state = {
    "normalized": {"policy_profile": "default", "target_weights": {"CCC": 0.5, "BBB": 0.5}},
    "snapshot_metrics": {"top_weight": 0.5, "hhi": 0.5},
}

result = constraints_evaluator(state)
findings = result["rule_findings"]
block_findings = [f for f in findings if f["severity"] == "block"]
assert len(block_findings) > 0  # CCC åœ¨ç¦æŠ•æ¸…å•ä¸­

print("âœ… ç¦æŠ•æ¸…å• block æµ‹è¯•é€šè¿‡")
```

**æ£€æŸ¥é¡¹ï¼š**
- [ ] `load_rules` è¿”å› `(dict, str)` å…ƒç»„ï¼Œä¸æ˜¯å•ä¸ª dict
- [ ] `_load_rules_cached` æœ‰ `@lru_cache` ç¼“å­˜
- [ ] `constraints_evaluator` è¿”å› `{"rule_findings": [...]}`ï¼Œä¸æ˜¯ä¿®æ”¹ state
- [ ] ç¦æŠ•æ¸…å•å‘½ä¸­æ—¶ severity ä¸º `"block"`
- [ ] åå‘æŒ‡æ ‡ï¼ˆmin_weighted_advï¼‰ç”¨ `<` åˆ¤æ–­

</details>

<details>
<summary><b>ğŸ’¡ æç¤ºä¸æŠ€å·§</b></summary>

**å…³é”®ç‚¹ï¼š**
1. **è¿”å›å€¼æ˜¯å…ƒç»„**ï¼š`load_rules` è¿”å› `(rules, version)`ï¼Œä¸æ˜¯å•ä¸ª dict
2. **`@lru_cache` ç¼“å­˜**ï¼š`_load_rules_cached` ç”¨ `path_str`ï¼ˆå­—ç¬¦ä¸²ï¼‰åš cache keyï¼Œä¸èƒ½ç”¨ Path å¯¹è±¡
3. **å†…éƒ¨è¾…åŠ©å‡½æ•°æ¨¡å¼**ï¼š`constraints_evaluator` å†…éƒ¨å®šä¹‰ `add`ã€`_check_max`ã€`_check_min` ç­‰é—­åŒ…ï¼Œå…±äº« `findings` åˆ—è¡¨
4. **åå‘æŒ‡æ ‡**ï¼š`min_weighted_adv` ç”¨ `_check_min`ï¼ˆvalue < limit æ—¶è§¦å‘ï¼‰ï¼Œå…¶ä»–éƒ½ç”¨ `_check_max`
5. **ç¦æŠ•æ¸…å•æ¥æº**ï¼šä¼˜å…ˆç”¨ `state["compliance_blocklist"]`ï¼ˆåˆè§„ Agent è¾“å‡ºï¼‰ï¼Œå›é€€åˆ° `rules["blocklist"]`

**å¸¸è§é”™è¯¯ï¼š**
- âŒ `load_rules` è¿”å› dict è€Œéå…ƒç»„ â€” è°ƒç”¨æ–¹ç”¨ `rules, version = load_rules(...)` è§£åŒ…
- âŒ `constraints_evaluator` ä¿®æ”¹å¹¶è¿”å› state â€” åº”è¯¥è¿”å› `{"rule_findings": findings}`
- âŒ å¿˜è®°å¤„ç† `max_adv_ratio` ä¸º None çš„æƒ…å†µï¼ˆmetrics ä¸­å¯èƒ½ä¸å­˜åœ¨ï¼‰

**å‚è€ƒèµ„æºï¼š**
- `cufel_practice_data/rules.yaml` â€” å®é™…çš„é˜ˆå€¼é…ç½®ï¼ˆè’™ç‰¹å¡æ´›æ ¡å‡†ç”Ÿæˆï¼‰
- `src/chains/common.py` ä¸­çš„ `load_rules_cached` â€” åˆ†æé“¾è·¯ä¹Ÿç”¨äº†è§„åˆ™åŠ è½½

</details>

---

## ğŸ“ æ¨¡å—9: å†³ç­–ä¸å®¡è®¡

<details open>
<summary><b>ğŸ“‹ æ¨¡å—ä¿¡æ¯</b></summary>

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **éš¾åº¦** | â­â­â­ ä¸­çº§ |
| **æ–‡ä»¶æ•°** | 4ä¸ª |
| **æ–‡ä»¶è·¯å¾„** | `src/chains/reducer.py`<br>`src/tools/decision.py`<br>`src/tools/solver.py`<br>`src/tools/audit.py` |
| **ä¾èµ–æ¨¡å—** | æ¨¡å—1, 5, 6, 8 |
| **è¢«ä¾èµ–** | æ—  |

</details>

<details>
<summary><b>ğŸ¯ å­¦ä¹ ç›®æ ‡</b></summary>

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ å°†æŒæ¡ï¼š
- [ ] å¤šæºæ•°æ®æ±‡æ€»æŠ€æœ¯
- [ ] å†³ç­–å¼•æ“è®¾è®¡æ¨¡å¼
- [ ] CVXPY çº¦æŸä¼˜åŒ–
- [ ] å®¡è®¡æ—¥å¿—è®¾è®¡

</details>

<details>
<summary><b>ğŸ“– åŠŸèƒ½è¯´æ˜</b></summary>

æœ¬æ¨¡å—æ˜¯ç³»ç»Ÿçš„å†³ç­–è¾“å‡ºå±‚ï¼Œè´Ÿè´£æ±‡æ€»åˆ†æç»“æœå¹¶ç»™å‡ºæœ€ç»ˆå†³ç­–ã€‚

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
1. **ç»“æœæ±‡æ€»**ï¼šæ•´åˆæ‰€æœ‰åˆ†æèŠ‚ç‚¹çš„è¾“å‡º
2. **å†³ç­–å¼•æ“**ï¼šåŸºäºè§„åˆ™å’Œé£é™©æŠ¥å‘Šç»™å‡ºå†³ç­–
3. **çº¦æŸæ±‚è§£**ï¼šåœ¨ restrict æƒ…å†µä¸‹ç”Ÿæˆè°ƒä»“å»ºè®®
4. **å®¡è®¡æ—¥å¿—**ï¼šè®°å½•å®Œæ•´çš„å†³ç­–è¿‡ç¨‹

**ä¸ºä»€ä¹ˆé‡è¦ï¼š**
å†³ç­–å±‚æ˜¯ç³»ç»Ÿçš„æœ€ç»ˆè¾“å‡ºï¼Œç›´æ¥å½±å“ç”¨æˆ·çš„äº¤æ˜“å†³ç­–ã€‚

</details>

<details>
<summary><b>âœ… å®ç°è¦æ±‚</b></summary>

### æ–‡ä»¶1ï¼š`src/chains/reducer.py`

**æ¨¡å—çº§å¸¸é‡ï¼š**
- `_DEFAULT_EVIDENCE_PREFIXES`ï¼šé»˜è®¤åˆæ³•è¯æ®å‰ç¼€ `("snapshot_metrics.", "rules.", "tool:")`
- `_FINDING_KEYS`ï¼š6 ä¸ª finding å­—æ®µåå…ƒç»„
- `_AGENT_SKILLS`ï¼šAgent åç§°åˆ° skill ç›®å½•çš„æ˜ å°„

**å®ç°å‡½æ•°ï¼ˆ3 ä¸ªï¼‰ï¼š**

1. `_is_allowed_ref(ref, prefixes) -> bool`ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰
   - æ£€æŸ¥è¯æ®å¼•ç”¨æ˜¯å¦ä»¥åˆæ³•å‰ç¼€å¼€å¤´

2. `_sanitize_evidence(state, evidence, gaps, prefixes) -> List[Dict]`ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰
   - è¿‡æ»¤éæ³•è¯æ®å¼•ç”¨ï¼Œè®°å½•åˆ° gaps
   - å¯¹ `snapshot_metrics.*` å¼•ç”¨ï¼Œä» state ä¸­è¡¥å……å®é™…å€¼

3. `reducer_chain(state: RiskState) -> Dict[str, Any]`
   - æ”¶é›† 6 ä¸ª `finding_*` å­—æ®µ
   - å¯¹æ¯ä¸ª findingï¼ŒåŠ è½½å¯¹åº” skill çš„ `evidence_prefixes` åšè¯æ®æ¸…æ´—
   - å¦‚æœ skill è¦æ±‚ `require_evidence`ï¼Œæ£€æŸ¥è¯æ®æ˜¯å¦æœ‰æ•ˆ
   - è®¡ç®— `overall_severity`ï¼ˆæ‰€æœ‰ finding ä¸­æœ€å¤§å€¼ï¼‰
   - è¿”å› `{"findings": [...], "risk_report": {...}, "data_gaps": [...]}`

### æ–‡ä»¶2ï¼š`src/tools/decision.py`

**æ¨¡å—çº§å¸¸é‡ï¼š**
- `_LEVEL`ï¼š`{"pass": 0, "warn": 1, "restrict": 2, "block": 3}`

**å®ç°å‡½æ•°ï¼ˆ2 ä¸ªï¼‰ï¼š**

1. `_max_level(findings) -> int`ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰
   - ä» rule_findings åˆ—è¡¨ä¸­å–æœ€å¤§ level å€¼

2. `decision_engine(state: RiskState) -> Dict[str, Any]`
   - å†³ç­–é€»è¾‘ï¼ˆä¸¤å±‚ï¼‰ï¼š
     - `rule_level >= 3` â†’ block
     - `rule_level >= 2` â†’ restrict
     - `report_level >= 2` â†’ restrict
     - `report_level >= 1` æˆ– data_quality degraded â†’ warn
     - å¦åˆ™ â†’ pass
   - æå– binding_constraintsï¼ˆseverity ä¸º restrict/block çš„è§„åˆ™ï¼‰
   - è¿”å› `{"decision": {...}, "binding_constraints": [...]}`

### æ–‡ä»¶3ï¼š`src/tools/solver.py`

**å®ç°å‡½æ•°ï¼ˆ6 ä¸ªï¼‰ï¼š**

1. `_strip_cash(weights, cash_symbol) -> Dict`ï¼ˆå†…éƒ¨ï¼‰
2. `_cap_and_fill(weights, cap, cash_symbol) -> Dict`ï¼ˆå†…éƒ¨ï¼‰â€” å•ä¸€ä»“ä½å°é¡¶ï¼Œè¶…é¢é‡åˆ†é…
3. `_blend_equal(weights, alpha) -> Dict`ï¼ˆå†…éƒ¨ï¼‰â€” å‘ç­‰æƒæ··åˆ
4. `_limit_holdings(weights, max_holdings, cash_symbol) -> Tuple[Dict, bool]`ï¼ˆå†…éƒ¨ï¼‰â€” é™åˆ¶æŒä»“æ•°
5. `_adjust_weights(target, profile, drivers, config) -> Tuple[Dict, List[str]]`ï¼ˆå†…éƒ¨ï¼‰â€” å¯å‘å¼è°ƒæ•´
6. `_solve_lp(target, current, profile, adv_by_symbol, aum, config) -> Optional[Dict]`ï¼ˆå†…éƒ¨ï¼‰â€” CVXPY çº¿æ€§è§„åˆ’

7. `constraint_solver(state: RiskState, config) -> Dict[str, Any]`
   - ä»…åœ¨ `decision == "restrict"` æ—¶è¿è¡Œ
   - ä¼˜å…ˆç”¨ LP æ±‚è§£ï¼Œå¤±è´¥åˆ™ç”¨å¯å‘å¼è°ƒæ•´
   - è¿”å› `{"recommended_actions": [...]}`

### æ–‡ä»¶4ï¼š`src/tools/audit.py`

**å®ç°å‡½æ•°ï¼ˆ2 ä¸ªï¼‰ï¼š**

1. `_hash_payload(payload) -> str`ï¼ˆå†…éƒ¨ï¼‰â€” SHA256 å‰ 16 ä½

2. `audit_log(state: RiskState, config) -> Dict[str, Any]`
   - æ±‡æ€»å®¡è®¡ä¿¡æ¯ï¼š
     - è§„åˆ™å¿«ç…§ + å“ˆå¸Œ
     - å·¥å…·è°ƒç”¨ç»Ÿè®¡ï¼ˆcount / errors / latencyï¼‰
     - LLM ä½¿ç”¨æƒ…å†µ
     - Gatekeeper / Supervisor è°ƒåº¦ä¿¡æ¯
     - Skills ç‰ˆæœ¬ä¿¡æ¯ï¼ˆä» load_skill è·å–ï¼‰
     - åˆè§„ç¦æŠ•æ¸…å•
     - trace_id + timestamp
   - è¿”å› `{"audit": {...}}`

</details>

<details>
<summary><b>ğŸ’» ä»£ç æ¨¡æ¿</b></summary>

### æ–‡ä»¶1ï¼š`src/chains/reducer.py`

```python
"""
ç»“æœæ±‡æ€»æ¨¡å— â€” æ”¶é›†æ‰€æœ‰ findingï¼Œæ¸…æ´—è¯æ®ï¼Œç”Ÿæˆ risk_reportã€‚
"""
from __future__ import annotations

from typing import Dict, Any, List

from ..state import RiskState, Finding
from ..skills_runtime import load_skill


_DEFAULT_EVIDENCE_PREFIXES = ("snapshot_metrics.", "rules.", "tool:")
_FINDING_KEYS = (
    "finding_market", "finding_concentration", "finding_diversification",
    "finding_liquidity", "finding_macro", "finding_compliance",
)
_AGENT_SKILLS = {
    "MarketRiskChain": "risk-market-assessor",
    "ConcentrationChain": "risk-market-assessor",
    "DiversificationChain": "risk-market-assessor",
    "LiquidityChain": "liquidity-execution-assessor",
    "MacroToolCallingAgent": "macro-tool-calling",
    "ComplianceToolCallingAgent": "compliance-evidence",
}


def _is_allowed_ref(ref: str, prefixes: tuple[str, ...]) -> bool:
    """æ£€æŸ¥è¯æ®å¼•ç”¨å‰ç¼€æ˜¯å¦åˆæ³•ã€‚"""
    # TODO: return any(ref.startswith(p) for p in prefixes)
    pass


def _sanitize_evidence(
    state: RiskState,
    evidence: List[Dict[str, Any]],
    gaps: List[Dict[str, Any]],
    prefixes: tuple[str, ...],
) -> List[Dict[str, Any]]:
    """è¿‡æ»¤éæ³•è¯æ®å¼•ç”¨ï¼Œå¯¹ snapshot_metrics.* è¡¥å……å®é™…å€¼ã€‚"""
    # TODO: éå† evidenceï¼Œæ£€æŸ¥ ref å‰ç¼€
    # TODO: éæ³•å¼•ç”¨è®°å½•åˆ° gapsï¼Œåˆæ³•å¼•ç”¨ä¿ç•™
    # TODO: snapshot_metrics.* å¼•ç”¨ä» state ä¸­è¡¥å…… value
    pass


def reducer_chain(state: RiskState) -> Dict[str, Any]:
    """æ±‡æ€»æ‰€æœ‰åˆ†æç»“æœï¼Œç”Ÿæˆ risk_reportã€‚"""
    findings: List[Finding] = []
    evidence_gaps: List[Dict[str, Any]] = []

    # TODO: éå† _FINDING_KEYSï¼Œä» state æ”¶é›† finding
    # TODO: å¯¹æ¯ä¸ª findingï¼ŒæŸ¥ _AGENT_SKILLS æ‰¾åˆ°å¯¹åº” skill
    # TODO: ç”¨ load_skill åŠ è½½ skillï¼Œå– evidence_prefixes
    # TODO: è°ƒç”¨ _sanitize_evidence æ¸…æ´—è¯æ®
    # TODO: å¦‚æœ skill.require_evidenceï¼Œæ£€æŸ¥è¯æ®æ˜¯å¦æœ‰æ•ˆ

    # TODO: è®¡ç®— overall_severity = max(æ‰€æœ‰ finding çš„ severity)
    # TODO: ç”Ÿæˆ summary

    # TODO: æ„é€  risk_report
    # report = {
    #     "overall_severity": overall,
    #     "summary": summary,
    #     "findings": findings,
    #     "data_gaps": (state.get("data_gaps") or []) + evidence_gaps,
    # }

    # TODO: return {"findings": findings, "risk_report": report, "data_gaps": report["data_gaps"]}
    pass
```

### æ–‡ä»¶2ï¼š`src/tools/decision.py`

```python
"""
å†³ç­–å¼•æ“æ¨¡å— â€” ç¡¬è§„åˆ™ä¼˜å…ˆï¼Œé£é™©æŠ¥å‘Šå…œåº•ï¼Œè¾“å‡º pass/warn/restrict/blockã€‚
"""
from __future__ import annotations

from typing import Any, Dict, List

from ..state import RiskState


_LEVEL = {"pass": 0, "warn": 1, "restrict": 2, "block": 3}


def _max_level(findings: List[Dict[str, Any]]) -> int:
    """ä» rule_findings ä¸­å–æœ€å¤§ levelã€‚"""
    # TODO: éå† findingsï¼Œå– f["level"]ï¼ˆæˆ–ä» f["severity"] æ˜ å°„ï¼‰
    # TODO: è¿”å› max(levels)ï¼Œç©ºåˆ—è¡¨è¿”å› 0
    pass


def decision_engine(state: RiskState) -> Dict[str, Any]:
    """å†³ç­–å¼•æ“ï¼šç¡¬è§„åˆ™ä¼˜å…ˆï¼Œé£é™©æŠ¥å‘Šå…œåº•ã€‚"""
    rule_findings = state.get("rule_findings") or []
    risk_report = state.get("risk_report") or {}
    data_quality = state.get("data_quality") or {}

    rule_level = _max_level(rule_findings)
    report_level = int(risk_report.get("overall_severity") or 0)

    # TODO: å†³ç­–é€»è¾‘
    # if rule_level >= 3: decision = "block"
    # elif rule_level >= 2: decision = "restrict"
    # elif report_level >= 2: decision = "restrict"
    # elif report_level >= 1 or data_quality.get("status") == "degraded": decision = "warn"
    # else: decision = "pass"

    # TODO: æå– binding_constraintsï¼ˆseverity ä¸º restrict/block çš„è§„åˆ™ï¼‰

    # TODO: return {"decision": {...}, "binding_constraints": [...]}
    pass
```

### æ–‡ä»¶3ï¼š`src/tools/solver.py`

```python
"""
çº¦æŸæ±‚è§£å™¨æ¨¡å— â€” LP ä¼˜å…ˆï¼Œå¯å‘å¼å…œåº•ï¼Œä»…åœ¨ restrict æ—¶è¿è¡Œã€‚
"""
from __future__ import annotations

from typing import Dict, Any, List, Tuple, Optional

try:
    import cvxpy as cp
except ImportError:
    cp = None

from ..config import RuntimeConfig, DEFAULT_CONFIG
from ..state import RiskState
from .rules import load_rules
from .utils import normalize_weights, compute_hhi, compute_effective_n


# ---------------------------------------------------------------------------
# å¯å‘å¼è¾…åŠ©å‡½æ•°
# ---------------------------------------------------------------------------
def _strip_cash(weights: Dict[str, float], cash_symbol: str) -> Dict[str, float]:
    """å»æ‰ç°é‡‘ä»“ä½ã€‚"""
    # TODO: è¿‡æ»¤æ‰ cash_symbol
    pass


def _cap_and_fill(weights: Dict[str, float], cap: float, cash_symbol: str) -> Dict[str, float]:
    """å•ä¸€ä»“ä½å°é¡¶ï¼Œè¶…é¢é‡åˆ†é…ç»™æœªæ»¡ä»“ä½ï¼Œå‰©ä½™å½’ç°é‡‘ã€‚"""
    # TODO: éå† weightsï¼Œè¶…è¿‡ cap çš„æˆªæ–­
    # TODO: è¶…é¢éƒ¨åˆ†æŒ‰å®¹é‡æ¯”ä¾‹åˆ†é…ç»™æœªæ»¡ä»“ä½
    # TODO: åˆ†é…ä¸å®Œçš„å½’ cash_symbol
    pass


def _blend_equal(weights: Dict[str, float], alpha: float) -> Dict[str, float]:
    """å‘ç­‰æƒæ··åˆï¼š(1-alpha)*åŸæƒé‡ + alpha*ç­‰æƒã€‚"""
    # TODO: equal = 1/n
    # TODO: è¿”å›æ··åˆåçš„æƒé‡
    pass


def _limit_holdings(
    weights: Dict[str, float], max_holdings: Optional[int], cash_symbol: str
) -> Tuple[Dict[str, float], bool]:
    """é™åˆ¶æŒä»“æ•°é‡ï¼Œä¿ç•™æƒé‡æœ€å¤§çš„ top-Nï¼Œå…¶ä½™å½’é›¶åé‡æ–°å½’ä¸€åŒ–ã€‚"""
    # TODO: å¦‚æœ max_holdings æ— æ•ˆæˆ–æŒä»“æ•°æœªè¶…é™ï¼Œç›´æ¥è¿”å›
    # TODO: å¦åˆ™å– top-Nï¼Œnormalize_weights
    pass


def _adjust_weights(
    target_weights: Dict[str, float],
    profile: Dict[str, Any],
    drivers: List[str],
    config: RuntimeConfig,
) -> Tuple[Dict[str, float], List[str]]:
    """å¯å‘å¼è°ƒæ•´ï¼šå°é¡¶ + åˆ†æ•£åŒ–ã€‚"""
    # TODO: å…ˆ _cap_and_fill
    # TODO: å¦‚æœ drivers åŒ…å« concentration/diversificationï¼Œå°è¯• _blend_equal
    # TODO: è¿”å› (adjusted, notes)
    pass


# ---------------------------------------------------------------------------
# CVXPY çº¿æ€§è§„åˆ’
# ---------------------------------------------------------------------------
def _solve_lp(
    target_weights: Dict[str, float],
    current_weights: Dict[str, float],
    profile: Dict[str, Any],
    adv_by_symbol: Dict[str, float],
    aum: Optional[float],
    config: RuntimeConfig,
) -> Optional[Dict[str, float]]:
    """ç”¨ CVXPY æ±‚è§£çº¦æŸä¼˜åŒ–é—®é¢˜ã€‚"""
    if cp is None:
        return None

    # TODO: æ„å»ºå†³ç­–å˜é‡ w = cp.Variable(n)
    # TODO: çº¦æŸï¼šw >= 0, sum(w) == 1, w <= max_single_weight
    # TODO: çº¦æŸï¼šturnover <= max_turnover, position_delta <= max_position_delta
    # TODO: çº¦æŸï¼šadv_ratio é™åˆ¶
    # TODO: ç›®æ ‡å‡½æ•°ï¼šMinimize(|w - target| + turnover_weight * |w - current|)
    # TODO: problem.solve()
    # TODO: è¿”å›æƒé‡å­—å…¸ï¼Œæ±‚è§£å¤±è´¥è¿”å› None
    pass


# ---------------------------------------------------------------------------
# ä¸»å‡½æ•°
# ---------------------------------------------------------------------------
def constraint_solver(state: RiskState, config: RuntimeConfig | None = None) -> Dict[str, Any]:
    """åœ¨ restrict æƒ…å†µä¸‹ç”Ÿæˆè°ƒä»“å»ºè®®ï¼ˆLP ä¼˜å…ˆï¼Œå¯å‘å¼å…œåº•ï¼‰ã€‚"""
    cfg = config or DEFAULT_CONFIG
    decision = (state.get("decision") or {}).get("decision")
    if decision != "restrict":
        return {}

    # TODO: ä» state æå– target_weights, current_weights, rules, aum ç­‰
    # TODO: ä» risk_report æå– driversï¼ˆseverity >= 2 çš„é£é™©ç±»å‹ï¼‰

    # TODO: å…ˆå°è¯• _solve_lp
    # TODO: LP æˆåŠŸåˆ™ _limit_holdings åè¿”å› rebalance action
    # TODO: LP å¤±è´¥åˆ™ _adjust_weights å¯å‘å¼è°ƒæ•´
    # TODO: éƒ½å¤±è´¥åˆ™è¿”å› review_targets guidance

    pass
```

### æ–‡ä»¶4ï¼š`src/tools/audit.py`

```python
"""
å®¡è®¡æ—¥å¿—æ¨¡å— â€” æ±‡æ€»è§„åˆ™å¿«ç…§ã€å·¥å…·è°ƒç”¨ã€æŠ€èƒ½ç‰ˆæœ¬ç­‰å®¡è®¡ä¿¡æ¯ã€‚
"""
from __future__ import annotations

import hashlib
import json
from datetime import datetime, timezone
from typing import Any, Dict

from ..state import RiskState
from ..skills_runtime import load_skill
from .rules import load_rules
from ..config import RuntimeConfig, DEFAULT_CONFIG


def _hash_payload(payload: Dict[str, Any]) -> str:
    """JSON åºåˆ—åŒ–å SHA256 å–å‰ 16 ä½ã€‚"""
    # TODO: json.dumps(payload, sort_keys=True, separators=(",", ":"))
    # TODO: hashlib.sha256 â†’ hexdigest()[:16]
    pass


def audit_log(state: RiskState, config: RuntimeConfig | None = None) -> Dict[str, Any]:
    """æ±‡æ€»å®¡è®¡ä¿¡æ¯ï¼Œè¿”å› {"audit": {...}}ã€‚"""
    runtime = config or DEFAULT_CONFIG

    # TODO: ä» state æå–åŸºç¡€ä¿¡æ¯
    # snapshot = state.get("snapshot_metrics") or {}
    # rule_findings = state.get("rule_findings") or []
    # profile = (state.get("normalized") or {}).get("policy_profile", "default")

    # TODO: åŠ è½½è§„åˆ™å¿«ç…§
    # rules, ruleset_version = load_rules(profile, runtime)

    # TODO: æ±‡æ€»å·¥å…·è°ƒç”¨ï¼ˆmacro + complianceï¼‰
    # tool_calls = []
    # tool_calls.extend(state.get("tool_calls_macro") or [])
    # tool_calls.extend(state.get("tool_calls_compliance") or [])
    # tool_latency = sum(t.get("latency_ms", 0) for t in tool_calls)
    # tool_error_count = sum(1 for t in tool_calls if t.get("error"))

    # TODO: æ£€æŸ¥ LLM ä½¿ç”¨æƒ…å†µ
    # llm_used = bool(state.get("llm_used_macro") or ...)
    # models = [state.get(key) for key in ("llm_model_macro", ...) if state.get(key)]

    # TODO: æ”¶é›† skills ç‰ˆæœ¬ä¿¡æ¯
    # skill_map = {"finding_market": "risk-market-assessor", ...}
    # éå† skill_mapï¼Œå¯¹æœ‰ finding çš„ skill è°ƒç”¨ load_skill è·å–ç‰ˆæœ¬

    # TODO: æ„é€  audit dictï¼ŒåŒ…å«ï¼š
    # - policy_profile, ruleset_version, rules_snapshot, rules_snapshot_hash
    # - data_snapshot_hash
    # - tool_calls, tool_call_summary (count/errors/total_latency_ms)
    # - llm_used, llm_model
    # - gatekeeper_used, gatekeeper_rationale, candidate_nodes
    # - supervisor_used, supervisor_rationale, nodes_to_run
    # - skills_used, node_outputs
    # - timestamp (UTC ISO), trace_id (_hash_payload)

    # TODO: å¯é€‰ï¼šæ·»åŠ  compliance_blocklist ç›¸å…³ä¿¡æ¯

    # TODO: return {"audit": audit}
    pass
```

</details>

<details>
<summary><b>ğŸ§ª æµ‹è¯•æ£€æŸ¥</b></summary>

### æµ‹è¯•1ï¼šReducer æ±‡æ€»

```python
from src.chains.reducer import reducer_chain

state = {
    "finding_market": {"agent": "MarketRiskChain", "severity": 1, "summary": "ok", "evidence": []},
    "finding_concentration": {"agent": "ConcentrationChain", "severity": 0, "summary": "ok", "evidence": []},
    "finding_liquidity": {"agent": "LiquidityChain", "severity": 2, "summary": "risk", "evidence": []},
}

result = reducer_chain(state)
assert "risk_report" in result
assert result["risk_report"]["overall_severity"] == 2
assert len(result["findings"]) == 3

print("âœ… Reducer æµ‹è¯•é€šè¿‡")
```

### æµ‹è¯•2ï¼šDecision å†³ç­–é€»è¾‘

```python
from src.tools.decision import decision_engine

# æµ‹è¯•ï¼šæ— é£é™©
state = {"rule_findings": [], "risk_report": {"overall_severity": 0}, "data_quality": {}}
result = decision_engine(state)
assert result["decision"]["decision"] == "pass"

# æµ‹è¯•ï¼šè§„åˆ™è§¦å‘ restrict
state = {
    "rule_findings": [{"severity": "restrict", "level": 2, "rule_id": "test"}],
    "risk_report": {"overall_severity": 0},
    "data_quality": {},
}
result = decision_engine(state)
assert result["decision"]["decision"] == "restrict"

# æµ‹è¯•ï¼šè§„åˆ™è§¦å‘ block
state = {
    "rule_findings": [{"severity": "block", "level": 3, "rule_id": "blocklist"}],
    "risk_report": {"overall_severity": 0},
    "data_quality": {},
}
result = decision_engine(state)
assert result["decision"]["decision"] == "block"

print("âœ… Decision æµ‹è¯•é€šè¿‡")
```

### æµ‹è¯•3ï¼šSolver ä»… restrict æ—¶è¿è¡Œ

```python
from src.tools.solver import constraint_solver

# decision != restrict æ—¶è¿”å›ç©º
state = {"decision": {"decision": "pass"}}
result = constraint_solver(state)
assert result == {}

print("âœ… Solver è·³è¿‡æµ‹è¯•é€šè¿‡")
```

### æµ‹è¯•4ï¼šAudit å®¡è®¡æ—¥å¿—

```python
from src.tools.audit import audit_log

state = {
    "normalized": {"policy_profile": "default"},
    "snapshot_metrics": {"hhi": 0.5},
    "rule_findings": [],
    "tool_calls_macro": [],
    "tool_calls_compliance": [],
}

result = audit_log(state)
assert "audit" in result
audit = result["audit"]
assert "timestamp" in audit
assert "trace_id" in audit
assert "policy_profile" in audit
assert audit["policy_profile"] == "default"

print("âœ… Audit æµ‹è¯•é€šè¿‡")
```

**æ£€æŸ¥é¡¹ï¼š**
- [ ] `reducer_chain` è¿”å› `{"findings": ..., "risk_report": ..., "data_gaps": ...}`
- [ ] `decision_engine` è¿”å› `{"decision": {...}, "binding_constraints": [...]}`
- [ ] `constraint_solver` åœ¨ decision != "restrict" æ—¶è¿”å›ç©º dict
- [ ] `audit_log` è¿”å› `{"audit": {...}}`ï¼ŒåŒ…å« trace_id å’Œ timestamp
- [ ] æ‰€æœ‰å‡½æ•°è¿”å› Dictï¼Œä¸ä¿®æ”¹ state

</details>

<details>
<summary><b>ğŸ’¡ æç¤ºä¸æŠ€å·§</b></summary>

**å…³é”®ç‚¹ï¼š**
1. **æ‰€æœ‰å‡½æ•°è¿”å› Dictï¼Œä¸ä¿®æ”¹ state** â€” ä¸æ—§æ¨¡æ¿ä¸­ `return state` çš„æ¨¡å¼å®Œå…¨ä¸åŒ
2. **reducer åšè¯æ®æ¸…æ´—** â€” ä¸æ˜¯ç®€å•æ±‡æ€»ï¼Œè¿˜è¦ç”¨ skill çš„ `evidence_prefixes` è¿‡æ»¤éæ³•å¼•ç”¨
3. **å†³ç­–ä¸¤å±‚é€»è¾‘** â€” å…ˆçœ‹ `rule_level`ï¼ˆç¡¬è§„åˆ™ï¼‰ï¼Œå†çœ‹ `report_level`ï¼ˆé£é™©æŠ¥å‘Šï¼‰
4. **solver ä¸‰çº§é™çº§** â€” LP æ±‚è§£ â†’ å¯å‘å¼è°ƒæ•´ â†’ review_targets guidance
5. **CVXPY å¯é€‰ä¾èµ–** â€” `try: import cvxpy` å¤±è´¥æ—¶ `_solve_lp` è¿”å› Noneï¼Œèµ°å¯å‘å¼
6. **audit ç”¨ load_skill è·å–ç‰ˆæœ¬** â€” skills_used ä¸æ˜¯æ•°å­—ï¼Œæ˜¯åŒ…å« name/policy_version/skills_hash çš„åˆ—è¡¨

**å¸¸è§é”™è¯¯ï¼š**
- âŒ `reducer_chain` è¿”å› `state` è€Œé Dict â€” åº”è¿”å› `{"findings": ..., "risk_report": ...}`
- âŒ `decision_engine` ç›´æ¥ç”¨ `risk_report["overall_severity"]` åšå†³ç­– â€” åº”å…ˆçœ‹ `rule_level`
- âŒ `constraint_solver` åœ¨ decision != "restrict" æ—¶ä»ç„¶è¿è¡Œ â€” åº”ç›´æ¥è¿”å› `{}`
- âŒ `audit_log` ç”¨ `uuid` ç”Ÿæˆ trace_id â€” åº”è¯¥ç”¨ `_hash_payload` åŸºäºæ—¶é—´æˆ³ç”Ÿæˆ

**å‚è€ƒèµ„æºï¼š**
- [CVXPY å®˜æ–¹æ–‡æ¡£](https://www.cvxpy.org/)
- `src/tools/utils.py` ä¸­çš„ `normalize_weights`ã€`compute_hhi`ã€`compute_effective_n`

</details>

---

## ğŸ“ æ¨¡å—10: é˜ˆå€¼æ ¡å‡†

<details open>
<summary><b>ğŸ“‹ æ¨¡å—ä¿¡æ¯</b></summary>

| é¡¹ç›® | å†…å®¹ |
|:---|:---|
| **éš¾åº¦** | â­â­â­ ä¸­çº§ |
| **æ–‡ä»¶æ•°** | 2ä¸ª |
| **æ–‡ä»¶è·¯å¾„** | `src/tools/calibrate_rules.py`<br>`src/tools/calibrate_macro_series.py` |
| **ä¾èµ–æ¨¡å—** | æ¨¡å—3 |
| **è¢«ä¾èµ–** | æ¨¡å—5, 6, 8 |

</details>

<details>
<summary><b>ğŸ¯ å­¦ä¹ ç›®æ ‡</b></summary>

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ å°†æŒæ¡ï¼š
- [ ] è’™ç‰¹å¡æ´›é‡‡æ ·æ–¹æ³•
- [ ] Dirichlet åˆ†å¸ƒåº”ç”¨
- [ ] åˆ†ä½æ•°é˜ˆå€¼ç”Ÿæˆ
- [ ] æ—¶åºæ•°æ®å¤„ç†

</details>

<details>
<summary><b>ğŸ“– åŠŸèƒ½è¯´æ˜</b></summary>

æœ¬æ¨¡å—è´Ÿè´£è‡ªåŠ¨æ ¡å‡†è§„åˆ™é˜ˆå€¼ï¼Œç¡®ä¿é˜ˆå€¼ç¬¦åˆå®é™…æ•°æ®åˆ†å¸ƒã€‚

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
1. **ç»„åˆè§„åˆ™æ ¡å‡†**ï¼šåŸºäºå†å²æ•°æ®éšæœºé‡‡æ ·ç”Ÿæˆé˜ˆå€¼
2. **å®è§‚åºåˆ—æ ¡å‡†**ï¼šåŸºäºæ—¶åºæ•°æ®å˜åŒ–å¹…åº¦ç”Ÿæˆé˜ˆå€¼

**ä¸ºä»€ä¹ˆé‡è¦ï¼š**
é˜ˆå€¼ç›´æ¥å½±å“é£é™©åˆ¤æ–­çš„å‡†ç¡®æ€§ï¼Œè‡ªåŠ¨æ ¡å‡†é¿å…äººå·¥è®¾å®šçš„ä¸»è§‚æ€§ã€‚

</details>

<details>
<summary><b>âœ… å®ç°è¦æ±‚</b></summary>

### æ–‡ä»¶1ï¼š`src/tools/calibrate_rules.py`

**å®ç° `calibrate_rules(asof_date, n, *, samples, seed) -> Dict[str, Any]` åŠè¾…åŠ©å‡½æ•°**

éœ€è¦å®ç°ä»¥ä¸‹å‡½æ•°ï¼š

| å‡½æ•° | è¯´æ˜ |
|:---|:---|
| `_env_float(name, default) -> float` | ä»ç¯å¢ƒå˜é‡è¯»å–æµ®ç‚¹æ•°ï¼Œå¤±è´¥å›é€€é»˜è®¤å€¼ |
| `_load_blocklist(path, fallback) -> List[str]` | ä»å·²æœ‰ rules.yaml åŠ è½½ blocklistï¼Œæ–‡ä»¶ä¸å­˜åœ¨åˆ™ç”¨ fallback |
| `_percentile(values, p) -> float` | çº¯ Python åˆ†ä½æ•°è®¡ç®—ï¼ˆä¸ä¾èµ– numpyï¼‰ï¼Œçº¿æ€§æ’å€¼ |
| `_dirichlet(n, rng) -> List[float]` | çº¯ Python Dirichlet é‡‡æ ·ï¼Œä½¿ç”¨ `rng.gammavariate(1.0, 1.0)` |
| `_load_market_metrics_range(start_date, end_date)` | è°ƒç”¨ `csv_data.market_metrics_by_range` åŠ è½½å¸‚åœºæŒ‡æ ‡ |
| `_simulate(codes, metrics, n, samples, seed)` | è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿï¼šéšæœºæŠ½æ · ETF + Dirichlet æƒé‡ï¼Œè®¡ç®— 6 ç±»æŒ‡æ ‡ |
| `_build_rules(series, *, high_warn, high_restrict, low_warn, low_restrict, blocklist)` | ç”¨åˆ†ä½æ•°ç”Ÿæˆä¸€å¥—å®Œæ•´è§„åˆ™ï¼ˆ17 ä¸ªé˜ˆå€¼ + blocklistï¼‰ |
| `calibrate_rules(asof_date, n, *, samples, seed)` | ä¸»å‡½æ•°ï¼šåŠ è½½æ•°æ® â†’ æ¨¡æ‹Ÿ â†’ ç”Ÿæˆ default + conservative ä¸¤å¥—è§„åˆ™ â†’ å†™å…¥ YAML |
| `main()` | CLI å…¥å£ï¼Œargparse è§£æå‚æ•° |

æ ¡å‡†æµç¨‹ï¼š
1. è§£æ asof_dateï¼Œè®¡ç®—æ•°æ®çª—å£ `[å¹´åˆ, asof_date]`
2. è°ƒç”¨ `_load_market_metrics_range` åŠ è½½ ETF å¸‚åœºæŒ‡æ ‡ï¼ˆvolatility, adv, spread_bpsï¼‰
3. `_simulate` è’™ç‰¹å¡æ´›é‡‡æ ·ï¼šéšæœºæŠ½ n åª ETF + Dirichlet æƒé‡ï¼Œè®¡ç®— 6 ç±»ç»„åˆæŒ‡æ ‡
4. `_build_rules` å¯¹æ¯ç±»æŒ‡æ ‡å–åˆ†ä½æ•°ç”Ÿæˆé˜ˆå€¼ï¼ˆé«˜æŒ‡æ ‡ç”¨ high åˆ†ä½æ•°ï¼Œä½æŒ‡æ ‡ç”¨ low åˆ†ä½æ•°ï¼‰
5. ç”Ÿæˆ default + conservative ä¸¤å¥—è§„åˆ™ï¼ˆåˆ†ä½æ•°å‚æ•°ä¸åŒï¼Œä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
6. å†™å…¥ `cufel_practice_data/rules.yaml`ï¼Œè¿”å› `{"rules_path": ..., "profiles": [...]}`

### æ–‡ä»¶2ï¼š`src/tools/calibrate_macro_series.py`

**å®ç° `calibrate_macro_series(asof_date, *, lookback_days, warn_pctl, restrict_pctl, min_samples, config_path) -> Dict[str, Any]` åŠè¾…åŠ©å‡½æ•°**

éœ€è¦å®ç°ä»¥ä¸‹å‡½æ•°ï¼š

| å‡½æ•° | è¯´æ˜ |
|:---|:---|
| `_parse_date(value) -> datetime \| None` | å¤šæ ¼å¼æ—¥æœŸè§£æï¼ˆYYYY-MM-DD / YYYYMMDD / ISOï¼‰ |
| `_format_tushare_date(value) -> str` | datetime â†’ YYYYMMDD æ ¼å¼ |
| `_format_tushare_year_start(value) -> str` | datetime â†’ å¹´åˆæ—¥æœŸ YYYYMMDD |
| `_percentile(values, p) -> float` | çº¯ Python åˆ†ä½æ•°è®¡ç®—ï¼Œçº¿æ€§æ’å€¼ |
| `_load_config(path) -> Dict` | åŠ è½½ macro_series.yaml é…ç½®ï¼Œæ ¡éªŒ series æ˜ å°„ |
| `_get_tushare_client()` | ä»ç¯å¢ƒå˜é‡è·å– tokenï¼Œè¿”å› `(pro, error)` å…ƒç»„ |
| `_fetch_series(pro, series, config, asof_date)` | è°ƒç”¨ Tushare API æ‹‰å–å•ä¸ªåºåˆ—ï¼Œè¿”å› `(rows, error)` |
| `_filter_window(rows, asof_date, lookback_days)` | æŒ‰æ—¶é—´çª—å£è¿‡æ»¤æ•°æ®ç‚¹ |
| `_compute_changes(values, mode, scale)` | è®¡ç®—ç›¸é‚»å€¼å˜åŒ–å¹…åº¦ï¼Œæ”¯æŒ pct/abs æ¨¡å¼å’Œ bp ç¼©æ”¾ |
| `calibrate_macro_series(asof_date, *, ...)` | ä¸»å‡½æ•°ï¼šéå†åºåˆ— â†’ æ‹‰æ•°æ® â†’ ç®—å˜åŒ– â†’ å–åˆ†ä½æ•° â†’ å›å†™ YAML |
| `main()` | CLI å…¥å£ï¼Œargparse è§£æå‚æ•° |

æ ¡å‡†æµç¨‹ï¼š
1. åŠ è½½ `macro_series.yaml` é…ç½®ï¼Œè·å–æ‰€æœ‰åºåˆ—å®šä¹‰
2. åˆå§‹åŒ– Tushare å®¢æˆ·ç«¯ï¼ˆä» `TUSHARE_TOKEN` ç¯å¢ƒå˜é‡ï¼‰
3. å¯¹æ¯ä¸ªåºåˆ—ï¼šæ‹‰å–æ•°æ® â†’ æŒ‰æ—¶é—´çª—å£è¿‡æ»¤ â†’ è®¡ç®—å˜åŒ–å¹…åº¦
4. å¯¹å˜åŒ–å¹…åº¦å– warn/restrict åˆ†ä½æ•°ï¼Œå›å†™åˆ°é…ç½®çš„ `warn_pct_change` / `restrict_pct_change`
5. å°†æ›´æ–°åçš„å®Œæ•´é…ç½®å†™å› YAML æ–‡ä»¶ï¼Œè¿”å› `{"config_path": ..., "updated": {...}}`

</details>

<details>
<summary><b>ğŸ’» ä»£ç æ¨¡æ¿</b></summary>

### æ–‡ä»¶1ï¼š`src/tools/calibrate_rules.py`

```python
"""
ç»„åˆè§„åˆ™é˜ˆå€¼æ ¡å‡†æ¨¡å—
"""
from __future__ import annotations

import argparse
import json
import math
import os
import random
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Tuple

import yaml

from .csv_data import market_metrics_by_range


def _percentile(values: List[float], p: float) -> float:
    """
    è®¡ç®—åˆ†ä½æ•°ï¼ˆçº¯ Python å®ç°ï¼Œçº¿æ€§æ’å€¼ï¼‰ã€‚

    Args:
        values: æ•°å€¼åˆ—è¡¨
        p: åˆ†ä½æ•° (0.0 - 1.0)
    """
    if not values:
        return 0.0
    v = sorted(values)
    # TODO: å®ç°åˆ†ä½æ•°è®¡ç®—é€»è¾‘
    # k = (len(v) - 1) * p
    # f = math.floor(k)
    # c = math.ceil(k)
    # çº¿æ€§æ’å€¼è¿”å› result
    pass


def _dirichlet(n: int, rng: random.Random) -> List[float]:
    """
    ç”Ÿæˆ Dirichlet åˆ†å¸ƒæ ·æœ¬ï¼ˆçº¯ Python å®ç°ï¼‰ã€‚
    ä½¿ç”¨ rng.gammavariate(1.0, 1.0) ç”Ÿæˆ n ä¸ªéšæœºæ•°å¹¶å½’ä¸€åŒ–ã€‚
    """
    # TODO: å®ç° Dirichlet é‡‡æ ·
    # raw = [rng.gammavariate(1.0, 1.0) for _ in range(n)]
    # total = sum(raw)
    # return [v / total for v in raw]
    pass


def _simulate(
    codes: List[str],
    metrics: Dict[str, Dict[str, float]],
    n: int,
    samples: int,
    seed: str | None,
) -> Dict[str, List[float]]:
    """
    è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿï¼šéšæœºç”Ÿæˆç»„åˆå¹¶è®¡ç®—æŒ‡æ ‡åˆ†å¸ƒã€‚

    Returns:
        Dict[metric_name, List[values]]
    """
    rng = random.Random(seed) if seed else random

    # ç»“æœå®¹å™¨
    vols: List[float] = []
    spreads: List[float] = []
    advs: List[float] = []
    hhis: List[float] = []
    tops: List[float] = []
    effns: List[float] = []

    # TODO: å¾ªç¯ samples æ¬¡
    # 1. éšæœºæŠ½å– n ä¸ªä»£ç : rng.sample(codes, n)
    # 2. ç”Ÿæˆæƒé‡: _dirichlet(n, rng)
    # 3. è®¡ç®—ç»„åˆæŒ‡æ ‡:
    #    - hhi = sum(w^2)
    #    - effn = 1/hhi
    #    - top = max(w)
    #    - vol = sum(w * m["volatility"])
    #    - spread = sum(w * m["spread_bps"])
    #    - adv = sum(w * m["adv"])
    # 4. å­˜å…¥ç»“æœåˆ—è¡¨

    return {
        "volatility": vols,
        "spread_bps": spreads,
        "adv": advs,
        "hhi": hhis,
        "top_weight": tops,
        "effective_n": effns,
    }


def _build_rules(
    series: Dict[str, List[float]],
    *,
    high_warn: float,
    high_restrict: float,
    low_warn: float,
    low_restrict: float,
    blocklist: List[str],
) -> Dict[str, Any]:
    """
    æ ¹æ®æ¨¡æ‹Ÿåˆ†å¸ƒç”Ÿæˆè§„åˆ™é˜ˆå€¼ã€‚
    """
    return {
        # TODO: ä½¿ç”¨ _percentile è®¡ç®—å„æŒ‡æ ‡é˜ˆå€¼
        # æ³¨æ„ï¼š
        # - é«˜é£é™©æŒ‡æ ‡ (vol, spread, hhi, top) ä½¿ç”¨ high_warn/high_restrict
        # - ä½é£é™©æŒ‡æ ‡ (adv, effective_n) ä½¿ç”¨ low_warn/low_restrict
        "max_single_weight": 0.0,
        "max_hhi": 0.0,
        "max_portfolio_volatility": 0.0,
        "max_weighted_spread_bps": 0.0,
        "min_weighted_adv": 0.0,

        "volatility_warn": 0.0,
        "volatility_restrict": 0.0,

        # ... å…¶ä»–æŒ‡æ ‡

        "blocklist": list(blocklist),
    }


def calibrate_rules(asof_date: str, n: int, *, samples: int | None = None, seed: str | None = None) -> Dict[str, Any]:
    """
    ä¸»å‡½æ•°ï¼šåŸºäºå†å²æ•°æ®æ ¡å‡†ç»„åˆè§„åˆ™é˜ˆå€¼ã€‚
    """
    # 1. è§£ææ—¥æœŸ
    # start_date = f"{year}-01-01"

    # 2. åŠ è½½æ•°æ®
    # codes, metrics = _load_market_metrics_range(start_date, asof_date)

    # 3. è¿è¡Œæ¨¡æ‹Ÿ
    # series = _simulate(codes, metrics, n, samples, seed)

    # 4. å®šä¹‰åˆ†ä½æ•°å‚æ•° (ä»ç¯å¢ƒå˜é‡è¯»å–æˆ–ä½¿ç”¨é»˜è®¤å€¼)
    # high_warn = 0.8, high_restrict = 0.9
    # low_warn = 0.2, low_restrict = 0.1

    # 5. ç”Ÿæˆä¸¤å¥—è§„åˆ™
    # rules = {
    #     "default": _build_rules(...),
    #     "conservative": _build_rules(...),
    # }

    # 6. ä¿å­˜åˆ° rules.yaml
    # Path(...).write_text(...)

    return {"profiles": list(rules.keys())}


def main() -> None:
    parser = argparse.ArgumentParser(description="Calibrate rules from ETF history.")
    parser.add_argument("--asof-date", type=str, required=True, help="YYYY-MM-DD")
    parser.add_argument("--n", type=int, default=5, help="portfolio size")
    parser.add_argument("--samples", type=int, default=5000)
    parser.add_argument("--seed", type=str, default=None)
    args = parser.parse_args()

    result = calibrate_rules(
        args.asof_date,
        args.n,
        samples=args.samples,
        seed=args.seed,
    )
    print(json.dumps(result, ensure_ascii=False))


if __name__ == "__main__":
    main()
```

### æ–‡ä»¶2ï¼š`src/tools/calibrate_macro_series.py`

```python
"""
å®è§‚åºåˆ—é˜ˆå€¼æ ¡å‡†æ¨¡å—
"""
from __future__ import annotations

import argparse
import json
import os
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Dict, List, Tuple

import tushare as ts
import yaml


def _parse_date(value: Any) -> datetime | None:
    """è§£æå¤šç§æ ¼å¼çš„æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD, YYYYMMDD, ISO)ã€‚"""
    pass


def _get_tushare_client() -> Tuple[Any | None, str | None]:
    """
    åˆå§‹åŒ– Tushare å®¢æˆ·ç«¯ã€‚
    ä»ç¯å¢ƒå˜é‡ TUSHARE_TOKEN è¯»å–ã€‚
    Returns: (pro_api, error_msg)
    """
    token = os.getenv("TUSHARE_TOKEN", "").strip()
    if not token:
        return None, "TUSHARE_TOKEN not configured"
    ts.set_token(token)
    return ts.pro_api(), None


def _fetch_series(
    pro: Any,
    series: str,
    config: Dict[str, Any],
    asof_date: datetime | None,
) -> Tuple[List[Tuple[datetime, float]], str | None]:
    """
    è°ƒç”¨ Tushare API æ‹‰å–å•æ¡åºåˆ—æ•°æ®ã€‚

    Args:
        config: series é…ç½® (api, params, fields ç­‰)
    Returns:
        (rows, error) å…¶ä¸­ rows æ˜¯ (date, value) çš„åˆ—è¡¨
    """
    # TODO: è§£æé…ç½®å‚æ•°
    # api_name = config.get("api")
    # params = config.get("params")
    # fields = config.get("fields")

    # TODO: è°ƒç”¨ Tushare
    # api = getattr(pro, api_name)
    # df = api(**params, fields=fields)

    # TODO: è½¬æ¢ä¸º (date, value) åˆ—è¡¨
    # éå† df.iterrows()
    # è§£ææ—¥æœŸå’Œæ•°å€¼
    # å¤„ç† bid/ask å‡å€¼é€»è¾‘

    rows = []
    return rows, None


def _filter_window(
    rows: List[Tuple[datetime, float]],
    asof_date: datetime | None,
    lookback_days: int | None,
) -> List[Tuple[datetime, float]]:
    """æŒ‰æ—¶é—´çª—å£è¿‡æ»¤æ•°æ®ã€‚"""
    if not rows:
        return rows
    # TODO: è¿‡æ»¤é€»è¾‘
    # keep if start_date <= date <= asof_date
    return rows


def _compute_changes(values: List[float], mode: str, scale: str | None) -> List[float]:
    """
    è®¡ç®—ç›¸é‚»å€¼å˜åŒ–å¹…åº¦ã€‚

    Args:
        mode: "pct" (ç™¾åˆ†æ¯”) æˆ– "abs" (ç»å¯¹å€¼)
        scale: "bp" (åŸºç‚¹) æˆ– None
    """
    changes: List[float] = []
    # TODO: è®¡ç®—é€»è¾‘
    # for i in range(1, len(values)):
    #     prev, curr = values[i-1], values[i]
    #     change = abs(curr - prev) if mode == "abs" else abs((curr - prev) / prev)
    #     if scale == "bp": change *= 100
    #     changes.append(change)
    return changes


def calibrate_macro_series(
    asof_date: str,
    *,
    lookback_days: int | None = None,
    warn_pctl: float | None = None,
    restrict_pctl: float | None = None,
    min_samples: int | None = None,
    config_path: str | None = None,
) -> Dict[str, Any]:
    """
    åŸºäºå®è§‚æ—¶åºæ•°æ®æ ¡å‡†å®è§‚æŒ‡æ ‡é˜ˆå€¼ã€‚

    æµç¨‹ï¼š
    1. åŠ è½½ config_path (macro_series.yaml)
    2. åˆå§‹åŒ– Tushare
    3. éå†æ¯ä¸ª series:
       - _fetch_series æ‹‰å–æ•°æ®
       - _filter_window è¿‡æ»¤çª—å£
       - _compute_changes è®¡ç®—æ³¢åŠ¨
       - _percentile è®¡ç®—åˆ†ä½æ•° (warn_pctl, restrict_pctl)
       - æ›´æ–° config å­—å…¸ä¸­çš„ warn_pct_change / restrict_pct_change
    4. å†™å› yaml æ–‡ä»¶
    """
    # 1. è§£æå‚æ•°
    # asof = _parse_date(asof_date)
    # config = _load_config(path)

    # 2. éå†å¤„ç†
    updated = {}
    # for series, cfg in config["series"].items():
    #     rows, err = _fetch_series(...)
    #     changes = _compute_changes(...)
    #     warn = _percentile(changes, warn_pctl)
    #     restrict = _percentile(changes, restrict_pctl)
    #     cfg["warn_pct_change"] = warn
    #     cfg["restrict_pct_change"] = restrict

    # 3. ä¿å­˜å¹¶è¿”å›
    # path.write_text(yaml.dump(config))
    return {"updated": updated}


def main() -> None:
    parser = argparse.ArgumentParser(description="Calibrate macro series thresholds.")
    parser.add_argument("--asof-date", default=os.getenv("ASOF_DATE", ""))
    parser.add_argument("--lookback-days", type=int, default=365 * 3)
    parser.add_argument("--warn-pctl", type=float, default=0.9)
    parser.add_argument("--restrict-pctl", type=float, default=0.98)
    parser.add_argument("--min-samples", type=int, default=30)
    parser.add_argument("--config", default="")
    args = parser.parse_args()

    result = calibrate_macro_series(
        args.asof_date or datetime.now().date().isoformat(),
        lookback_days=args.lookback_days,
        warn_pctl=args.warn_pctl,
        restrict_pctl=args.restrict_pctl,
        min_samples=args.min_samples,
        config_path=args.config or None,
    )
    print(json.dumps(result, ensure_ascii=False))


if __name__ == "__main__":
    main()
```

</details>

<details>
<summary><b>ğŸ§ª æµ‹è¯•æ£€æŸ¥</b></summary>

### æµ‹è¯•æ ¡å‡†æµç¨‹

```bash
# æ ¡å‡†ç»„åˆè§„åˆ™ (ç”Ÿæˆçš„ rules.yaml ä¼šåŒ…å« default å’Œ conservative ä¸¤å¥—é…ç½®)
uv run --env-file .env -- python -u -m src.tools.calibrate_rules \
  --asof-date 2025-11-14 \
  --n 5 \
  --samples 1000 \
  --seed 42

# æ ¡å‡†å®è§‚åºåˆ— (ä¼šè¯»å– TUSHARE_TOKEN å¹¶æ›´æ–° macro_series.yaml)
uv run --env-file .env -- python -u -m src.tools.calibrate_macro_series \
  --asof-date 2025-11-14 \
  --lookback-days 1095
```

**æ£€æŸ¥é¡¹ï¼š**
- [ ] `cufel_practice_data/rules.yaml` å·²æ›´æ–°ï¼ŒåŒ…å« blocklist å’Œå®Œæ•´çš„é˜ˆå€¼
- [ ] `cufel_practice_data/macro_series.yaml` å·²æ›´æ–°ï¼Œå„åºåˆ—çš„ `warn_pct_change` å’Œ `restrict_pct_change` å·²å¡«å……
- [ ] é˜ˆå€¼æ•°å€¼åˆç† (restrict > warn)
- [ ] `macro_series.yaml` ä¸­æ²¡æœ‰ "error" å­—æ®µ

</details>

<details>
<summary><b>ğŸ’¡ æç¤ºä¸æŠ€å·§</b></summary>

**å…³é”®ç‚¹ï¼š**
1. **çº¯ Python Dirichlet é‡‡æ ·**ï¼š
   ä¸ä¾èµ– numpyï¼Œä½¿ç”¨ gamma åˆ†å¸ƒç”Ÿæˆï¼š
   ```python
   def _dirichlet(n, rng):
       raw = [rng.gammavariate(1.0, 1.0) for _ in range(n)]
       total = sum(raw)
       return [x / total for x in raw]
   ```

2. **çº¯ Python åˆ†ä½æ•°è®¡ç®—**ï¼š
   å…ˆæ’åºï¼Œå†çº¿æ€§æ’å€¼ï¼š
   ```python
   def _percentile(values, p):
       v = sorted(values)
       k = (len(v) - 1) * p
       f = int(k)  # floor
       c = math.ceil(k)
       if f == c: return v[int(k)]
       return v[f] + (v[c] - v[f]) * (k - f)
   ```

3. **Tushare é”™è¯¯å¤„ç†**ï¼š
   - ç½‘ç»œè¶…æ—¶æˆ– Token é”™è¯¯æ—¶ï¼Œä¸è¦è®©æ•´ä¸ªç¨‹åºå´©æºƒï¼Œè®°å½• error åˆ°ç»“æœä¸­
   - æ•°æ®ä¸è¶³ (`len(changes) < min_samples`) æ—¶è·³è¿‡æ ¡å‡†

**å¸¸è§é”™è¯¯ï¼š**
- âŒ å¿˜è®°è®¾ç½® `TUSHARE_TOKEN` ç¯å¢ƒå˜é‡
- âŒ å®è§‚æ•°æ®å˜åŒ–å¹…åº¦è®¡ç®—é”™è¯¯ï¼ˆç™¾åˆ†æ¯” vs ç»å¯¹å€¼ï¼‰
- âŒ `rules.yaml` æ ¼å¼é”™è¯¯ï¼ˆç¼©è¿›ä¸æ­£ç¡®ï¼‰

**å‚è€ƒèµ„æºï¼š**
- [Dirichlet Distribution (Wikipedia)](https://en.wikipedia.org/wiki/Dirichlet_distribution)
- [Tushare Pro API æ–‡æ¡£](https://tushare.pro/document/2)

</details>

---

##  ğŸ§© æ‹“å±•é¢˜ï¼šæ›´ç§‘å­¦çš„é˜ˆå€¼ç”Ÿæˆä¸äºŒæ¬¡æ ¡å‡†

æœ¬æ‹“å±•é¢˜é¼“åŠ±ä½ åœ¨â€œç»Ÿè®¡åŸºçº¿â€çš„åŸºç¡€ä¸Šè®¾è®¡æ›´å¯é çš„é˜ˆå€¼ç”Ÿæˆè§„åˆ™ï¼Œå¹¶ç»“åˆé£æ§æ”¿ç­–æˆ–ä¸šåŠ¡çº¦æŸåšäºŒæ¬¡æ ¡å‡†ã€‚

### èƒŒæ™¯è¯´æ˜
åŸºç¡€æ–¹æ¡ˆé€šå¸¸æ˜¯ï¼šåœ¨å†å²çª—å£å†…éšæœºæŠ½æ ·ç»„åˆï¼ˆå¦‚ Dirichlet æƒé‡ï¼‰ï¼Œè®¡ç®—æ³¢åŠ¨ç‡/HHI/ADV/spread ç­‰æŒ‡æ ‡ï¼Œå¾—åˆ°ç»éªŒåˆ†å¸ƒï¼Œå†å–åˆ†ä½æ•°ä½œä¸º warn/restrict é˜ˆå€¼ã€‚  
è¯¥æ–¹æ³•æ˜¯**ç»Ÿè®¡åŸºçº¿**ï¼Œä½†ä¸ç­‰åŒäºç›‘ç®¡æ ‡å‡†ï¼Œå®é™…è½åœ°éœ€è¦äºŒæ¬¡æ ¡å‡†ã€‚

### ä½ çš„ä»»åŠ¡
è®¾è®¡ä¸€ä¸ªæ›´ç§‘å­¦çš„é˜ˆå€¼é…ç½®ç”Ÿæˆæµç¨‹ï¼Œå¹¶è¯´æ˜å®ƒä¸ºä½•æ›´ç¨³å¥ã€‚

### è®¾è®¡è¦ç‚¹ï¼ˆå»ºè®®è¦†ç›–ï¼‰
1. **é‡‡æ ·è®¾è®¡æ›´è´´è¿‘çœŸå®ç»„åˆ**
   - åˆ†å±‚æŠ½æ ·ï¼ˆæŒ‰ ETF ç±»å‹/è§„æ¨¡/æµåŠ¨æ€§åˆ†æ¡¶ï¼‰
   - æƒé‡åˆ†å¸ƒæ›´è´´è¿‘çœŸå®æŒä»“ï¼ˆDirichlet Î± å‚æ•°å¯è§£é‡Šï¼‰
   - çº¦æŸå¼é‡‡æ ·ï¼ˆå•æ ‡çš„ä¸Šé™ã€æœ€å°æƒé‡ç­‰ï¼‰

2. **é˜ˆå€¼æ›´ç¨³å¥**
   - å»æå€¼æˆ– winsorize
   - å¤šçª—å£/å¤šå‘¨æœŸç»Ÿè®¡ï¼Œå–ç¨³å¥å‡å€¼æˆ–ä¸­ä½æ•°
   - warn/restrict åˆ†ä½æ•°å…·æœ‰æ˜ç¡®åŒºé—´ï¼ˆå¦‚ 80% / 95%ï¼‰

3. **äºŒæ¬¡æ ¡å‡†**
   - å¼•å…¥ä¸šåŠ¡è§„åˆ™ï¼ˆé›†ä¸­åº¦ã€è§„æ¨¡çº¦æŸã€åˆè§„é™åˆ¶ï¼‰
   - å‹åŠ›æœŸæ ¡éªŒï¼ˆæ³¢åŠ¨å‰§çƒˆæ—¶æœŸé˜ˆå€¼ä¸åº”è¿‡æ¾ï¼‰
   - é˜ˆå€¼ç¨³å®šæ€§çº¦æŸï¼ˆé¿å…å¤§å¹…æ¼‚ç§»ï¼‰

### äº¤ä»˜è¯´æ˜
è¯·æäº¤ä¸€ä»½â€œé˜ˆå€¼ç”Ÿæˆä¸æ ¡å‡†è¯´æ˜æ–‡æ¡£â€ï¼ŒåŒ…å«ï¼š
- é‡‡æ ·è®¾è®¡æ€è·¯ä¸å…¬å¼/ä¼ªä»£ç 
- åˆ†ä½æ•°é€‰æ‹©ç†ç”±
- äºŒæ¬¡æ ¡å‡†è§„åˆ™ä¸çº¦æŸ
- ä¸åŸå§‹åŸºçº¿ç›¸æ¯”çš„æ”¹è¿›ç‚¹

---

## ğŸ“¥ è¾“å…¥è¾“å‡ºç¤ºä¾‹

### è¾“å…¥ç¤ºä¾‹

```python
intent = {
    "date": "2025-11-15",
    "mode": "target",
    "targets": {
        "159213": 0.25,
        "159959": 0.25,
        "511960": 0.20,
        "516310": 0.20,
        "561180": 0.10
    }
}

context = {
    "current_positions": {
        "159213": 0.20,
        "159959": 0.20,
        "511960": 0.20,
        "516310": 0.20,
        "561180": 0.20
    },
    "policy_profile": "default",
    "aum": 1000000.0
}
```

### è¾“å‡ºç¤ºä¾‹

```python
{
    "decision": "pass",  # æˆ– warn/restrict/block
    "binding_constraints": [],
    "recommended_actions": [],
    "audit": {
        "policy_profile": "default",
        "llm_used": True,
        "supervisor_used": True,
        "nodes_to_run": ["market", "concentration", ...],
        "trace_id": "..."
    }
}
```

---


## ğŸ“š å‚è€ƒèµ„æº

- [LangGraph å®˜æ–¹æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)
- [LangChain å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/)
- æ ·æœ¬ç­”æ¡ˆï¼š`risk-mas/` ç›®å½•ï¼ˆä»…ä¾›å‚è€ƒï¼Œå»ºè®®å…ˆç‹¬ç«‹å®Œæˆï¼‰

---

## âœ… æäº¤è¦æ±‚

1. ä»£ç ç»“æ„ç¬¦åˆè¦æ±‚
2. é€šè¿‡åŸºæœ¬æµ‹è¯•ç”¨ä¾‹
3. æä¾› README.md è¯´æ˜å¦‚ä½•è¿è¡Œ
4. æäº¤å®Œæ•´çš„é¡¹ç›®ä»£ç 

---
**ç¥ä½ ç»ƒä¹ é¡ºåˆ©ï¼** ğŸ‰
